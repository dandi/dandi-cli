{"version":2,"ops":[{"type":1,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1644439117,"metadata":{"github-id":"I_kwDODBZtRc5DSyZQ","github-url":"https://github.com/dandi/dandi-cli/issues/903","origin":"github"},"title":"upload: make \"digesting\" in-band for zarr uploads OR just speed/cache zarr checksumming?","message":".zarr folder could be large in size/number of files and require  substantial investment to compute (even once) before upload even starts to commence.\n\n### Some summary notes on current design\n\n- `blobs` \n  -  globally de-duplicated across dandisets based on their checksums\n- `zarr` files \n   - are not *intended* to  be de-duplicated ATM, so there is per se no API to get the one \"matching\"\n     - it is possible to go through checksums of `GET /zarr` records to find one 'matching' **but** those could mutate in place ATM, thus it must not be relied on\n     - eventually we must make some zarrs immutable for `publish`ing\n   -  current implementation always mints a new `zarr_id` (UUID) for any given `name`\n   - `name` provided by dandi-cli is just a file name (lacks path to it)\n   - on S3 `zarr_id` is used for \"prefix\", not the name, so we cannot conflict/modify existing zarr\n\n### Upload logic and checksumming\n\nOverall ATM `upload` relies on the computed digest to decide either a given asset (blob or zarr) is to be uploaded at all since we are getting information from the server/metadata first if an asset with the etag already exists.  **I think this logic is generally applicable/beneficial to both blobs and zarr folders e.g. in cases of copying/moving zarr folders within/across dandisets. But we might need to do some sacrifices in case of zarr, especially since we are not \"deduplicating\" them ATM.**\n\nIn detail, while going through upload code logic some notes/observations:\n- `C1`: Having an etag (unconditionally) computed, `upload` code then uses it in case of an asset already existing (in `overwrite` and `refresh` modes) at the path on either that upload/change to asset should be skipped entirely. So, in principle, \n   - **etag computation could be delayed until if needed -- an asset at the path exists, is of the same size and in case of zarr -- that zarr has the same number of files** (if different -- we know that it is different -- upload regardless of checksum match)\n- if we are to proceed with upload, \n  - for a blob we do need etag to initiate upload, we \"compute\" it again (fscached call of `get_dandietag`, so not really). server can return 409 (and a blob_id) if blob with that etag exists, and we produce an asset which uses that blob_id.  \n     - **so for blobs we cannot avoid not pre-computing etag but it also could be delayed until/if needed**\n  - for zarr upload \n    - `C2`: we do use full zarr checksum to error out if zarr checksum is different from the one on server since ATM we do not support \"updates\".  \n      - It seems there is no fscacher involved so we do it 2nd time (in addition to `C1`)!\n      - It is conditional on `dandi-zarr-checksum` already to be known to metadata (on server?) so checksumming could in principle be delayed (but shouldn't be done 2nd time if was done in C1) till this point, to similarly as in `C1` decide if we need to error out\n      - Q1: it is not clear to me (@jwodder?) why we would like to proceed with the upload if checksum is known and \"the same\" since how server would know full checksum unless we finished uploading entire zarr properly?\n    - We do not really need full zarr's checksum/digest to initiate upload . We do provide per file digests here.\n    - At the end (after the `/complete` for the last batch) we verify that locally known checksum is identical to the one on server\n    - If checksum identical -- we create an asset, if not -- we error out\n    - **Let's sacrifice the check of checksum at `C1` for zarr, always `iter_upload`ing even if there is a zarr asset at the path already. Then we can avoid doing expensive \"digesting\".  `upload` logic will guard against uploading unless \"overwrite\" or \"refresh\" mode are specified**\n\n### Solution `#1`: avoid pre-checksumming of zarr files\n\n- `C1` should completely avoid checksumming of zarr archives \n- proceed to upload in \"overwrite\" unconditionally (no checksum for zarr),\n- and in \"refresh\" only if newer (but again - no checksum)\n- remove `C2` safeguard for zarrs, \n- compute zarr checksum  \"in-band\" while uploading \n- verify correctness of the upload based on that.  UX will be\n\n@jwodder - is my analysis correct above (note `Q1` question) or what other aspects am I missing, and either needed changes/developments make sense to avoid zarr checksum compute before upload?\n\n### Solution `#2`: do not make \"zarr\" too special, just speed things up\n\nIn principle, if checksumming of entire directories is made more efficient and fscaching of them more efficient (https://github.com/con/fscacher/issues/66) , I do not see how checksumming of 'zarr' directory of e.g. X GB is performance-wise different from checksumming of a blob of the same X GB, and we do/require it.  Above \"avoidance\" of checksumming zarr directories prior upload makes them \"special\", but may be  not for so good reason -- in principle we already can make them \"deduplicated\" quite easily and may be could eventually add some flag \"immutable: bool\" which would be set to `True` whenever full upload is finalized, thus letting them to be immutable forever (API should the refuse to modify an immutable zarr_id) -- needed for publishing anyways.  May be we should just keep that digesting, speed it up/fscache the result, and add API to dandi-archive to get a zarr given a checksum (as how we do for blobs)?  \nAlso in some cases pre-checksumming of zarrs could be avoided, e.g. if know that it is different (based on size, number of files) so we could instead optimize logic of upload to delay checksumming until really needed.\n\nWDYT @satra ?  Also\n\n- I do not think there is much of a point to provide any longer \"name\" (e.g. include folder from the top of the dataset) or demand `dandiset_id` field for the point of this particular issue.","files":null}]}