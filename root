{"version":2,"ops":[{"type":1,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1708722676,"metadata":{"github-id":"I_kwDODBZtRc6AQfCi","github-url":"https://github.com/dandi/dandi-cli/issues/1411","origin":"github"},"title":"Parallelize removal of extra files in Zarr","message":"related\n- #1410 \ndescribes the use-case.  I think that removal is going very slow and primarily since we do it serially on groups of keys.  Couldn't we parallelize (using the same jobs) and issue bunch of requests (with retries if needed) to that API DELETE endpoint?","files":null}]}