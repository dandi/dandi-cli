{"version":2,"ops":[{"type":1,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601651553,"metadata":{"github-id":"MDU6SXNzdWU3MTM2OTc3NzU=","github-url":"https://github.com/dandi/dandi-cli/issues/250","origin":"github"},"title":"Instantiate DataLad dandisets from backup","message":"Continuation to #243 which was addressed by providing/running https://github.com/dandi/dandi-cli/blob/master/tools/instantiate-dandisets.py .\n\nWhile API is still being cooking, I think it would be already beneficial to start providing proper DataLad datasets for all our dandisets.  \n\n# Short term implementation\n\nBuild on top of `instantiate-dandisets.py` so it would  (option names etc could be not exactly correct, just typing)\n\n- not just `mkdir` but use `ds = Dataset(path).create(cfg='text2git')` (if there is none) to establish a datalad dataset and configure text files, such as `dandiset.yaml` to go to git (everything is public anyways ATM and .nwb's are binary)\n  - by default datalad makes annex to use md5 based backend... our API ATM, and also git-annex by default uses SHA256.  So even though I would have liked shorter/faster SHA1, let's go with sha256e backend for annex, so create above should specify that one to use\n- after `cp`ing file from the local assetstore \n  - `ds.repo.add` it\n  - if metadata had digests\n  - `ds.repo.add_url_to_file` with URL pointing to redirected to location in the bucket , not girder API one so it still has a chance to work after girder is brought down but we still have assetstore in current shape (actually it would be even easier since script already works based on the path in the asset store!)\n- at the end -- just `ds.save` all the changes\n\nBut then add logic for proper updates \n- we should not re-cp/add file if did not change \n  - if metadata contains digest, we could base off that\n  - if not - we could use size/mtime to check if file was updated on dandi\n- if file (directory) is removed on dandi, we should remove locally\n\nCrawler (next section) has/uses https://github.com/datalad/datalad-crawler/blob/master/datalad_crawler/dbs/files.py#L89 to store/update information on each file status, but I guess we might just avoid all of that if we just store the full dump of assets listing metadata somewhere under `.datalad/dandi/` so that next time we run it we have a full list of files/assets from previous update to perform \"update\" actions listed above.\n\n# Alternative (a bit more involved, not sure if worthwhile ATM)\n\nCan be a \"datalad crawler\" pipeline (see https://github.com/datalad/datalad-crawler/), which would (internally) \n\n- take care about checking time/size \n   - might need to be extended to support check by digest\n- persistently storing that information per file (which it does already)\n\nBut since datalad-crawler, although functioning etc, is yet another thing to figure out and still uses older DataLad interfaces, primarily talking directly via GitRepo and AnnexRepo interfaces and without really taking advantage of higher level ones, I think it might be a bigger undertaking.\n\nNevertheless here are some pointers on that\n\n- very basic and scarce docs about \"crawler\" and its nodes and pipelines:  http://docs.datalad.org/projects/crawler/en/stable/basics.html  \n- https://github.com/datalad/datalad-crawler/blob/master/datalad_crawler/pipelines/xnat.py#L375 - a sample simple pipeline for XNAT.  It all boils down to just a few helpers to get listing, and pass it into annexificator\n- https://github.com/datalad/datalad-crawler/blob/master/datalad_crawler/pipelines/simple_with_archives.py#L28 -- one of the \"simple\" pipelines, which actually is not that simple since it implements 3-branch approach (`incoming` -\u003e `incoming-processed` -\u003e `master`) to support working with data from tarballs, and is \"reused\" in other pipelines I believe.\n- there was some thinking/idea about RFing or just providing a new (more modernly designed) pipeline which would \"integrate\" with `datalad addurls` -- some older thinking is at https://github.com/datalad/datalad-crawler/issues/22 .  So we might get there -- it could be just combining two calls - get assets list from dandi, tune them up just a bit, pass to the stock pipeline which would take care about doing all the time checks, removal of now obsolete files, and then passing the rest to addurls to do the rest (possibly splitting into subdatasets etc)\n\n# Longer term\nAlso, I hope that would just start updating datalad datasets straight within API backend/workers reacting to API calls thus making datalad datasets immediately reflecting introduced changes. For that we would not need a dedicated script or a crawler.","files":null}]}