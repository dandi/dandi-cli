{"version":2,"ops":[{"type":1,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1638885634,"metadata":{"github-id":"I_kwDODBZtRc4_-plq","github-url":"https://github.com/dandi/dandi-cli/issues/848","origin":"github"},"title":"more efficient caching","message":"the two imaging dandisets are large and will continuously run into caching efficiency. giacomo’s is only about 5TB but lee’s is around 120TB and growing. any kind of bids-related rewrite could thus involve significant checksum computation overhead that could take weeks. i would say it’s time to consider efficiency of both zarr versions and large files qua local checksum computation. i would say the overall problem is to ensure that a local directory can be checksummed efficiently. \n\none easy way is to maintain a table of mtime+size checksum alongside a dandi-etag in the cache. a rename or a move of a file doesn’t change this checksum and can be copied even across filesystems with both of those elements maintained.  thus having a table that is simply an LRU type cache would allow for local movement instead of tying it to a path name.","files":null}]}