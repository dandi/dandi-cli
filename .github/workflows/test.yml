name: Tests

on:
  push:
  pull_request:
  schedule:
    - cron: '0 6 * * *'

jobs:
  test:
    runs-on: ${{ matrix.os }}
    env:
      NO_ET: 1
      DATACITE_DEV_PASSWORD: ${{ secrets.DATACITE_DEV_PASSWORD }}

    strategy:
      fail-fast: false
      matrix:
        os:
          - windows-2019
          - ubuntu-18.04
          - macos-latest
        python:
          - 3.7
          - 3.8
          - 3.9
        dandi_api:
          - false
        dev_deps:
          - false
        include:
          - os: ubuntu-18.04
            python: 3.7
            dandi_api: true
            dev_deps: false
          - os: ubuntu-18.04
            python: 3.7
            dandi_api: false
            dev_deps: true
        exclude:
          # Temporarily disabled due to h5py/hdf5 dependency issue
          # See <https://github.com/dandi/dandi-cli/pull/315>
          - os: windows-2019
            python: 3.9

    steps:
    - name: Set up environment
      uses: actions/checkout@v1
      with:
        # Fetch all commits so that versioneer will return something compatible
        # with semantic-version
        fetch-depth: 0
    - name: Set up Python ${{ matrix.python }}
      uses: actions/setup-python@v1
      with:
        python-version: ${{ matrix.python }}

    - name: Install hdf5 (Ubuntu)
      if: matrix.python == '3.9' && startsWith(matrix.os, 'ubuntu')
      run: sudo apt-get update && sudo apt-get install -y libhdf5-dev

    - name: Install hdf5 (macOS)
      if: matrix.python == '3.9' && startsWith(matrix.os, 'macos')
      run: |
        brew install hdf5@1.8
        brew link hdf5@1.8

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip wheel
        # As of 2021-02-01, the latest version of hdmf (v2.3.0) requires numpy
        # >=1.16, <1.19.4, so we end up with numpy 1.19.3 installed.  However,
        # due to the release of numpy 1.20.0 on January 30, h5py gets built
        # against the later version instead, and because numpy 1.20.0 changed
        # the size of ndarray, an error results at runtime.  This can be worked
        # around by installing numpy before the other dependencies so that it
        # is available when h5py is built.
        pip install 'numpy<1.19.4'
        pip install ".[test]"

    - name: Install dev version of pynwb
      if: ${{ matrix.dev_deps }}
      run: |
        pip install git+https://github.com/NeurodataWithoutBorders/pynwb

    - name: Set environment variable when using Python 3.7 to cover more code
      run: echo DANDI_LOG_GIRDER=1 >> "$GITHUB_ENV"
      if: matrix.python == '3.7'

    - name: Run all tests
      if: "!matrix.dandi_api"
      run: |
        python -m pytest -s -v --cov=dandi --cov-report=xml dandi

    - name: Run Dandi API tests only
      if: matrix.dandi_api
      run: |
        python -m pytest -s -v --cov=dandi --cov-report=xml --dandi-api dandi

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v1
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        file: ./coverage.xml
        flags: unittests
        # name: codecov-umbrella
        # yml: ./codecov.yml
        fail_ci_if_error: false
