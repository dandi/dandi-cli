{"version":2,"ops":[{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606163203,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczMjQwNjkyOA==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-732406928"},"message":"@yarikoptic What's the canonical location for the documentation for the new API?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606163203,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDE2MDA0NTEz"},"target":"a5a888f85fb5ab0a228b63929a3fdf5a468796197c9bd32cc5d8ef9bd507f5a5","message":"@yarikoptic What's the canonical (or at least most up-to-date) location for the documentation for the new API?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1606168246,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczMjQ0NjkwNg==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-732446906"},"message":"https://api.dandiarchive.org/swagger would be the \"documentation\" ATM besides that actual code ;)  it should be close if not identical to that API google doc document we had been working on (let me know if you can't locate it)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606230128,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczMzAzMDI3NA==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-733030274"},"message":"@yarikoptic \n* What structure is required for the return value of `DandiAPIClient.get_dandiset()`?  The new API currently does not include the normal Dandiset metadata in its responses.\n* For the first step, exactly what refactoring is required beyond using the new endpoints (and proper pagination)?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606234825,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczMzA4NjI3Mw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-733086273"},"message":"@yarikoptic Can you set up builds on Docker Hub of [this Dockerfile](https://github.com/dandi/dandi-api/blob/master/dev/django-public.Dockerfile) under the name `dandiarchive/dandiarchive-api`?  When I try to do it, after selecting the \"dandi\" GitHub organization, the repository drop-down becomes greyed out.  This happens in both Safari and Firefox.","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606234825,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDE2MjUyNDEz"},"target":"d47059239e690c64f44737c72fe55d48b1ea8afcc77bf69c36d69b06444d37ba","message":"@yarikoptic Can you set up builds on Docker Hub of [this Dockerfile](https://github.com/dandi/dandi-api/blob/master/dev/django-public.Dockerfile) under the name `dandiarchive/dandiarchive-api`?  When I try to do it, after selecting the \"dandi\" GitHub organization, the repository drop-down becomes greyed out.  This happens in both Safari and Firefox.  (Could this be due to messing with dandibot's permissions a few days ago?)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606250332,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczMzIyMDY2Mw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-733220663"},"message":"@yarikoptic Is there a description of the upload workflow that matches what Swagger shows?  It looks like uploaders are supposed to:\n\n* POST the file name \u0026 size to `/uploads/initialize/` and get back some keys and a list of how large each part should be and what URL to upload each one to (which strikes me as a very ... odd design)\n* Upload each part to the respective URLs\n* Collect ETags for the parts somehow?\n* POST the keys and a list of each piece's size and ETag to `/uploads/complete/`\n* At some point, POST to `/uploads/validate/` to make the server validate the asset's hash\n* Wait for the validation to complete?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1606272548,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczMzQyODQ0Ng==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-733428446"},"message":"\u003e What structure is required for the return value of DandiAPIClient.get_dandiset()? The new API currently does not include the normal Dandiset metadata in its responses.\n\u003e For the first step, exactly what refactoring is required beyond using the new endpoints (and proper pagination)?\n\nto be able to download/upload/ls -- those common operations which we provided for previous girder and \"publish\" mix, but now could all go through DANDI API for both drafts and \"released\" versions","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1606272948,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczMzQzMDM5NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-733430395"},"message":"re upload: I will try to get a better sense of it as well tomorrow to comment on.  We might need to at least tune up API implementation right away to make initial initialization of upload endpoint closer to what we envisioned already etc, while \"naming\" this particular upload mechanism as \"s3-multipart\" or alike so (as we had \"provisioned\" for various in the initial API sketch document) we could keep the door open for other mechanisms (like later we might have `\"s3-copy\"`, to just perform copy within bucket from old store to new; or \"s3-simple\" which would not bother with multipart; be or later \"dropbox\"/... to download content from dropbox/... etc)","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1606316178,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczMzc1NDA3Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-733754077"},"message":"Re upload, FWIW here is a workflow summary from @dchiquit \n\n```\nSince it's not really obvious from the swagger page, this is the flow uploads should use if you're trying to implement upload from dandi-cli:\n1. POST /uploads/validate/{sha256}/ - Don't supply object_key, since you don't know it yet. This checks if the blob already exists. If it does, skip to step 5. If it fails, you gotta upload\n2. POST /uploads/initialize - give it the info on your file, it will return a list of URLs and sizes you can use to upload the parts. Keep track of the ETag header on the responses.\n3. POST /uploads/finalize - give this the ETags you uploaded and it will give you a URL you can use to finalize the object.\n4. POST /uploads/validations/ - Give it the object_key of the uploaded object and sha256 and it will run a validation. Right now this is a no-op that takes 10s to complete, just for testing.\n5. GET /uploads/validations/{sha256}/ - check on the progress of the validation\n6. POST /dandisets/{version__dandiset__pk}/versions/{version__version}/assets/ - Create the object. This checks that the validation has succeeded.\n```","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606316178,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDE2NTM3Njk1"},"target":"f046a076a8019c628452b567ce9e38a27491babd18832903426e44caf19d196e","message":"Re upload, FWIW here is a workflow summary from @dchiquit \n\n\u003e Since it's not really obvious from the swagger page, this is the flow uploads should use if you're trying to implement upload from dandi-cli:\n\u003e 1. POST /uploads/validate/{sha256}/ - Don't supply object_key, since you don't know it yet. This checks if the blob already exists. If it does, skip to step 5. If it fails, you gotta upload\n\u003e 2. POST /uploads/initialize - give it the info on your file, it will return a list of URLs and sizes you can use to upload the parts. Keep track of the ETag header on the responses.\n\u003e 3. POST /uploads/finalize - give this the ETags you uploaded and it will give you a URL you can use to finalize the object.\n\u003e 4. POST /uploads/validations/ - Give it the object_key of the uploaded object and sha256 and it will run a validation. Right now this is a no-op that takes 10s to complete, just for testing.\n\u003e 5. GET /uploads/validations/{sha256}/ - check on the progress of the validation\n\u003e 6. POST /dandisets/{version__dandiset__pk}/versions/{version__version}/assets/ - Create the object. This checks that the validation has succeeded.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606317439,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczMzc2OTE0MQ==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-733769141"},"message":"@dchiquit \n\u003e 1. POST /uploads/validate/{sha256}/\n\nThere's no such endpoint listed on the Swagger page.  Did you mean `GET /uploads/validations/{sha256}/`?  The docs for that endpoint have no mention of an `object_key`.\n\n* Should the file name posted to `/uploads/initialize/` be just the basename of the file, or should the directory name (e.g., \"sub-P10HMH\") be included?  Or should the latter only be used for the \"path\" key in step 6?\n\n* I assume that by `POST /uploads/finalize` and `POST /uploads/validations/` you mean `POST /uploads/complete/` and `POST /uploads/validate/`.\n\n* What should be included in the metadata posted for the asset in step 6?","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606754559,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNTkwMjAwMQ==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-735902001"},"message":"Excellent questions.\n* I meant `POST /uploads/validate/`, which does have the correct documentation regarding `object_key`. That begins the validation process which can be checked on with `GET /uploads/validations/{sha256}/`.\n* The behavior of `file_name` in `/uploads/initialize/` is still not quite fully implemented. No matter what you call it, it will be referenced by it's sha256 checksum, so in some sense it's arbitrary. The only thing it's used for is determining the name of the S3 object, so I guess use the full path to avoid collisions.\n  The \"real\" path of the blob in the dandiset is set when you register the asset.\n* You are correct, I got mixed up.\n* That is the metadata of the file in the dandiset. My understanding is that there is dandiset level metadata and asset level metadata, so that metadata would describe the content of that NWB file.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606762729,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNTk3NzU5Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-735977597"},"message":"@dchiquit So when the POST request is made to `/uploads/validate/`, how does the client determine whether the blob already exists?  The endpoint doesn't return any data.  Am I expected to also make a request to `GET /uploads/validations/{sha256}/` before uploading anything?\n\n\u003e POST /uploads/finalize - give this the ETags you uploaded and it will give you a URL you can use to finalize the object.\n\nWhat exactly do I do with this URL?  Just make an empty POST request?","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606768520,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNjAyNzQ4Mg==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-736027482"},"message":"Before uploading anything, you should do a `GET /uploads/validate/` with the sha256 of the data you would be uploading. If it fails, that data hasn't been uploaded yet, so you need to upload it. If it succeeds, you can skip uploading. There should be more docs for response codes #288 \n\nI got that endpoint mixed up with `POST /uploads/complete/`. After you have uploaded the actual data to S3, you still need to confirm with S3 that you are finished and want to create the object. `POST /uploads/complete/` does describe the data you need to include; the most important thing is the `ETag` for each part you uploaded, which can be found in a special `ETag` header in the response from S3. `POST /uploads/complete/` returns another presigned URL that you need to visit.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606769443,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNjAzNTE0NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-736035145"},"message":"@dchiquit \n\n\u003e POST /uploads/complete/ returns another presigned URL that you need to visit.\n\nThat's the URL that I was asking about.  Do I make a GET or a POST to this \"presigned URL\"?","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606771369,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNjA2NDcyOA==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-736064728"},"message":"`POST`. This is the operation that is being presigned: https://docs.aws.amazon.com/AmazonS3/latest/API/API_CompleteMultipartUpload.html","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606832130,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNjU3ODIxNw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-736578217"},"message":"@dchiquit How do I authenticate with the new API?  I'm working with a Docker Compose environment in which a superuser is created in the Django container with `./manage.py createsuperuser`, but querying http://localhost:8000/api/auth/token/ with the username \u0026 password used in the createsuperuser command returns \"Invalid username/password.\"  I also tried setting the Authorization header to \"bearer {token}\", where the token is obtained from `./manage.py drf_create_token`, but that resulted in the error \"Authentication credentials were not provided.\"","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606834014,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNjU5NzQ4OA==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-736597488"},"message":"I generate mine by logging into the swagger page at http://localhost:8000/swagger/, clicking `Django Login`, logging in with superuser credentials, then using the swagger page to call `GET /auth/token/`.\nRunning `./manage.py drf_create_token` generates the same token for me, so I think your problem is specifying the token in the request. The header should be formatted as `Authorization: Token e9d103cdb6bb1907a501282ac02b3d5265dac487`, not 'bearer'. This is a curl that works for me:\n```\ncurl -X POST \"http://localhost:8000/api/auth/token/\" -H  \"accept: application/json\" -H \"Authorization: Token e9d103cdb6bb1907a501282ac02b3d5265dac487\"\n```","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606855795,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNjgwMzMzNA==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-736803334"},"message":"@dchiquit Your explanations of the validation endpoints are inconsistent and keep conflicting with the endpoints listed [here](https://api.dandiarchive.org/swagger/).  Exactly what validation-related API requests do I make before starting an upload, what are the possible status codes of the responses, and what should I do based on those status codes?\n\nIs it:\n\n1. `POST /uploads/validate/`, which should always succeed, followed by `GET /uploads/validation/{sha256}/`, which fails if \u0026 only if the blob is unknown to the server (i.e., if \u0026 only if the client should go through with the upload), or ...\n\n2. Just `POST /uploads/validate/`, which fails if \u0026 only if the blob is unknown to the server (i.e., if \u0026 only if the client should go through with the upload)\n\nAdditionally, once the upload is complete, what requests does the client make to the validation endpoints to ensure that the validation completes?  Does this differ from the requests that would be made if there were no upload?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606855795,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDE3ODI1MDg3"},"target":"661cee856a5f0d5fd75ec3e7fb3322396764d5596a5dc2d8535ba2cd201eba14","message":"@dchiquit Your explanations of the validation endpoints are inconsistent and keep conflicting with the endpoints listed [here](https://api.dandiarchive.org/swagger/).  Exactly what validation-related API requests do I make before starting an upload, what are the possible status codes of the responses, and what should I do based on those status codes?\n\nIs it:\n\n1. `POST /uploads/validate/`, which should always succeed, followed by `GET /uploads/validation/{sha256}/`, which fails if \u0026 only if the blob is unknown to the server (i.e., if \u0026 only if the client should go through with the upload), or ...\n\n2. Just `POST /uploads/validate/`, which fails if \u0026 only if the blob is unknown to the server (i.e., if \u0026 only if the client should go through with the upload), or ...\n\n3. Something else?\n\nAdditionally, once the upload is complete, what requests does the client make to the validation endpoints to ensure that the validation completes?  Does this differ from the requests that would be made if there were no upload?","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606926923,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzM0NzU4Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737347587"},"message":"Option 2) is correct. `POST /uploads/validate/` tries to start an asynchronous task to do the validation. If anything would prevent that from happening, like the object not existing in S3, it will return a 400. To avoid unnecessary uploading, you can call `POST /uploads/validate/` without an `object_key`. if an object with that checksum has been validated before, it will look up the object key used in the previous validation and use it to start a new validation; if that checksum is unrecognized, it will fail. `GET /uploads/validation/{sha256}/` returns the state of the asynchronous vaidation task.\n\nOnce `POST /uploads/validate/` has returned 204, calling `GET /uploads/validation/{sha256}/` will return the current state of the validation as a JSON blob. There is a field called `state` (poorly named, sorry) that is one of `\"IN_PROGRESS\"`, `\"SUCCEEDED\"`, or `\"FAILED\"`. The client needs to keep calling `GET /uploads/validation/{sha256}/` until the state is no longer `IN_PROGRESS`.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606928178,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzM2MDQ2Mg==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737360462"},"message":"@dchiquit OK, now that I'm actually uploading, the response to the first (and only?) chunk is:\n\n```\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\n\u003cError\u003e\u003cCode\u003eSignatureDoesNotMatch\u003c/Code\u003e\u003cMessage\u003eThe request signature we calculated does not match the signature you provided. Check your key and signing method.\u003c/Message\u003e\u003cKey\u003etesting/simple1.nwb\u003c/Key\u003e\u003cBucketName\u003edandi-dandisets\u003c/BucketName\u003e\u003cResource\u003e/dandi-dandisets/testing/simple1.nwb\u003c/Resource\u003e\u003cRequestId\u003e164CF40A1086394C\u003c/RequestId\u003e\u003cHostId\u003e3b7230f2-2264-4096-83bd-89ea2c3552f3\u003c/HostId\u003e\u003c/Error\u003e\n```\n\nI'm quite certain I'm calculating the hash and uploading the exact piece correctly.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606935765,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzQzMjMyNg==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737432326"},"message":"Perhaps you are using POST instead of PUT? This is the operation that is being performed: https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPart.html\n\nHere's a code snippet I pieced together from our test suite that uploads a file:\n```\n\ndef initialize_upload(filename):\n    print(f'Initializing upload of {filename}...')\n    filesize = os.path.getsize(filename)\n    resp = requests.post(\n        f'{URL_ROOT}/api/uploads/initialize/',\n        headers={'Authorization': f'Token {AUTH_TOKEN}'},\n        json={'file_name': filename, 'file_size': filesize},\n    )\n    assert resp.status_code == 200\n    return resp.json()\n\n\ndef complete_upload(object_key, upload_id, transferred_parts):\n    print('Presigning upload completion...')\n    resp = requests.post(\n        f'{URL_ROOT}/api/uploads/complete/',\n        headers={'Authorization': f'Token {AUTH_TOKEN}'},\n        json={\n            'object_key': object_key,\n            'upload_id': upload_id,\n            'parts': transferred_parts,\n        },\n    )\n    assert resp.status_code == 200\n    return resp.json()\n\n\ndef upload(filename):\n    initialization = initialize_upload(filename)\n    object_key = initialization['object_key']\n    upload_id = initialization['upload_id']\n    parts = initialization['parts']\n\n    transferred_parts = []\n    part_number = 1\n    with open(filename, 'rb') as f:\n        for part in parts:\n            print(f'Uploading part {part_number}...')\n            data = f.read(part['size'])\n            part_transfer = requests.put(part['upload_url'], data=data)\n            etag = part_transfer.headers['etag']\n            transferred_parts.append(\n                {'part_number': part_number, 'size': part['size'], 'etag': etag}\n            )\n            part_number += 1\n\n    completion = complete_upload(object_key, upload_id, transferred_parts)\n\n    print('Completing upload...')\n    completion_response = requests.post(completion['complete_url'], data=completion['body'])\n\n    return object_key\n```\n\nI can find examples for the rest of the flow as well if that would be easier than this back and forth.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606937107,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzQ0NDM1NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737444355"},"message":"@dchiquit OK, now when I do `POST /uploads/validate/` after completing the upload in order to validate the blob, I get a 400 response.  If you have a code sample that addresses this part of the flow, I'd be glad to see it.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606937259,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzQ0NTU4Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737445587"},"message":"```\n\ndef wait_for_validation(sha256):\n    print(f'Waiting for blob {sha256} to finish validating...')\n    state = None\n    while state != 'SUCCEEDED':\n        time.sleep(1)\n        resp = requests.get(\n            f'{URL_ROOT}/api/uploads/validations/{sha256}/',\n            headers={'Authorization': f'Token {AUTH_TOKEN}'},\n        )\n        state = resp.json()['state']\n        if state == 'FAILED':\n            raise Exception('Validation failed.')\n\n\ndef validate_existing_blob(sha256):\n    print(f'Attempting to validate a blob with checksum {sha256}...')\n    resp = requests.post(\n        f'{URL_ROOT}/api/uploads/validate/',\n        headers={'Authorization': f'Token {AUTH_TOKEN}'},\n        json={'sha256': sha256},\n    )\n    if resp.status_code == 204:\n        wait_for_validation(sha256)\n    elif resp.status_code == 400:\n        print('No validation found for that blob.')\n        raise ObjectIsNotUploaded() # This is a special exception to indicate that the blob needs to be uploaded\n    else:\n        raise Exception(f'Unknown response status code {resp.status_code}.')\n\n\ndef validate_uploaded_blob(sha256, object_key):\n    print(f'Validating a newly uploaded blob {object_key} with checksum {sha256}...')\n    resp = requests.post(\n        f'{URL_ROOT}/api/uploads/validate/',\n        headers={'Authorization': f'Token {AUTH_TOKEN}'},\n        json={'sha256': sha256, 'object_key': object_key},\n    )\n    if resp.status_code == 204:\n        wait_for_validation(sha256)\n    else:\n        raise Exception(f'Unknown response status code {resp.status_code}.')\n\n```","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606937789,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzQ0OTk0NA==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737449944"},"message":"Also, 400 responses should also include useful error messages in the response body. I cannot make the same promise for 500 errors though.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606938615,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzQ1NzA0NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737457045"},"message":"@dchiquit That code was helpful, but now my code has been polling `/uploads/validations/{sha256}/` for more than ten minutes without the state changing from `IN_PROGRESS`.  I don't think validating a 17KB file should take that long.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606938761,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzQ1ODI4Mw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737458283"},"message":"I agree, it should only be calculating the checksum of the file and verifying that it matches. FWIW I have verified that very small files work correctly. I will look in to that behavior.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606939237,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzQ2MjMwNw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737462307"},"message":"I just verified uploads of around that size work correctly locally and on api.dandiarchive.org. The most likely cause is that you aren't running celery, which is what runs the asynchronous task. I'm not 100% confident in the docker compose file if that's what you're using, so I will verify that it's working correctly later today.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1606939552,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzQ2NTE4Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737465187"},"message":"@dchiquit [This](https://github.com/dandi/dandi-cli/blob/gh-277/dandi/tests/data/dandiarchive-docker/docker-compose.yml) is the Docker Compose file I'm using.  Both the django and celery containers are brought up towards the end of the startup process, and Celery seems to be running fine.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1606949475,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczNzU0MzgwOQ==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-737543809"},"message":"The problem is that the validation task has to download the data from the object store, but cannot because of some unfortunate docker-compose networking quirks.\nWhen the `celery` service needs to communicate with other services (specifically `minio`), it needs to address them directly by service name like `http://minio:9000/whatever` instead of `http://localhost:9000/whatever`. When generating the URL to download the data for validation, it looks something like `http://localhost:9000/dandi-dandisets/test.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=minioAccessKey%2F20201202%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Date=20201202T223646Z\u0026X-Amz-Expires=604800\u0026X-Amz-SignedHeaders=host\u0026X-Amz-Signature=6ba8022bd45a5e9242c71adc891f3f724a4ac4ac1f83ed7b2f221f00d9ced7ec`. This is arguably correct, because this link works correctly from outside the containers. When the `celery` service tries to use that URL however, it does not know to associate `localhost` with the `minio` service, so it just fails. This is a hard problem that will probably not be fixed this week.\n\nThe easiest solution for right now is to just use `https://api.dandiarchive.org` to test it. It will be cleaned up anyway and I've verified that it works correctly.\nFor local development, the README has instructions on `Develop Natively (advanced)` that uses docker compose for the backing services (postgres, minio, rabbitmq) and runs django and celery as native processes, avoiding this problem.\n\nThe error handling for this failure is very lackluster, so I will be fixing it so it fails with a meaningful error rather than hanging indefinitely.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1607024868,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczODI2MDc5Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-738260797"},"message":"@yarikoptic If we go with testing against https://api.dandiarchive.org instead of Docker Compose, I'll need a secret containing an API key for the new API.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1607029892,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczODMxNDIzOQ==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-738314239"},"message":"I've been developing locally using the `Develop Natively (advanced)` instructions from the README.\nYou should be able to generate your own API key. Go to https://api.dandiarchive.org/swagger/, click `Django Login`, sign up with github, go back to the swagger page, then use `GET /auth/token/` to get an API key.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1607030122,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczODMxOTQ0MA==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-738319440"},"message":"@dchiquit I know I can generate my own key, but it'll need to be added as a GitHub secret to be available to the automated tests, and I don't have permission to do that.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1607034160,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczODM2OTkyNA==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-738369924"},"message":"I would really prefer not to test full upload/download cycle on the central api.dandiarchive.org (assuming that at some point it would be the one hosting real data).  I bet there must be a way to pre-seed or automagically grab some token for docker compose setup, or add some API endpoint available only if ran in some \"devel\" mode (e.g. if some env var is set) for that until we do not figure some better way.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1607089865,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDczODc5NDgzNA==","github-url":"https://github.com/dandi/dandi-cli/issues/277#issuecomment-738794834"},"message":"@yarikoptic There is a way to get a token for the Docker Compose setup; the problem is that the Docker Compose setup has a networking issue that's currently preventing uploads from finishing.","files":null},{"type":4,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1608139638,"metadata":{"github-id":"MDExOkNsb3NlZEV2ZW50NDEyMDkxMDMyMw=="},"status":2}]}