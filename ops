{"version":2,"ops":[{"type":1,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1645039736,"metadata":{"github-id":"I_kwDODBZtRc5D-hp5","github-url":"https://github.com/dandi/dandi-cli/issues/914","origin":"github"},"title":"do not fscache individual files digests for zarr-checksum","message":"A follow up to #913 which might have more timing information.  For that issue timings I disabled (after first physically moving aside the entire fsccher's cache for dandi-digests) fscacher, and the run took about 23 seconds.  Whenever I stopped disabling cache, the run took over 5 minutes (so fscacher gave 1500% overhead if I got it right):\n```\n(dandi-devel) jovyan@jupyter-yarikoptic:/shared/ngffdata$ time dandi digest -d zarr-checksum test64.ngff/0/0/0/0\ntest64.ngff/0/0/0/0: 53ad9d9fe8b4f34882fdaea76599da22\n2022-02-16 19:02:54,861 [    INFO] Logs saved in /home/jovyan/.cache/dandi-cli/log/20220216185712Z-5383.log\n\nreal    5m43.311s\nuser    0m24.188s\nsys     0m13.128s\n```\nand rerunning,  took 4 sec (which is better than original 24sec, but slower than just full recompute could be, see #913):\n\n```\n(dandi-devel) jovyan@jupyter-yarikoptic:/shared/ngffdata$ time dandi digest -d zarr-checksum test64.ngff/0/0/0/0\ntest64.ngff/0/0/0/0: 53ad9d9fe8b4f34882fdaea76599da22\n2022-02-16 19:05:03,854 [    INFO] Logs saved in /home/jovyan/.cache/dandi-cli/log/20220216190459Z-5515.log\n\nreal    0m4.576s\nuser    0m2.878s\nsys     0m4.593s\n```\n\nand that is using fscacher of https://github.com/con/fscacher/pull/67 .... may be that one needs really needs to become more efficient .\n\nFWIW\n\n\u003cdetails\u003e\n\u003csummary\u003ewith this patch I disabled caching individual files digests but added one for zarr folder\u003c/summary\u003e \n\n```patch\n(dandi-devel) jovyan@jupyter-yarikoptic:~/dandi-cli$ git diff\ndiff --git a/dandi/support/digests.py b/dandi/support/digests.py\nindex 2226ea8..74e5199 100644\n--- a/dandi/support/digests.py\n+++ b/dandi/support/digests.py\n@@ -81,7 +81,7 @@ class Digester:\n checksums = PersistentCache(name=\"dandi-checksums\", envvar=\"DANDI_CACHE\")\n \n \n-@checksums.memoize_path\n+#@checksums.memoize_path\n def get_digest(filepath: Union[str, Path], digest: str = \"sha256\") -\u003e str:\n     if digest == \"dandi-etag\":\n         return cast(str, get_dandietag(filepath).as_str())\n@@ -96,6 +96,7 @@ def get_dandietag(filepath: Union[str, Path]) -\u003e DandiETag:\n     return DandiETag.from_file(filepath)\n \n \n+@checksums.memoize_path\n def get_zarr_checksum(\n     path: Path,\n     basepath: Optional[Path] = None,\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eand it ran \"fast\" in those original 22 sec, and reloaded result in fscacher in its 3-4 sec\u003c/summary\u003e \n\n```shell\n(dandi-devel) jovyan@jupyter-yarikoptic:/shared/ngffdata$ mv  ~/.cache/fscacher/dandi-checksums{,.aside2}\n(dandi-devel) jovyan@jupyter-yarikoptic:/shared/ngffdata$ time dandi digest -d zarr-checksum test64.ngff/0/0/0/0\ntest64.ngff/0/0/0/0: 53ad9d9fe8b4f34882fdaea76599da22\n2022-02-16 19:22:10,832 [    INFO] Logs saved in /home/jovyan/.cache/dandi-cli/log/20220216192149Z-6188.log\n\nreal    0m22.338s\nuser    0m8.445s\nsys     0m6.815s\n(dandi-devel) jovyan@jupyter-yarikoptic:/shared/ngffdata$ time dandi digest -d zarr-checksum test64.ngff/0/0/0/0\ntest64.ngff/0/0/0/0: 53ad9d9fe8b4f34882fdaea76599da22\n2022-02-16 19:23:02,610 [    INFO] Logs saved in /home/jovyan/.cache/dandi-cli/log/20220216192259Z-8240.log\n\nreal    0m3.401s\nuser    0m2.926s\nsys     0m5.002s\n```\n\u003c/details\u003e\n\nMeanwhile, I think it would be worth to just disable the fscaching of both individual file digests in the zarr archive (overhead from storing that many digests on initial run seems too great to ignore) and zarr folder altogether (until we make fscaching of folder more efficient).  \n\nNot sure if alternatively we should/could come up with some smarter policy/specification for what to cache, e.g. we could parametrize `memoize_path` to cache only if a file is larger than some specified size (e.g. 500KB or 1MB in the case of digests), but it would still need some `io.stat` to make the decision etc thus altogether might still have some overhead... but I think it might make it more flexible/generic.","files":null}]}