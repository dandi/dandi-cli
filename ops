{"version":2,"ops":[{"type":1,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1612833170,"metadata":{"github-id":"MDU6SXNzdWU4MDQxMDI3Mjc=","github-url":"https://github.com/dandi/dandi-cli/issues/367","origin":"github"},"title":"upload: dandi-api - no mtimes are uploaded, so we can neither provide \"refresh\" nor fast check","message":"In interactions with girder server, besides metadata record, we had uploaded `uploaded_mtime`  record which was used to\n\n- quickly assess if file on remote is potentially different (if taken with size) from what is on the server, so we could have a large collection of files locally and quickly decide either any is needed to be uploaded\n- quickly judge either file on the server is older than the one we have locally, and upload only a newer one in \"--existing=refresh\" mode\n\nWith new dandi-api server we do not upload mtime as part of the AssetMeta, which precludes both of the aforementioned modes of operation.  Possible ways forward\n\n1: introduce [modifiedTime](https://schema.org/modifiedTime) to assetMeta.  I dislike this since it is highly volatile and per  se not metadata of the asset as stored within the file.  May be there is some \"last modified\" time stamp in nwb but then it would make it nwb specific and thus also not good and thus be avoided.\n2: we could introduce that \"objectId\" from nwb into metadata somewhere which would allow assess if file is changed somewhat quickly under assumption that objectId should be changed by any modification: well -- direct hdf5 manipulations would not do that. again - nwb specific.\n3: keep model as is, forget about \"refresh\" mode, and to avoid lengthy re-digesting of every file we would need to memoize result of computing digest per each path.  Should generally work, be slow only on initial sensing/upload of the files (to actually estimate digest). But I feel a bit unsettled about memoizing checksums, not sure if should be default (with option to disable) or some explicit `--fast` mode.\n4: alternative to easy to do memoization of (only) checksums - per each \"local\" copy of a dandiset keep information about each uploaded file (the same mtime, inode, size which are used during memoization) and thus be able later to check those mtimes. But IMHO too much hassle and does not help with \"racy\" uploads from multiple locations\n\nSo I do not see any generically nice way keep `dandi upload` as usable on large volumes of data and help to avoid useless re-uploads (the same file re-uploaded back and forth from two different locations) etc. `3` is probably the most reasonable, but would not provide \"refresh\" mode.\n\nAny additional ideas @satra ?","files":null}]}