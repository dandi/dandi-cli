{"version":2,"ops":[{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1638887181,"metadata":{"github-id":"IC_kwDODBZtRc46402s","github-url":"https://github.com/dandi/dandi-cli/issues/848#issuecomment-987975084"},"message":"@satra So you're saying that if a file about to be digested has the same mtime and size as any previously digested file, it should be assumed to be the same file and the previous digest returned?  That seems liable to frequently give wrong results.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1638889860,"metadata":{"github-id":"IC_kwDODBZtRc464-18","github-url":"https://github.com/dandi/dandi-cli/issues/848#issuecomment-988015996"},"message":"@jwodder - mtime + size by itself is insufficient as a checksum. however, in a specific setting of a dandiset, it can be used as a proxy. in the cache i would maintain path and allow the option of replacing the path with a new path. so if someone is doing a rename, this should work.  for example say `dandi mv path1 path2` would update the cache and not require re-checksumming.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1638892400,"metadata":{"github-id":"IC_kwDODBZtRc465IYc","github-url":"https://github.com/dandi/dandi-cli/issues/848#issuecomment-988055068"},"message":"I really would like to avoid mtime+size alone as some kind of a \"proxy\" measure. IMHO it is more important to have it *AFAIK correct (even if slow)* than *likely to be correct (but faster)*.\n\nIn principle `dandi mv` could be implemented but we would need to figure out how to RF [fscacher](https://github.com/con/fscacher/) we developed/used for the purpose of caching such compute results. \n(note that also any dandi-cli, dandischema, or pynwb or other listed library update would also invalidate cache - may be for digesting we should relax that a bit)\n\nA complimentary/alternative is for people to use git-annex (and/or datalad) to version their data (unless on windows or file system without symlinks support), and then fscacher AFAIK (since we have a test https://github.com/con/fscacher/blob/HEAD/src/fscacher/tests/test_cache.py#L146 and  https://github.com/con/fscacher/issues/44 open) would resolve the symlink to the actual file with content, thus avoiding redigesting.  If it still does -- we should have that fixed I guess.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1638892704,"metadata":{"github-id":"IC_kwDODBZtRc465JcI","github-url":"https://github.com/dandi/dandi-cli/issues/848#issuecomment-988059400"},"message":"also if we are talking about `mv` in the archive itself, may be such `mv` (or `move`?) should parallel [delete](https://dandi.readthedocs.io/en/latest/cmdline/delete.html?highlight=delete) which we already have: move them in the archive (may be with an option to move locally as well).","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1638896274,"metadata":{"github-id":"IC_kwDODBZtRc465U4U","github-url":"https://github.com/dandi/dandi-cli/issues/848#issuecomment-988106260"},"message":"\u003e I really would like to avoid mtime+size alone as some kind of a \"proxy\" measure. IMHO it is more important to have it AFAIK correct (even if slow) than likely to be correct (but faster).\n\nslow is what we are trying to avoid, so we need the engineering to avoid slow. \n\n\u003e (note that also any dandi-cli, dandischema, or pynwb or other listed library update would also invalidate cache - may be for digesting we should relax that a bit)\n\nindeed i am not a fan of why digesting would be invalidated by those libraries. the intent of a digest is that it is not dependent on a library version. \n\ngit-annex / datalad can be an option when this becomes something most neuroscientists can do. at present we will have a lot of microscopy data coming in and most of the awardees have indicated that the terminal is not something they use daily. we have to adjust our efforts towards ease.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1638899418,"metadata":{"github-id":"IC_kwDODBZtRc465eGS","github-url":"https://github.com/dandi/dandi-cli/issues/848#issuecomment-988144018"},"message":"\u003e \n\n\u003e the intent of a digest is that it is not dependent on a library version.\n\nwell, if library \"fixes\" digest algorithm, cache should be invalidated.  But indeed, since we already well-tested this functionality, @jwodder -- may be instead of library versions, just add some explicit token value (e.g. `1`) for \"digest implementation\", so we could still be able to invalidate prior cache by explicitly changing that token.\n\n\u003e slow is what we are trying to avoid, so we need the engineering to avoid slow.\n\nwell, caching already allows to avoid it for many scenarios. \n\nBesides explicit `dandi mv` I do not see some magical way to speed it up more (without having some underlying mechanism -- from filesystem or git-annex to provide reliable \"content identity\" information)\n\nAs for support for some `move` in the fscacher, let's look into feasibility separately -- https://github.com/con/fscacher/issues/57 .\n\nRe-implementing support for \"more efficient caching\" straight in dandi-cli, I guess could be done, but at large would probably end up being just an \"overfit fork\" of fscacher and/or joblib (which fscacher relies upon) implementation... might need to be done, but I think we should first research more into possible solutions/approaches.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1639609685,"metadata":{"github-id":"IC_kwDODBZtRc47Utbz","github-url":"https://github.com/dandi/dandi-cli/issues/848#issuecomment-995284723"},"message":"\u003e \u003e the intent of a digest is that it is not dependent on a library version.\n\u003e \n\u003e well, if library \"fixes\" digest algorithm, cache should be invalidated. But indeed, since we already well-tested this functionality, @jwodder -- may be instead of library versions, just add some explicit token value (e.g. `1`) for \"digest implementation\", so we could still be able to invalidate prior cache by explicitly changing that token.\n\nFWIW: I checked the code. Apparently we do not add any token ATM, so no upgrades should invalidate that cache (may be only if joblib does some invalidation upon its upgrade, didn't check): https://github.com/dandi/dandi-cli/blob/master/dandi/support/digests.py#L76 . So nothing todo on this regard.","files":null}]}