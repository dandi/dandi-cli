{"version":2,"ops":[{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1645042851,"metadata":{"github-id":"IC_kwDODBZtRc4-HkZu","github-url":"https://github.com/dandi/dandi-cli/issues/915#issuecomment-1042171502"},"message":"@dchiquito - the presigned url generation should make the md5 parameter optional. we are planning to upload and check on the client side and the client can push a tree-checksum after upload is complete that the server could verify.\n\nbut for the moment i would disable generation of checksum files.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1645048183,"metadata":{"github-id":"IC_kwDODBZtRc4-IOTC","github-url":"https://github.com/dandi/dandi-cli/issues/915#issuecomment-1042343106"},"message":"\u003e and the client can push a tree-checksum after upload is complete that the server could verify.\n\nhm, this sounds a bit backwards since we do not have really \"invalid\" state on the server (and what to do if we get into one?), that is why client is \"pull\"ing the checksum, verifies and alerts the user.","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1645048194,"metadata":{"github-id":"UCE_lALODBZtRc4-IOTCziD61uI"},"target":"f0b2fdb5f3a405821d288fbffb99c29775c130582a1a7c257ab54919f21fe212","message":"\u003e and the client can push a tree-checksum after upload is complete that the server could verify.\n\nhm, this sounds a bit backwards since we do not have really \"invalid\" state on the server (and what to do if we get into one?), that is why client is \"pull\"ing the checksum, verifies and alerts the user. seems needs more thinking","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1645057762,"metadata":{"github-id":"IC_kwDODBZtRc4-ImYX","github-url":"https://github.com/dandi/dandi-cli/issues/915#issuecomment-1042441751"},"message":"we do have invalid state - checksum pending for normal assets, so when server finishes calculating checksum, which can take some time potentially, it can update the asset state.  the client would have to wait, which in some cases could be minutes, and i don't think we want the client during upload to do that. unless we know that checksum is within say 30s of the last batch uploaded. if that is the case, we could have the client wait to get the value back.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1645124851,"metadata":{"github-id":"IC_kwDODBZtRc4-L7gt","github-url":"https://github.com/dandi/dandi-cli/issues/915#issuecomment-1043314733"},"message":"\u003e we do have invalid state - checksum pending for normal assets, so when server finishes calculating checksum, which can take some time potentially, it can update the asset state\n\nyes. In the description above for me of importance is that the \"state\" is not informed by client but by the server -- checksum is computed (on the server), and whatever is computed it is what server can rely to be valid (since it is the server who did it).  It is nohow informed by the checksum client might provide (which might be incorrect, not uptodate or whatnot), thus invalidating a \"valid\" (as on S3) checksum for the zarr.  That is why IMHO it is for client to verify that server has something what client expects, and if not -- mitigate (reupload fully or partially) but not mess with the state of the asset as server knows directly.  (and that is why we compute sha256 instead of just taking it from client)\n\n\u003e but for the moment i would disable generation of checksum files.\n\nis that generation/updates is a bottleneck (I do not remember clear demonstration that it is)? If so, then IMHO we should look into mitigating it (\"aggregate\" .checksums files? or keep them in DB entirely) instead of completely disabling, because ensuring consistency might only require more complicated implementation later (e.g. needing to lock zarr to avoid any changes while checksum is computed; dandi-cli would need to channel that error to users etc).  But I would you and @dchiquito to decide on what we do on that.\n\nIf/when checksums updates are disabled, we would need to adjust dandi-cli to not wait/expect them, and also to use ETag to decide on either a specific file needs to be uploaded or not if API would stop providing checksums. That would also require interactions with S3 per each tiny file, making it slow.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645459933,"metadata":{"github-id":"IC_kwDODBZtRc4-aIVz","github-url":"https://github.com/dandi/dandi-cli/issues/915#issuecomment-1047037299"},"message":"@yarikoptic \n\n\u003e overall zarr archive checksum'ing on the server will be disabled, so we should then stop checking if our upload of the zarr is consistent with the remote upon completion.\n\nIs there a dandi-archive issue for this?  Aside from the contemplative changes to the API that @satra's talking about, this seems to be the current blocker for this issue.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1645461284,"metadata":{"github-id":"IC_kwDODBZtRc4-aNIY","github-url":"https://github.com/dandi/dandi-cli/issues/915#issuecomment-1047056920"},"message":"@dchiquito - is the md5 parameter optional? if not, can it be made optional, for this strategy to be implemented. \n\n@yarikoptic, @dchiquito, and @jwodder - regarding checksums\n- the zarr trees are going to be large, whether now or in the future. \n- we will need algorithm to determine what the delete/add/update operations are going to be given a local tree and a remote tree (including an empty or a partial upload tree)\n- we need efficiency (not brute force). if we are to use checksum files for every directory/file, we need to ensure that traversal in the context of the algorithm is operationally acceptable, and so is any delete operation (which can be time consuming).","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1645478216,"metadata":{"github-id":"IC_kwDODBZtRc4-a4mj","github-url":"https://github.com/dandi/dandi-cli/issues/915#issuecomment-1047234979"},"message":"@satra  insofar we have no timings on different approaches to tell \"efficient\" from \"brute force\" apart or what those would mean.  IMHO it would indeed be more efficient to operate on tree checksums, but ATM we are trying to get away from computing those due to shortcoming which ruins upload efficiency.  Or am I missing something?  \n\n\u003e @yarikoptic\n\u003e \n\u003e \u003e overall zarr archive checksum'ing on the server will be disabled, so we should then stop checking if our upload of the zarr is consistent with the remote upon completion.\n\u003e \n\u003e Is there a dandi-archive issue for this? Aside from the contemplative changes to the API that @satra's talking about, this seems to be the current blocker for this issue.\n\nthere is https://github.com/dandi/dandi-archive/issues/912 now , but in original issue description I was not even aiming for that right away (that is why \"for now\" 3rd clause) and IMHO think now that it is not even really needed since is not the \"slow downer\" really for the uploads: besides the first batch we could pre-digest for the next batch (in a separate thread) before we request URLs for the next batch, thus eliminate any individual digestion files wait time.   That would retain that portion of the upload code. We just need to eliminate the \"entire zarr folder\" pre-digestion.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1645480489,"metadata":{"github-id":"IC_kwDODBZtRc4-a89p","github-url":"https://github.com/dandi/dandi-cli/issues/915#issuecomment-1047252841"},"message":"\u003e insofar we have no timings on different approaches to tell \"efficient\" from \"brute force\" apart or what those would mean.\n\nwe do have timings of many different kinds.\n1. with the fastio threaded approach we have timings for digesting. i think john has already shown this to be much faster.\n2. upload timing based on wrapper around s5cmd. that's the benchmark to beat. but to truly compare, we would need to disable md5 digest. s5cmd doesn't do digesting, it simply checks timestamp + filesize. this is also much faster than anything with CLI or even just vanilla api scripts (\u003e 2x as shown by jake's experiments)\n3. and we have timing for fetching the digests from s3 with s5cmd (this is also very fast compared to doing the same thing with aws cli).\n\ni have run 1 -\u003e 2-\u003e 3 on an example ngff directory. that overall timing with s5cmd is significantly faster than my previous attempt to upload with dandi cli.  caveat: because of impending needs, i have only tested upload of a fresh batch. not syncing of modified batches.\n\nmy current attention is on trying to get this data rechunked and uploaded as fast as possible, and every aspect of that process needs hypertuning. hence, i was hoping that someone else would do the comparison given all the scripts (on the hub) and data (in staging and hub).","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645540524,"metadata":{"github-id":"IC_kwDODBZtRc4-dRGT","github-url":"https://github.com/dandi/dandi-cli/issues/915#issuecomment-1047859603"},"message":"@yarikoptic So the things to do for this issue, in no particular order, are:\n\n1. Disable the initial checksumming of Zarr assets in the `upload()` function, always re-upload Zarrs as though `--existing` were set to `force`, and don't verify any digests in the Zarr metadata passed to `ZarrAsset.iter_upload()`\n    - This will require the server to fill in the Zarr's checksum in the \"digest\" field in the asset metadata; does it do that?\n2. Don't digest individual Zarr entries until the digest is actually needed, either for comparing with a remote entry at the same path of the same size or for use in the `/zarr/{zarr_id}/upload/` payload\n3. Do as much entry digestion in threads as possible\n    - How many threads?  Should this be controlled by the same setting that determines the number of upload threads per asset?  Note that, until (6) is implemented, we'd have to be running digestion threads in parallel with upload threads.\n4. Don't use fscacher when digesting local entries\n5. Don't compute the complete Zarr checksum, and don't verify the server's checksum after upload\n    - Is this blocked by anything on the dandi-archive side?\n6. (Blocked by https://github.com/dandi/dandi-archive/issues/912) Don't digest entries for the upload payload\n7. Digest entries while uploading and compare the result to the returned ETag\n    - Should this only be done as part of (6)?\n\nIs that complete/correct?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645550599,"metadata":{"github-id":"UCE_lALODBZtRc4-dRGTziEeRG8"},"target":"1ec96b5a70a93b358871d09e11f2b93bf57ba15e1f09616d3d0f782d04fb57f7","message":"@yarikoptic So the things to do for this issue, in no particular order, are:\n\n- [x] Disable the initial checksumming of Zarr assets in the `upload()` function, always re-upload Zarrs as though `--existing` were set to `force`, and don't verify any digests in the Zarr metadata passed to `ZarrAsset.iter_upload()`\n    - This will require the server to fill in the Zarr's checksum in the \"digest\" field in the asset metadata; does it do that?\n- [ ] Don't digest individual Zarr entries until the digest is actually needed, either for comparing with a remote entry at the same path of the same size or for use in the `/zarr/{zarr_id}/upload/` payload\n- [ ] Do as much entry digestion in threads as possible\n    - How many threads?  Should this be controlled by the same setting that determines the number of upload threads per asset?  Note that, until \"Optional digests\" is implemented, we'd have to be running digestion threads in parallel with upload threads.\n- [ ] Don't use fscacher when digesting local entries\n- [x] Don't compute the complete Zarr checksum, and don't verify the server's checksum after upload\n    - Is this blocked by anything on the dandi-archive side?\n- [ ] (\"Optional digests\") Don't digest entries for the upload payload (Blocked by https://github.com/dandi/dandi-archive/issues/912)\n- [ ] Digest entries while uploading and compare the result to the returned ETag\n    - Should this only be done as part of \"Optional digests\"?\n\nIs that complete/correct?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645550606,"metadata":{"github-id":"UCE_lALODBZtRc4-dRGTziEeRKE"},"target":"1ec96b5a70a93b358871d09e11f2b93bf57ba15e1f09616d3d0f782d04fb57f7","message":"@yarikoptic So the things to do for this issue, in no particular order, are:\n\n- [x] Disable the initial checksumming of Zarr assets in the `upload()` function, always re-upload Zarrs as though `--existing` were set to `force`, and don't verify any digests in the Zarr metadata passed to `ZarrAsset.iter_upload()`\n    - This will require the server to fill in the Zarr's checksum in the \"digest\" field in the asset metadata; does it do that?\n- [ ] Don't digest individual Zarr entries until the digest is actually needed, either for comparing with a remote entry at the same path of the same size or for use in the `/zarr/{zarr_id}/upload/` payload\n- [ ] Do as much entry digestion in threads as possible\n    - How many threads?  Should this be controlled by the same setting that determines the number of upload threads per asset?  Note that, until \"Optional digests\" is implemented, we'd have to be running digestion threads in parallel with upload threads.\n- [ ] Don't use fscacher when digesting local entries\n- [ ] Don't compute the complete Zarr checksum, and don't verify the server's checksum after upload\n    - Is this blocked by anything on the dandi-archive side?\n- [ ] (\"Optional digests\") Don't digest entries for the upload payload (Blocked by https://github.com/dandi/dandi-archive/issues/912)\n- [ ] Digest entries while uploading and compare the result to the returned ETag\n    - Should this only be done as part of \"Optional digests\"?\n\nIs that complete/correct?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645551099,"metadata":{"github-id":"UCE_lALODBZtRc4-dRGTziEeVWY"},"target":"1ec96b5a70a93b358871d09e11f2b93bf57ba15e1f09616d3d0f782d04fb57f7","message":"@yarikoptic So the things to do for this issue, in no particular order, are:\n\n- [x] Disable the initial checksumming of Zarr assets in the `upload()` function, always re-upload Zarrs as though `--existing` were set to `force`, and don't verify any digests in the Zarr metadata passed to `ZarrAsset.iter_upload()`\n    - This will require the server to fill in the Zarr's checksum in the \"digest\" field in the asset metadata; does it do that?\n- [ ] Don't digest individual Zarr entries until the digest is actually needed, either for comparing with a remote entry at the same path of the same size or for use in the `/zarr/{zarr_id}/upload/` payload\n- [ ] Do as much entry digestion in threads as possible\n    - How many threads?  Should this be controlled by the same setting that determines the number of upload threads per asset?  Note that, until \"Optional digests\" is implemented, we'd have to be running digestion threads in parallel with upload threads.\n- [ ] Don't use fscacher when digesting local entries\n- [x] Don't compute the complete Zarr checksum, and don't verify the server's checksum after upload\n    - Is this blocked by anything on the dandi-archive side?\n- [ ] (\"Optional digests\") Don't digest entries for the upload payload (Blocked by https://github.com/dandi/dandi-archive/issues/912)\n- [ ] Digest entries while uploading and compare the result to the returned ETag\n    - Should this only be done as part of \"Optional digests\"?\n\nIs that complete/correct?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645551345,"metadata":{"github-id":"UCE_lALODBZtRc4-dRGTziEeXPs"},"target":"1ec96b5a70a93b358871d09e11f2b93bf57ba15e1f09616d3d0f782d04fb57f7","message":"@yarikoptic So the things to do for this issue, in no particular order, are:\n\n- [x] Disable the initial checksumming of Zarr assets in the `upload()` function, always re-upload Zarrs as though `--existing` were set to `force`, and don't verify any digests in the Zarr metadata passed to `ZarrAsset.iter_upload()`\n    - This will require the server to fill in the Zarr's checksum in the \"digest\" field in the asset metadata; does it do that?\n- [ ] Don't digest individual Zarr entries until the digest is actually needed, either for comparing with a remote entry at the same path of the same size or for use in the `/zarr/{zarr_id}/upload/` payload\n- [ ] Do as much entry digestion in threads as possible\n    - How many threads?  Should this be controlled by the same setting that determines the number of upload threads per asset?  Note that, until \"Optional digests\" is implemented, we'd have to be running digestion threads in parallel with upload threads.\n    - Until \"Optional digests\" is implemented, there are two reasons why we would digest an entry: to compare it to a remote entry with the same path \u0026 size in order to determine whether to upload the entry, and to provide the digest in the upload payload.  Digestions of the first type need to be completed before starting any actual uploads if we want the upload percentages yielded by `ZarrAsset.iter_upload()` to correctly reflect the total amount of data scheduled for upload; should that happen or not?\n- [ ] Don't use fscacher when digesting local entries\n- [x] Don't compute the complete Zarr checksum, and don't verify the server's checksum after upload\n    - Is this blocked by anything on the dandi-archive side?\n- [ ] (\"Optional digests\") Don't digest entries for the upload payload (Blocked by https://github.com/dandi/dandi-archive/issues/912)\n- [ ] Digest entries while uploading and compare the result to the returned ETag\n    - Should this only be done as part of \"Optional digests\"?\n\nIs that complete/correct?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645552282,"metadata":{"github-id":"UCE_lALODBZtRc4-dRGTziEeedY"},"target":"1ec96b5a70a93b358871d09e11f2b93bf57ba15e1f09616d3d0f782d04fb57f7","message":"@yarikoptic So the things to do for this issue, in no particular order, are:\n\n- [x] Disable the initial checksumming of Zarr assets in the `upload()` function, always re-upload Zarrs as though `--existing` were set to `force`, and don't verify any digests in the Zarr metadata passed to `ZarrAsset.iter_upload()`\n    - This will require the server to fill in the Zarr's checksum in the \"digest\" field in the asset metadata; does it do that?\n- [ ] Don't digest individual Zarr entries until the digest is actually needed, either for comparing with a remote entry at the same path of the same size or for use in the `/zarr/{zarr_id}/upload/` payload\n- [ ] Do as much entry digestion in threads as possible\n    - How many threads?  Should this be controlled by the same setting that determines the number of upload threads per asset?  Note that, until \"Optional digests\" is implemented, we'd have to be running digestion threads in parallel with upload threads.\n    - Until \"Optional digests\" is implemented, there are two reasons why we would digest an entry: to compare it to a remote entry with the same path \u0026 size in order to determine whether to upload the entry, and to provide the digest in the upload payload.  Digestions of the first type need to be completed before starting any actual uploads if we want the upload percentages yielded by `ZarrAsset.iter_upload()` to correctly reflect the total amount of data scheduled for upload; should that happen or not?\n- [x] Don't use fscacher when digesting local entries\n- [x] Don't compute the complete Zarr checksum, and don't verify the server's checksum after upload\n    - Is this blocked by anything on the dandi-archive side?\n- [ ] (\"Optional digests\") Don't digest entries for the upload payload (Blocked by https://github.com/dandi/dandi-archive/issues/912)\n- [ ] Digest entries while uploading and compare the result to the returned ETag\n    - Should this only be done as part of \"Optional digests\"?\n\nIs that complete/correct?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645564736,"metadata":{"github-id":"UCE_lALODBZtRc4-dRGTziEf8Ss"},"target":"1ec96b5a70a93b358871d09e11f2b93bf57ba15e1f09616d3d0f782d04fb57f7","message":"@yarikoptic So the things to do for this issue, in no particular order, are:\n\n- [x] Disable the initial checksumming of Zarr assets in the `upload()` function, always re-upload Zarrs as though `--existing` were set to `force`, and don't verify any digests in the Zarr metadata passed to `ZarrAsset.iter_upload()`\n    - This will require the server to fill in the Zarr's checksum in the \"digest\" field in the asset metadata; does it do that?\n- [ ] Don't digest individual Zarr entries until the digest is actually needed, either for comparing with a remote entry at the same path of the same size or for use in the `/zarr/{zarr_id}/upload/` payload\n- [ ] Do as much entry digestion in threads as possible\n    - How many threads?  Should this be controlled by the same setting that determines the number of upload threads per asset?  Note that, until \"Optional digests\" is implemented, we'd have to be running digestion threads in parallel with upload threads.  (Should those run in separate thread pools or the same thread pool?)\n    - Until \"Optional digests\" is implemented, there are two reasons why we would digest an entry: to compare it to a remote entry with the same path \u0026 size in order to determine whether to upload the entry, and to provide the digest in the upload payload.  Digestions of the first type need to be completed before starting any actual uploads if we want the upload percentages yielded by `ZarrAsset.iter_upload()` to correctly reflect the total amount of data scheduled for upload; is that OK?\n- [x] Don't use fscacher when digesting local entries\n- [x] Don't compute the complete Zarr checksum, and don't verify the server's checksum after upload\n    - Is this blocked by anything on the dandi-archive side?\n- [ ] (\"Optional digests\") Don't digest entries for the upload payload (Blocked by https://github.com/dandi/dandi-archive/issues/912)\n- [ ] Digest entries while uploading and compare the result to the returned ETag\n    - Should this only be done as part of \"Optional digests\"?\n\nIs that complete/correct?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645565107,"metadata":{"github-id":"UCE_lALODBZtRc4-dRGTziEf_KA"},"target":"1ec96b5a70a93b358871d09e11f2b93bf57ba15e1f09616d3d0f782d04fb57f7","message":"@yarikoptic So the things to do for this issue, in no particular order, are:\n\n- [x] Disable the initial checksumming of Zarr assets in the `upload()` function, always re-upload Zarrs as though `--existing` were set to `force`, and don't verify any digests in the Zarr metadata passed to `ZarrAsset.iter_upload()`\n    - This will require the server to fill in the Zarr's checksum in the \"digest\" field in the asset metadata; does it do that?\n- [x] Don't digest individual Zarr entries until the digest is actually needed, either for comparing with a remote entry at the same path of the same size or for use in the `/zarr/{zarr_id}/upload/` payload\n- [ ] Do as much entry digestion in threads as possible\n    - How many threads?  Should this be controlled by the same setting that determines the number of upload threads per asset?  Note that, until \"Optional digests\" is implemented, we'd have to be running digestion threads in parallel with upload threads.  (Should those run in separate thread pools or the same thread pool?)\n    - Until \"Optional digests\" is implemented, there are two reasons why we would digest an entry: to compare it to a remote entry with the same path \u0026 size in order to determine whether to upload the entry, and to provide the digest in the upload payload.  Digestions of the first type need to be completed before starting any actual uploads if we want the upload percentages yielded by `ZarrAsset.iter_upload()` to correctly reflect the total amount of data scheduled for upload; is that OK?\n- [x] Don't use fscacher when digesting local entries\n- [x] Don't compute the complete Zarr checksum, and don't verify the server's checksum after upload\n    - Is this blocked by anything on the dandi-archive side?\n- [ ] (\"Optional digests\") Don't digest entries for the upload payload (Blocked by https://github.com/dandi/dandi-archive/issues/912)\n- [ ] Digest entries while uploading and compare the result to the returned ETag\n    - Should this only be done as part of \"Optional digests\"?\n\nIs that complete/correct?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645626244,"metadata":{"github-id":"UCE_lALODBZtRc4-dRGTziElDno"},"target":"1ec96b5a70a93b358871d09e11f2b93bf57ba15e1f09616d3d0f782d04fb57f7","message":"@yarikoptic So the things to do for this issue, in no particular order, are:\n\n- [x] Disable the initial checksumming of Zarr assets in the `upload()` function, always re-upload Zarrs as though `--existing` were set to `force`, and don't verify any digests in the Zarr metadata passed to `ZarrAsset.iter_upload()`\n    - This will require the server to fill in the Zarr's checksum in the \"digest\" field in the asset metadata; does it do that?\n- [x] Don't digest individual Zarr entries until the digest is actually needed, either for comparing with a remote entry at the same path of the same size or for use in the `/zarr/{zarr_id}/upload/` payload\n- [x] Do as much entry digestion in threads as possible\n    - How many threads?  Should this be controlled by the same setting that determines the number of upload threads per asset?  Note that, until \"Optional digests\" is implemented, we'd have to be running digestion threads in parallel with upload threads.  (Should those run in separate thread pools or the same thread pool?)\n    - Until \"Optional digests\" is implemented, there are two reasons why we would digest an entry: to compare it to a remote entry with the same path \u0026 size in order to determine whether to upload the entry, and to provide the digest in the upload payload.  Digestions of the first type need to be completed before starting any actual uploads if we want the upload percentages yielded by `ZarrAsset.iter_upload()` to correctly reflect the total amount of data scheduled for upload; is that OK?\n- [x] Don't use fscacher when digesting local entries\n- [x] Don't compute the complete Zarr checksum, and don't verify the server's checksum after upload\n    - Is this blocked by anything on the dandi-archive side?\n- [ ] (\"Optional digests\") Don't digest entries for the upload payload (Blocked by https://github.com/dandi/dandi-archive/issues/912)\n- [ ] Digest entries while uploading and compare the result to the returned ETag\n    - Should this only be done as part of \"Optional digests\"?\n\nIs that complete/correct?","files":null},{"type":5,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1645627171,"metadata":{"github-id":"LE_lADODBZtRc5D-pefzwAAAAFtG_tH"},"added":["zarr"],"removed":[]},{"type":4,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1645819120,"metadata":{"github-id":"CE_lADODBZtRc5D-pefzwAAAAFuD8YQ"},"status":2}]}