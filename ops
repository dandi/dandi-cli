{"version":2,"ops":[{"type":5,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1613672506,"metadata":{"github-id":"MDEyOkxhYmVsZWRFdmVudDQzNDgyNzU3Mzc="},"added":["UX"],"removed":[]},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1615422950,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NjMyNDI5MQ==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-796324291"},"message":"I will leave UX decision to you (a separate column or just reuse existing)","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1615422950,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDQxODUxNjM4"},"target":"d1dbf4ecaff03c8c6c878ae5aba07fb3a5fe4b4b702f736831b8af9e546179fc","message":"I will leave UX decision to you (a separate column or just reuse existing) @jwodder","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615469447,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NjczNjU5OA==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-796736598"},"message":"@yarikoptic I believe `joblib.Memory` (on which fscacher depends) uses pickle to store data, and iterators aren't pickleable.  In order to implement this, we'd need a way to query the cache to see whether it already has an entry for a given path and a way to manually insert entries in the cache, neither of which seem to be supported by `joblib`.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1615471119,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5Njc1NDcwOA==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-796754708"},"message":"perhaps use pydra instead  and store the cache directory locally somewhere.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615471421,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5Njc1ODUzNg==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-796758536"},"message":"@satra Exactly what relevant features does pydra have?  Digest progress is the easy part; caching it on the filesystem is the hard part.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1615471741,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5Njc2MjYyMw==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-796762623"},"message":"that's exactly what pydra would provide in addition to parallelization of a process :) \n\ni would suggest going through the tutorial: https://github.com/nipype/pydra-tutorial","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615477515,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NjgzMDkxNw==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-796830917"},"message":"@satra I see that that provides caching of functions, but I don't see how one would get digest progress information out of it.  Keep in mind that we're currently digesting using sha256, which isn't parallelizable, unlike the contemplated Merkel tree hash.","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615477515,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDQyMDc3NDY4"},"target":"8837fdb5fed91df78407d58a6dee1dfd3d8fde9dbb7b1234a861f355e6d32fde","message":"@satra I see that that provides caching of functions, but I don't see how one would get digest progress information out of it.  Keep in mind that we're currently digesting using sha256, which isn't parallelizable, unlike the contemplated Merkel tree hash.  Or are you recommending pydra due to it having a queryable cache?  I don't see that in the docs.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1615478214,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5Njg0MDExNA==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-796840114"},"message":"@satra FWIW pydra is relatively heavy of a dependency (and dependencies down) to use just for caching.  Even joblib is somewhat too much since we use just memoization part... Also I see it depends on cloudpickle -- is that is what it uses for pickling? its [README](https://github.com/cloudpipe/cloudpickle) warns \n\n```\nCloudpickle can only be used to send objects between the exact same version of Python.\n\nUsing cloudpickle for long-term object storage is not supported and strongly discouraged.\n```\nwhich possibly makes it suboptimal (although I am not sure how good we are now for sure anyways: https://github.com/con/fscacher/issues/36)","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1615478828,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5Njg0ODEzMw==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-796848133"},"message":"@jwodder , with all the generators it needs more thinking on how to wrap it all up ... meanwhile - having support for them in fscacher would be useful, so filed https://github.com/con/fscacher/issues/37","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1615508225,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzE0MzI3NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797143275"},"message":"perhaps i misunderstood what you meant by\n\n\u003e caching it on the filesystem is the hard part.\n\nare you saying caching the progressing state so you could start at whatever point it stopped? or are you caching the entire hash?  indeed pydra is not suited for progressive filesystem caching. but whenever you run a function, if it's completed the local cache is like a memoization on disk.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615509458,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzE1MDk1NA==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797150954"},"message":"@satra We need the cache to be a datastore mapping tuples to bytes that (a) can be freely queried \u0026 modified, not just used to dumbly memoize a function, and (b) is threading- and multiprocessing-safe.  I originally envisioned something like how joblib.Memory works, where pickled files are laid out in a directory tree based on hashes of the inputs or something like that, which would mean coming up with our own layout scheme, though now that I think about it more, we might just be able to get away with a sqlite database....","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1615511322,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzE2MTk3Mw==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797161973"},"message":"i think i'm still not fully following what the cache stores i.e what the contents of tuples are what bytes, but sounds like you have figured out a solution :)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615563696,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzU1OTU4OQ==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797559589"},"message":"@yarikoptic Regarding the pyout columns: I've renamed \"upload\" to \"pct\" and used it for both digest progress and upload progress, but it appears that the \"size\" and \"pct\" columns both stop updating after digesting is done, even though they should start counting from zero again while uploading.  Do you know why this is?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615563696,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDQyNDMzODk1"},"target":"08df6e528095315c8bd0c627ea40afce9c0abb70a5c2e4e281f8dcaf9a2ffadb","message":"@yarikoptic Regarding the pyout columns: I've renamed \"upload\" to \"pct\" and used it for both digest progress and upload progress, but it appears that the \"size\" and \"pct\" columns both stop updating after digesting is done, even though they should start counting from zero again while uploading.  Do you know why this is?\n\nEDIT: Never mind, reusing \"upload\" messed with a custom display function, so I've put the digest progress in the \"message\" column for now.","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615564575,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDQyNDM4Njk1"},"target":"08df6e528095315c8bd0c627ea40afce9c0abb70a5c2e4e281f8dcaf9a2ffadb","message":"@yarikoptic Regarding the pyout columns: I've renamed \"upload\" to \"pct\" and used it for both digest progress and upload progress, but it appears that the \"size\" and \"pct\" columns both stop updating after digesting is done, even though they should start counting from zero again while uploading.  Do you know why this is?\n\nEDIT: Never mind, reusing \"upload\" messed with a custom display function, so I've put the digest progress in the \"message\" column for now.  The downside is that this ruins the summary at below the table.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1615565123,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzU4NjU1NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797586555"},"message":"no problem -- `summary` could be customized and we could exclude progress reporting. ATM it is\n```python\n        \"message\": dict(\n            color=dict(\n                re_lookup=[[\"^exists\", \"yellow\"], [\"^(failed|error|ERROR)\", \"red\"]]\n            ),\n            aggregate=counts,\n        ),\n```\nso you could just provide a `counts_no_progress` (or just `lambda` or `filter` right there on top of counts) which would first filter entries","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1615565123,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDQyNDQxMzU5"},"target":"8a881d8b86f0a4ba15dd309298f56848cdfee9116afa9b33f4d64d7487d09562","message":"no problem -- `summary` could be customized and we could exclude progress reporting. ATM it is\n```python\n        \"message\": dict(\n            color=dict(\n                re_lookup=[[\"^exists\", \"yellow\"], [\"^(failed|error|ERROR)\", \"red\"]]\n            ),\n            aggregate=counts,\n        ),\n```\nso for `aggregate` you could just provide a `counts_no_progress` (or just `lambda` or `filter` right there on top of counts) which would first filter entries","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615565867,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzU5NTA4Mg==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797595082"},"message":"@yarikoptic I tried [this](https://github.com/dandi/dandi-cli/pull/465/commits/73aacb71e491fbfb6dec1539297349b692c675e2), but it didn't make a difference.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1615566467,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzYwMTUzOA==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797601538"},"message":"re the overall approach... I think it could also be done quite non-intrusively (although not sure if having a callback bound to Digester would have some negative side effect on fscacher) via use of the `generator_from_callback` we have and used for reporting progress from `upload` for girder backend: \n\nadd optional callback to Digester, wrap call to a digester into `generator_from_callback` and iterate it while getting the final result from `StopIteration` exception since that is where the final result is provided by the `generator_from_callback`\n```python\n                ret = func(callback)\n                raise StopIteration(ret) if ret is not None else StopIteration\n```","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1615566819,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzYwNTM4Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797605387"},"message":"\u003e @yarikoptic I tried [this](https://github.com/dandi/dandi-cli/pull/465/commits/73aacb71e491fbfb6dec1539297349b692c675e2), but it didn't make a difference.\n\ndidn't try to debug but may be because `%` is [yielded in \"status\"](https://github.com/dandi/dandi-cli/pull/465/files#diff-b338a27af45ae7897ca46a8130b866561898827c8cbdac88985370d6827a909cR148) and not \"message\" field [which you are filtering?](https://github.com/dandi/dandi-cli/pull/465/files#diff-28d9bc8a6606b4a5ea8161e32c1aa050edaf50bf81d22d5aed69917127e4ea60R176)\n\nplease also see https://github.com/dandi/dandi-cli/issues/400#issuecomment-797601538 on may be a simpler path toward needed functionality and avoiding adding a new dependency (diskcache) with a known NFS issue","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615567071,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzYwODExOA==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797608118"},"message":"@yarikoptic That was it.  The summary is still rather flickery, though.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615568158,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzYyMDA3MQ==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797620071"},"message":"@yarikoptic If I'm understanding `generator_from_callback()` correctly, you use it by creating a function `func` that takes a callback, and then you do `generator_from_callback(func)` to get an iterable of the return values from the callback.  Is that correct?  What are you envisioning as the `func` in this case?  It can't be `get_digest()`, because then the callback argument would mess with argument caching.  It can't be a function called by `get_digest()`, because then the generator would be inside `get_digest()`, and we're back to square one with the impossibility of returning an iterator from a cached function.  It can't be a function that calls `get_digest()`, because `get_digest()` doesn't return progress information and can't without being uncacheable.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1615580301,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5NzczMTY5MA==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797731690"},"message":"\u003e It can't be `get_digest()`, because then the callback argument would mess with argument caching\n\nwe can parametrize `PersistentCache` with specific `exclude_kwargs=None|iterable` (e.g. in our case `exclude_kwargs=['callback']`) to be excluded from the signature used for caching . Then callback could be passed through without affecting caching","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1615584950,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDc5Nzc2OTEwNQ==","github-url":"https://github.com/dandi/dandi-cli/issues/400#issuecomment-797769105"},"message":"@yarikoptic The good news is that joblib's Memory.cache has an `ignore` parameter that can be used to implement this.  The bad news is that the implementation does not work with fscacher's repeated function-wrapping.  [I filed a bug report.](https://github.com/joblib/joblib/issues/1164)","files":null},{"type":5,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1616599370,"metadata":{"github-id":"MDEyOkxhYmVsZWRFdmVudDQ1MDI2MTU2MjQ="},"added":["blocked"],"removed":[]}]}