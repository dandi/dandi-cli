{"version":2,"ops":[{"type":1,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1570817538,"metadata":{"github-id":"MDU6SXNzdWU1MDU5ODkxNDE=","github-url":"https://github.com/dandi/dandi-cli/issues/21","origin":"github"},"title":"CMD (idea): compress","message":"I have noted that network traffic while rcloning Svoboda's data is only about 10% of the local \"write\" IO .\n\nThat observation is confirmed by simply compressing the obtained .nwb files using tar/gz:\n```shell\nsmaug:/mnt/btrfs/datasets/datalad/crawl-misc/svoboda-rclone/Exported NWB 2.0\n$\u003e du -scm Chen\\ 2017*\n35113   Chen 2017\n3298    Chen 2017.tgz\n38410   total\n```\nso indeed -- x10 factor!\n\nApparently hdmf/pynwb does not bother compressing stored in the .nwb data arrays. They do both document ability to pass compression parameters down (to h5py I guess) though, but as far as I saw it, compression is not on by default.  Sure thing hdf5 end compression ration might not reach 10 since not all data will be compressed, but I expect that it will be notable.\n\nAs we keep running into those, it might be valuable to provide a `dandi compress` command which would take care about (re)compressing provided .nwb files (inplace or into a new file).  \nPerspective interface:\n\n    dandi compress [-i|--inplace] [-o|--output FILE] [-c|--compression METHOD (default gzip)] [-l|--level LEVEL (default 5)] [FILES]\n\n- `--inplace` to explicitly state to (re)compress each file in place (might want to do not really \"inplace\" but rather into a new file, and then replace old one -- this would provide a better workflow for git-annex'ed files, where original ones by default would be read/only)\n- `--output filename` - where to store output file (then a single FILE is expected to be provided)","files":null}]}