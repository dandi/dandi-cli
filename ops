{"version":2,"ops":[{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1680695571,"metadata":{"github-id":"IC_kwDODBZtRc5ZP9_B","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1497358273"},"message":"@colleenjg - any reason not to put them in the same nwb file ? \n\nps. we are currently debating whether we create a special folder for derivatives in the dandiset, or allow for separate dandisets, one containing raw data, and another containing data after processing (cell extractions, spikes, etc.,.)","files":null},{"type":3,"author":{"id":"23a0b490ec2a3aef708e5bf3548508030984c8bd"},"timestamp":1680697087,"metadata":{"github-id":"IC_kwDODBZtRc5ZQGcO","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1497392910"},"message":"@satra yeah - it's really been my on-going debate about whether to have multiple version of the files or just one. A lot of the compute I use is offline (clusters with offline compute nodes), so streaming isn't always an option. And in general, I haven't moved to fully remote compute - I still do a lot locally, as do a lot of my colleagues. \n\nMy full dataset is 2.2 TB with the imaging stacks, which is just a massive amount of data to download and store. It's a very manageable 15 GB without the imaging stacks, which I'm keen to keep the \"basic\" file versions on Dandi.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1680697621,"metadata":{"github-id":"IC_kwDODBZtRc5ZQJM0","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1497404212"},"message":"in that case, let's do an experiment, would you be willing to put the raw data into a separate dandiset? and then we can link the derived data to it. i can show you how at the asset level. on the dandiset level, it's going to be via resource links.","files":null},{"type":3,"author":{"id":"23a0b490ec2a3aef708e5bf3548508030984c8bd"},"timestamp":1680697685,"metadata":{"github-id":"IC_kwDODBZtRc5ZQJg9","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1497405501"},"message":"yeah! Happy to give it a try!","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1680697763,"metadata":{"github-id":"IC_kwDODBZtRc5ZQKA7","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1497407547"},"message":"@CodyCBakerPhD, @bendichter - any suggestions. there doesn't seem to be a neural datatype distinction we can make. perhaps something to consider in the schema. this has come up a few times. it's similar to how cameras store raw, raw+jpeg, jpeg.","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1680702132,"metadata":{"github-id":"IC_kwDODBZtRc5ZQi-P","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1497509775"},"message":"@satra there actually are neurodata type distinctions here. Raw ophys data has the neurodata type `TwoPhotonSeries` (or the recent addition, `OnePhotonSeries`). Processed ophys does not have these but has the data types `PlaneSegmentation`, `ImageSegmentation`, and `RoiResponseSeries`. DANDI clusters all of these into \"ophys\", but we could just as easily separate them into e.g. \"ophys_raw\" \"ophys_processed\" and  \"ophys_raw_and_processed\".\n\nBy the way, the same is true for extracellular electrophysiology. Raw has `ElectricalSeries` and procesesed does not but has `Units`. These could be split into different categories of raw and processed but currently are not.","files":null},{"type":6,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1680702177,"metadata":{"github-id":"UCE_lALODBZtRc5ZQi-Pzi-QdNA"},"target":"ff6c8c9934a51da876da1a2c972c8b7075b641d337d4f78a2e666e0be06681fc","message":"@satra there actually are neurodata type distinctions here. Raw ophys data has the neurodata type `TwoPhotonSeries` (or the recent addition, `OnePhotonSeries`). Processed ophys does not have these but has the data types `PlaneSegmentation`, `ImageSegmentation`, and `RoiResponseSeries`. dandi-cli groups all of these into \"ophys\", but we could just as easily separate them into e.g. \"ophys_raw\" \"ophys_processed\" and  \"ophys_raw_and_processed\".\n\nBy the way, the same is true for extracellular electrophysiology. Raw has `ElectricalSeries` and procesesed does not but has `Units`. These could be split into different categories of raw and processed but currently are not.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1680702319,"metadata":{"github-id":"IC_kwDODBZtRc5ZQkF_","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1497514367"},"message":"thanks @bendichter. @yarikoptic - seems like we should do a remapping. this was done very early in dandi's lifecycle. i'll open a separate issue for tackling this as well as techniques and modalities together.","files":null},{"type":3,"author":{"id":"23a0b490ec2a3aef708e5bf3548508030984c8bd"},"timestamp":1680703024,"metadata":{"github-id":"IC_kwDODBZtRc5ZQojk","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1497532644"},"message":"I think that would be really great for my use case, and hopefully many others.","files":null},{"type":3,"author":{"id":"7eea5bdd45d6b031d3bf16a241db06db985fb528"},"timestamp":1680703888,"metadata":{"github-id":"IC_kwDODBZtRc5ZQvNd","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1497559901"},"message":"One thing I will mention while endeavoring upon this experiment - please be sure that there is no difference between the 'acquisition-only' files and any derivatives in terms of the metadata structure; the only difference being the absence of the series containing the bulk data in downstream derivatives\n\nIn particular, please ensure that all of the hierarchical metadata belonging to that series (which is very lightweight) remains in any processed files; that is, imaging planes, optic channel info, devices (for ecephys; the electrode table, electrode groupings, devices)\n\nThat metadata remains very informative and valuable for interpreting any derived data, even in the absence of the actual raw series","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681137343,"metadata":{"github-id":"IC_kwDODBZtRc5ZhRZq","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501894250"},"message":"@jwodder here's the conversation we were discussing. Could you point me to the place where the mapping is made between neurodata types and filenames?","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681137492,"metadata":{"github-id":"IC_kwDODBZtRc5ZhSC-","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501896894"},"message":"@satra we also need to converge on a naming convention. I proposed \"ophys_raw\" \"ophys_processed\" and \"ophys_raw_and_processed\" but I'm not sure that's the best","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681137867,"metadata":{"github-id":"IC_kwDODBZtRc5ZhTl0","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501903220"},"message":"@bendichter \n\n\u003e Could you point me to the place where the mapping is made between neurodata types and filenames?\n\nI don't understand the question.  Can you give an example of the mapping you're referring to?","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681139127,"metadata":{"github-id":"IC_kwDODBZtRc5ZhaAC","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501929474"},"message":"@jwodder \n\ndataset contains ElectricalSeries -\u003e suffix = \"ecephys\"\ndataset contains TwoPhotonSeries -\u003e suffix contains \"ophys\"\netc.","files":null},{"type":6,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681139141,"metadata":{"github-id":"UCE_lALODBZtRc5ZhaACzi-6uSI"},"target":"25c3b2c147a5874ea6dbb7efb1f0b0739baad03dfc975893458f6c0c177451d4","message":"@jwodder \n\ndataset contains ElectricalSeries -\u003e suffix = \"ecephys\"\ndataset contains TwoPhotonSeries -\u003e suffix = \"ophys\"\netc.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681139510,"metadata":{"github-id":"IC_kwDODBZtRc5ZhcA2","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501937718"},"message":"@bendichter That's not defined by dandi anywhere.  I think those strings may come from the pynwb metadata.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1681139720,"metadata":{"github-id":"IC_kwDODBZtRc5Zhc9K","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501941578"},"message":"@jwodder - somewhere in `organize` a renaming is taking place with suffixes being added to the filename. could you please point @bendichter to that location? indeed the information about the suffixes are coming from pynwb metadata, but the name is being created somewhere in organize.","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681140198,"metadata":{"github-id":"IC_kwDODBZtRc5ZhfCA","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501950080"},"message":"I think this dictionary is used:\n\nhttps://github.com/dandi/dandi-cli/blob/cfb39beb809972f55de9fa5507610d9564c5a5aa/dandi/metadata.py#L748-L935","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681140332,"metadata":{"github-id":"IC_kwDODBZtRc5Zhf4B","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501953537"},"message":"@satra If by \"suffixes\" you mean the filename substring of the form `_WORD[+WORD][+WORD]`, that's the modalities field.  The individual modalities are extracted by [`_populate_modalities()`](https://github.com/dandi/dandi-cli/blob/cfb39beb809972f55de9fa5507610d9564c5a5aa/dandi/organize.py#L404), and they're joined together and added to the filename as part of [`_assign_dandi_names()`](https://github.com/dandi/dandi-cli/blob/cfb39beb809972f55de9fa5507610d9564c5a5aa/dandi/organize.py#L351).","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681140394,"metadata":{"github-id":"IC_kwDODBZtRc5ZhgJ8","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501954684"},"message":"@bendichter That dict is only used when converting NWB metadata to Dandi Schema asset metadata.  It plays no role in file organization.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1681140534,"metadata":{"github-id":"IC_kwDODBZtRc5ZhgtK","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501956938"},"message":"thank you @jwodder. @bendichter the populate modalities function and it's underlying call to get at the modalities from pynwb is what you are looking for at the moment.","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681140738,"metadata":{"github-id":"IC_kwDODBZtRc5Zhhli","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501960546"},"message":"oh, ok I get it. In [`get_neurodata_types_to_modalities_map`](https://github.com/dandi/dandi-cli/blob/cfb39beb809972f55de9fa5507610d9564c5a5aa/dandi/pynwb_utils.py#L133-L173\n) we are getting the module of the neurodata type and using that for the name. While it's nice to have a perfect 1-to-1 with the modules of PyNWB, there is nothing stopping us from making our names more descriptive, adding information like whether the dataset is acquisition data, processed data, or both. That would require us to manually create a map from neurodata type to name.","files":null},{"type":6,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681140784,"metadata":{"github-id":"UCE_lALODBZtRc5Zhhlizi-7Au8"},"target":"518ef985a9b5524f897e288757702cfde8ff0a7d4663390709390f206744c5f9","message":"oh, ok I get it. In [`get_neurodata_types_to_modalities_map`](https://github.com/dandi/dandi-cli/blob/cfb39beb809972f55de9fa5507610d9564c5a5aa/dandi/pynwb_utils.py#L133-L173\n) we are getting the module of the neurodata type and using that for the name. The output of this function is the dictionary we are talking about. While it's nice to have a perfect 1-to-1 with the modules of PyNWB, there is nothing stopping us from making our names more descriptive, adding information like whether the dataset is acquisition data, processed data, or both. That would require us to manually create a map from neurodata type to name.","files":null},{"type":6,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681140854,"metadata":{"github-id":"UCE_lALODBZtRc5Zhhlizi-7Blw"},"target":"518ef985a9b5524f897e288757702cfde8ff0a7d4663390709390f206744c5f9","message":"oh, ok I get it. In [`get_neurodata_types_to_modalities_map`](https://github.com/dandi/dandi-cli/blob/cfb39beb809972f55de9fa5507610d9564c5a5aa/dandi/pynwb_utils.py#L133-L173\n) we are getting the module of the neurodata type and using that for the name. The output of this function is the dictionary we are talking about. While it's nice to have a perfect 1-to-1 with the modules of PyNWB, there is nothing stopping us from making our names more descriptive, adding information like whether the dataset is acquisition data, processed data, or both. That would require us to manually create a map from neurodata type to filename \"pseuffix\".","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1681140971,"metadata":{"github-id":"IC_kwDODBZtRc5ZhijK","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501964490"},"message":"@bendichter - you can also add more info to that dictionary you pointed to in metadata.py if that helps and use it in that function. we do want to keep that suffix space somewhat compact though especially given nwb can store a lot of different types of info.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1681141003,"metadata":{"github-id":"IC_kwDODBZtRc5Zhirt","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501965037"},"message":"even better if a dictionary like that comes from pynwb itself !! :)","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681141427,"metadata":{"github-id":"IC_kwDODBZtRc5ZhkWE","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501971844"},"message":"I think what we are looking for here is a distinction between raw and processed data.\n\nOne way to do this would be to look at neurodata types. PyNWB does not make this distinction, so we would need to do it ourselves.\n\nAnother way, which is probably more accurate, is to look at the location of the neurodata object within the file, whether the data is in `/acquisition` or `/processing`. This is more accurate because there are some cases where you can have data types that would normally be considered acquisition but sometimes they are processing. For instance, an `ElectricalSeries` is usually acquisition, but if you have an LFP signal that you got through offline downsampling that you want to save to NWB, you would save that as an `ElectricalSeries` inside an `LFP` object inside of a `ProcessingModule`. I can create a function that uses the location information, but we'll need to create a naming function that is a bit more complex than a simple ndtype -\u003e name dictionary.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1681141930,"metadata":{"github-id":"IC_kwDODBZtRc5ZhmVD","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1501979971"},"message":"@bendichter - i think your knowledge of nwb will help in creating that function. i think i was saying that we can start with putting it in the CLI, but perhaps some of these nwb things could go into pynwb itself, and that way others could use it to.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1681304244,"metadata":{"github-id":"IC_kwDODBZtRc5Zt_7F","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1505230533"},"message":"@bendichter - are you going to send a PR for this? just checking so that @colleenjg can have a version before she needs to upload the data.","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681309853,"metadata":{"github-id":"IC_kwDODBZtRc5ZulHI","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1505382856"},"message":"@satra yes, I'm working on it. To do this right we would need to base the name creation on the NWB file rather than the metadata extracted from the NWB file, so it's going to take some time. I can try to have this done by end of week.","files":null},{"type":3,"author":{"id":"23a0b490ec2a3aef708e5bf3548508030984c8bd"},"timestamp":1681310702,"metadata":{"github-id":"IC_kwDODBZtRc5Zuqv-","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1505405950"},"message":"@bendichter @satra End of week would be great for me - that should give me a few days to upload the large files with the raw imaging data","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681332815,"metadata":{"github-id":"IC_kwDODBZtRc5ZwpVw","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1505924464"},"message":"How about this for a naming convention:\n\n'ecephys(acq)'\n'ecephys(proc)'\n'ecephys(acq+proc)'\n'ecephys(acq+proc)+ophys(acq)'\n'ophys(proc)+behavior(proc)'","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681333445,"metadata":{"github-id":"IC_kwDODBZtRc5Zwtqr","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1505942187"},"message":"After thinking more about it, @satra, you were right. In @colleenjg 's particular case we could have looked solely at neurodata types, but if we want to solve the problem more generally we will need to look not only at what neurodata types are present but also at where those types are in the file. Specifically, acquisition data can be found in the `/acquisition` group, and processed data can be found in the `/processing` group. The metadata object does not hold information about where the neurodata objects are in the NWB file, so I instead determined the modality section of the filename from the `NWBFile` object. I am not sure how I would incorporate this into `dandi organize`. It is not a drop-in replacement for the existing function, since the existing function uses `metadata` as input, not the opened NWBFile. Maybe we could maintain a dictionary as we do the initial metadata parsing.\n\nHere's the code:\n\n```python\nimport importlib\nimport inspect\nimport pynwb\n\nmodule_names = (\"ecephys\", \"ophys\", \"icephys\", \"image\", \"retinotopy\", \"ogen\", \"behavior\")\n\nmodule_map = dict()\nfor module_name in module_names:\n    module = importlib.import_module(\"pynwb.\" + module_name)\n    for x in module.__dict__.values():\n        if inspect.isclass(x) and \\\n        issubclass(x, pynwb.core.NWBMixin) and \\\n        x.__module__ == \"pynwb.\" + module_name:\n            module_map[x] = module_name\n            \ndef modalities_string(nwbfile: pynwb.file.NWBFile):\n    \n    def get_processing_neurodata_objects(nwbfile: pynwb.file.NWBFile):\n        for mod in nwbfile.processing.values():\n            for data_interface in mod.data_interfaces.values():\n                yield data_interface\n                \n    def check_for_module(list_of_ndobjects: list, module_name: str):\n        for ndobj in list_of_ndobjects:\n            for cls in type(ndobj).mro():\n                if module_map.get(type(ndobj), None) == module_name:\n                    return True\n                 \n    processing_neurodata_objects = list(get_processing_neurodata_objects(nwbfile))\n    \n    module_strs = []\n    for module_name in module_names:\n        \n        acq = check_for_module(nwbfile.acquisition.values(), module_name)\n        proc = check_for_module(processing_neurodata_objects, module_name)\n        \n        # handle the Units table, which is an exception to the rule.\n        if module_name == \"ecephys\" and nwbfile.units is not None:\n            proc = True\n            \n        if not (acq or proc):\n            continue\n        elif acq and not proc:\n            module_strs.append(f\"{module_name}(acq)\")\n        elif not acq and proc:\n            module_strs.append(f\"{module_name}(proc)\")\n        else:\n            module_strs.append(f\"{module_name}(acq+proc)\")\n            \n    return \"+\".join(module_strs)\n```\n\nhere are some tests:\n```python\nfrom pynwb.testing.mock.file import mock_NWBFile\nfrom pynwb.testing.mock.ecephys import mock_ElectricalSeries\nfrom pynwb.testing.mock.ophys import mock_TwoPhotonSeries, mock_OnePhotonSeries, mock_DfOverF\nfrom pynwb.testing.mock.behavior import mock_SpatialSeries\nfrom pynwb.ecephys import ElectricalSeries\n\n# ecephys acquisition\nnwbfile = mock_NWBFile()\nnwbfile.add_acquisition(mock_ElectricalSeries())\nassert modalities_string(nwbfile) == 'ecephys(acq)'\n\n# ecephys processing\nnwbfile2 = mock_NWBFile()\nproc_mod = nwbfile2.create_processing_module(\"ecephys\", \"ecephys\")\n#proc_mod.add(mock_ElectricalSeries())\nnwbfile2.add_unit(spike_times=[1.0, 2.0, 3.0])\nassert modalities_string(nwbfile2) == 'ecephys(proc)'\n\n# ecephys acquisition and processing\nnwbfile3 = mock_NWBFile()\nnwbfile3.add_acquisition(mock_ElectricalSeries())\nproc_mod = nwbfile3.create_processing_module(\"ecephys\", \"ecephys\")\nproc_mod.add(mock_ElectricalSeries())\nassert modalities_string(nwbfile3) == 'ecephys(acq+proc)'\n\n# ecephys acquisition and processing and ophys acq\nnwbfile4 = mock_NWBFile()\nnwbfile4.add_acquisition(mock_ElectricalSeries())\nnwbfile4.add_acquisition(mock_TwoPhotonSeries())\nproc_mod = nwbfile4.create_processing_module(\"ecephys\", \"ecephys\")\nproc_mod.add(mock_ElectricalSeries())\nassert modalities_string(nwbfile4) == 'ecephys(acq+proc)+ophys(acq)'\n\n# ophys processing and behavior processing\nnwbfile5 = mock_NWBFile()\nproc_mod = nwbfile5.create_processing_module(\"ophys\", \"ophys\")\nproc_mod.add(mock_DfOverF())\nproc_mod2 = nwbfile5.create_processing_module(\"behavior\", \"behavior\")\nproc_mod2.add(mock_SpatialSeries())\nassert modalities_string(nwbfile5) == 'ophys(proc)+behavior(proc)'\n```\n\n@jwodder do you have ideas about what might be the best way to incorporate this into `dandi organize`?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1681334557,"metadata":{"github-id":"IC_kwDODBZtRc5ZwzvV","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1505967061"},"message":"@bendichter - nice! perhaps replace the `(` with `-` to make filenames be a bit more terminal friendly\n\n```\n'ecephys-acq'\n'ecephys-proc'\n'ecephys-acq-proc'\n'ecephys-acq-proc+ophys-acq'\n'ophys-proc+behavior-proc'\n```\n\nalso in terms of the code, you could extract those strings when extracting metadata, and forgo the mapping function. there is a specific raw nwb metadata extractor function.","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681334773,"metadata":{"github-id":"IC_kwDODBZtRc5Zw0sC","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1505970946"},"message":"Here's a version that uses dashes instead of parens:\n\n```python\nimport importlib\nimport inspect\nimport pynwb\n\nmodule_names = (\"ecephys\", \"ophys\", \"icephys\", \"image\", \"retinotopy\", \"ogen\", \"behavior\")\n\nmodule_map = dict()\nfor module_name in module_names:\n    module = importlib.import_module(\"pynwb.\" + module_name)\n    for x in module.__dict__.values():\n        if inspect.isclass(x) and \\\n        issubclass(x, pynwb.core.NWBMixin) and \\\n        x.__module__ == \"pynwb.\" + module_name:\n            module_map[x] = module_name\n            \ndef modalities_string(nwbfile):\n    \n    def get_processing_neurodata_objects(nwbfile):\n        for mod in nwbfile.processing.values():\n            for data_interface in mod.data_interfaces.values():\n                yield data_interface\n                \n    def check_for_module(list_of_ndobjects: list, module_name: str):\n        for ndobj in list_of_ndobjects:\n            for cls in type(ndobj).mro():\n                if module_map.get(type(ndobj), None) == module_name:\n                    return True\n                 \n    processing_neurodata_objects = list(get_processing_neurodata_objects(nwbfile))\n    \n    module_strs = []\n    for module_name in module_names:\n        \n        acq = check_for_module(nwbfile.acquisition.values(), module_name)\n        proc = check_for_module(processing_neurodata_objects, module_name)\n        \n        # handle the Units table, which is an exception to the rule.\n        if module_name == \"ecephys\" and nwbfile.units is not None:\n            proc = True\n            \n        if not (acq or proc):\n            continue\n        elif acq and not proc:\n            module_strs.append(f\"{module_name}-acq\")\n        elif not acq and proc:\n            module_strs.append(f\"{module_name}-proc\")\n        else:\n            module_strs.append(f\"{module_name}-acq-proc\")\n            \n    return \"+\".join(module_strs)\n        \n```\n\ntests:\n\n```python\nfrom pynwb.testing.mock.file import mock_NWBFile\nfrom pynwb.testing.mock.ecephys import mock_ElectricalSeries\nfrom pynwb.testing.mock.ophys import mock_TwoPhotonSeries, mock_OnePhotonSeries, mock_DfOverF\nfrom pynwb.testing.mock.behavior import mock_SpatialSeries\nfrom pynwb.ecephys import ElectricalSeries\n\n# ecephys acquisition\nnwbfile = mock_NWBFile()\nnwbfile.add_acquisition(mock_ElectricalSeries())\nassert modalities_string(nwbfile) == 'ecephys-acq'\n\n# ecephys processing\nnwbfile2 = mock_NWBFile()\nproc_mod = nwbfile2.create_processing_module(\"ecephys\", \"ecephys\")\n#proc_mod.add(mock_ElectricalSeries())\nnwbfile2.add_unit(spike_times=[1.0, 2.0, 3.0])\nassert modalities_string(nwbfile2) == 'ecephys-proc'\n\n# ecephys acquisition and processing\nnwbfile3 = mock_NWBFile()\nnwbfile3.add_acquisition(mock_ElectricalSeries())\nproc_mod = nwbfile3.create_processing_module(\"ecephys\", \"ecephys\")\nproc_mod.add(mock_ElectricalSeries())\nassert modalities_string(nwbfile3) == 'ecephys-acq-proc'\n\n# ecephys acquisition and processing and ophys acq\nnwbfile4 = mock_NWBFile()\nnwbfile4.add_acquisition(mock_ElectricalSeries())\nnwbfile4.add_acquisition(mock_TwoPhotonSeries())\nproc_mod = nwbfile4.create_processing_module(\"ecephys\", \"ecephys\")\nproc_mod.add(mock_ElectricalSeries())\nassert modalities_string(nwbfile4) == 'ecephys-acq-proc+ophys-acq'\n\n# ophys processing and behavior processing\nnwbfile5 = mock_NWBFile()\nproc_mod = nwbfile5.create_processing_module(\"ophys\", \"ophys\")\nproc_mod.add(mock_DfOverF())\nproc_mod2 = nwbfile5.create_processing_module(\"behavior\", \"behavior\")\nproc_mod2.add(mock_SpatialSeries())\nassert modalities_string(nwbfile5) == 'ophys-proc+behavior-proc'\n\n```","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681390782,"metadata":{"github-id":"IC_kwDODBZtRc5Z0db_","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1506924287"},"message":"@bendichter Is this code supposed to completely replace the current modalities calculation or just augment it somehow?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681391198,"metadata":{"github-id":"UCE_lALODBZtRc5Z0db_zi_m5A8"},"target":"799592939c8718f864442143ead7b024ac62a771300263ff421877578a663221","message":"@bendichter Is this code supposed to completely replace the current modalities calculation or just augment it somehow?  Specifically, is the goal to append `-acq` and/or `-proc` to every modality value in organized NWB files?","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681395775,"metadata":{"github-id":"IC_kwDODBZtRc5Z0_D0","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1507062004"},"message":"\u003e Is this code supposed to completely replace the current modalities calculation or just augment it somehow?\n\nThis is meant to replace a section of [`_assign_dandi_names`](https://github.com/dandi/dandi-cli/blob/cfb39beb809972f55de9fa5507610d9564c5a5aa/dandi/organize.py#L351-L377), establishing a new method for creating the modalities section of filenames.\n\n\u003e Specifically, is the goal to append -acq and/or -proc to every modality value in organized NWB files?\n\nYes, that's the goal.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681395855,"metadata":{"github-id":"IC_kwDODBZtRc5Z0_kO","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1507064078"},"message":"@bendichter I would start updating the code as follows:\n\n```diff\ndiff --git a/dandi/metadata.py b/dandi/metadata.py\nindex 2597a32..2b126ef 100644\n--- a/dandi/metadata.py\n+++ b/dandi/metadata.py\n@@ -53,6 +53,7 @@ lgr = get_logger()\n def get_metadata(\n     path: str | Path | Readable,\n     digest: Optional[Digest] = None,\n+    modalities: bool = False,\n ) -\u003e Optional[dict]:\n     \"\"\"\n     Get \"flatdata\" from a .nwb file\n@@ -127,7 +128,7 @@ def get_metadata(\n         tried_imports = set()\n         while True:\n             try:\n-                meta.update(_get_pynwb_metadata(r))\n+                meta.update(_get_pynwb_metadata(r, modalities=modalities))\n                 break\n             except KeyError as exc:  # ATM there is\n                 lgr.debug(\"Failed to read %s: %s\", r, exc)\ndiff --git a/dandi/organize.py b/dandi/organize.py\nindex 32dcadc..8894d14 100644\n--- a/dandi/organize.py\n+++ b/dandi/organize.py\n@@ -108,9 +108,6 @@ def create_unique_filenames_from_metadata(\n     # Additional fields\n     #\n \n-    # Add \"modalities\" composed from the ones we could deduce\n-    _populate_modalities(metadata)\n-\n     # handle cases where session_id was not provided\n     # In some of those we could have session_start_time, so we could produce\n     # session_id based on those\n@@ -401,28 +398,6 @@ def _sanitize_value(value, field):\n     return value\n \n \n-def _populate_modalities(metadata):\n-    ndtypes_to_modalities = get_neurodata_types_to_modalities_map()\n-    ndtypes_unassigned = set()\n-    for r in metadata:\n-        mods = set()\n-        nd_types = r.get(\"nd_types\", [])\n-        if isinstance(nd_types, str):\n-            nd_types = nd_types.split(\",\")\n-        for nd_rec in nd_types:\n-            # split away the count\n-            ndtype = nd_rec.split()[0]\n-            mod = ndtypes_to_modalities.get(ndtype, None)\n-            if mod:\n-                if mod not in (\"base\", \"device\", \"file\", \"misc\"):\n-                    # skip some trivial/generic ones\n-                    mods.add(mod)\n-            else:\n-                ndtypes_unassigned.add(ndtype)\n-        # tuple so we could easier figure out \"unique\" values below\n-        r[\"modalities\"] = tuple(sorted(mods.union(set(r.get(\"modalities\", {})))))\n-\n-\n def _populate_session_ids_from_time(metadata):\n     ses_times = [m.get(\"session_start_time\", None) for m in metadata]\n     if not all(ses_times):\n@@ -799,7 +774,7 @@ def organize(\n \n         def _get_metadata(path):\n             try:\n-                meta = get_metadata(path)\n+                meta = get_metadata(path, modalities=True)\n             except Exception as exc:\n                 meta = {}\n                 failed.append(path)\ndiff --git a/dandi/pynwb_utils.py b/dandi/pynwb_utils.py\nindex 5e1afff..d843db5 100644\n--- a/dandi/pynwb_utils.py\n+++ b/dandi/pynwb_utils.py\n@@ -202,7 +202,7 @@ def _scan_neurodata_types(grp: h5py.File) -\u003e List[Tuple[Any, Any]]:\n     return out\n \n \n-def _get_pynwb_metadata(path: str | Path | Readable) -\u003e dict[str, Any]:\n+def _get_pynwb_metadata(path: str | Path | Readable, modalities: bool = False) -\u003e dict[str, Any]:\n     out = {}\n     with open_readable(path) as fp, h5py.File(fp) as h5, NWBHDF5IO(\n         file=h5, load_namespaces=True\n@@ -255,6 +255,9 @@ def _get_pynwb_metadata(path: str | Path | Readable) -\u003e dict[str, Any]:\n         # get external_file data:\n         out[\"external_file_objects\"] = _get_image_series(nwb)\n \n+        if modalities:\n+            out[\"modalities\"] = modalities_string(nwb)\n+\n     return out\n \n \n@@ -554,3 +557,7 @@ def open_readable(r: str | Path | Readable) -\u003e IO[bytes]:\n         return r.open()\n     else:\n         return open(r, \"rb\")\n+\n+\n+def modalities_string(nwbfile: pynwb.NWBFile) -\u003e str:\n+    ### Your code here ###\n```","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1681744168,"metadata":{"github-id":"IC_kwDODBZtRc5aGIHY","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1511555544"},"message":"Sorry for the delay in joining this conversation -- was traveling etc.  My thinking:\n\n- `dandi organize` if first of all just a helper!  If it does not arrive to the final desired file naming, there is always a simple `mv` (or other command/interface) to rename file(s) further to the desired naming without needing to await for some logic to be implemented in `dandi organize`.  Moreover, since dandi archive is smart enough to not require reupload of the same blobs, and there is even `dandi move` command to rename files locally/in the archive -- I would recommend to proceed with upload of the .nwb files the content of which you do not expect to change as soon as you have a viable naming convention (and possibly improve/rename later). Relevant here:\n   - As discussed above -- it is recommended to store \"raw\" data in separate files or even dandisets.\n- the main body of discussion seems to be about filename annotation of having processed data in the dandiset/files.  @bendichter and @satra exercised above ideas on extending our ad-hoc DANDI filenaming/layout with placing more of metadata into \"modalities\" suffix of the filename.  Although looks nice on the initial sight, I do not think it is a good idea:\n  - DANDI layout (result of `dandi organize`) was introduced to largely mimic BIDS layout. \n  - **DANDI layout is not really \"standardized\" beyond what is implemented in `dandi organize` and as an archive we should avoid leading a standardization effort, and should strive the balance between remaining flexible to suite users' use cases while adopting (and influencing) standards which exist or being developed**\n  - DANDI layout filename is `sub-\u003clabel\u003e_ses-\u003clabel\u003e[_\u003centity\u003e-\u003cvalue\u003e]*_\u003cmodalities\u003e.\u003cext\u003e` at large mimics BIDS and `entities` we accept do largely come from BIDS and we work with BIDS and  BEPs to introduce new. \n  - `_\u003centity\u003e-\u003cvalue\u003e` are there to be added to filename to provide extra \"hints\" (not really to encode metadata values) on how two or more files differ. `dandi organize` logic \"automates\" and \"accents\" on above - it adds an \"_\u003centity\u003e-...\" (unless explicitly listed in `--required-field`) only if such entity differs across files  \n  - `_\u003cmodalities\u003e` was introduced also to mimic BIDS suffixies but since NWB allow for multiple modalities to be present in the same file, we allowed for composition (thus `+` between different modalities)\n  - BIDS has a number of \"entities\" which were introduced to \"hint\" that/how files differ based on processing done\n     - during data acquisition (by hardware) , e.g. `_rec-` (for \"reconstruction\") and/or `_proc-`  (for \"processing\").  Also `_acq-` is there to \"note\" on differences in acquisition (e.g. parameters). Again, just a \"note\" and details should be contained inside files, not filenames\n     - after having being acquired: [`_desc-` (for \"description\")](https://bids-specification.readthedocs.io/en/stable/appendices/entities.html#desc) as a generic entity to apply to derived data files.\n  - I would not expect two files which would have somewhat duplicate/conflicting combinations like  `_ecephys-acq` and `_ecephys-acq-proc`. Indeed there could be different set of \"acq/proc\" across different modalities, but I would expect it to be consistent across files within  subject/session (e.g. not to have in one `_ecephys-acq` and another `_ecephys-acq-proc`).\n  - So it feels natural to me for us to **extend with *support* of `_proc` and `_desc-` entities to allow users to describe difference between two files in case of files containing derived/processed data** and for `dandi organize` make use of either or both of them instead of coming up with an additional ad-hoc way to extend \"_\u003cmodalities\u003e\" to become a composite \"modalities and processing hints\" (wherever we use `_entity-value` for the \"hints\"). Adding yet another \"ad-hoc\" annotation within  \n\nIf we agree that we should just add `_proc-` and/or `_desc-`, then we can discuss on how `organize` could/should make use of them (e.g. may be via looking at the unique sets of \"modality + processing\" parametrizations etc).","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1681744268,"metadata":{"github-id":"UCE_lALODBZtRc5aGIHYzjAKsag"},"target":"1ce521cc97fa2c79c47d6b40883545259e8e8138901030a603ea205b3427c035","message":"Sorry for the delay in joining this conversation -- was traveling etc.  My thinking:\n\n- `dandi organize` if first of all just a helper!  If it does not arrive to the final desired file naming, there is always a simple `mv` (or other command/interface) to rename file(s) further to the desired naming without needing to await for some logic to be implemented in `dandi organize`.  Moreover, since dandi archive is smart enough to not require reupload of the same blobs, and there is even `dandi move` command to rename files locally/in the archive -- I would recommend to proceed with upload of the .nwb files the content of which you do not expect to change as soon as you have a viable naming convention (and possibly improve/rename later). Relevant here:\n   - As discussed above -- it is recommended to store \"raw\" data in separate files or even dandisets.\n- the main body of discussion seems to be about filename annotation of having processed data in the dandiset/files.  @bendichter and @satra exercised above ideas on extending our ad-hoc DANDI filenaming/layout with placing more of metadata into \"modalities\" suffix of the filename.  Although looks nice on the initial sight, I do not think it is a good idea:\n  - DANDI layout (result of `dandi organize`) was introduced to largely mimic BIDS layout. \n  - **DANDI layout is not really \"standardized\" beyond what is implemented in `dandi organize` and as an archive we should avoid leading a standardization effort, and should strive the balance between remaining flexible to suite users' use cases while adopting (and influencing) standards which exist or being developed**\n  - DANDI layout filename is `sub-\u003clabel\u003e_ses-\u003clabel\u003e[_\u003centity\u003e-\u003cvalue\u003e]*_\u003cmodalities\u003e.\u003cext\u003e` at large mimics BIDS and `entities` we accept do largely come from BIDS and we work with BIDS and  BEPs to introduce new. \n  - `_\u003centity\u003e-\u003cvalue\u003e` are there to be added to filename to provide extra \"hints\" (not really to encode metadata values) on how two or more files differ. `dandi organize` logic \"automates\" and \"accents\" on above - it adds an `_\u003centity\u003e-\u003cvalue\u003e` (unless explicitly listed in `--required-field`) only if such \"entity\" differs across files  \n  - `_\u003cmodalities\u003e` was introduced also to mimic BIDS suffixies but since NWB allow for multiple modalities to be present in the same file, we allowed for composition (thus `+` between different modalities)\n  - BIDS has a number of \"entities\" which were introduced to \"hint\" that/how files differ based on processing done\n     - during data acquisition (by hardware) , e.g. `_rec-` (for \"reconstruction\") and/or `_proc-`  (for \"processing\").  Also `_acq-` is there to \"note\" on differences in acquisition (e.g. parameters). Again, just a \"note\" and details should be contained inside files, not filenames\n     - after having being acquired: [`_desc-` (for \"description\")](https://bids-specification.readthedocs.io/en/stable/appendices/entities.html#desc) as a generic entity to apply to derived data files.\n  - I would not expect two files which would have somewhat duplicate/conflicting combinations like  `_ecephys-acq` and `_ecephys-acq-proc`. Indeed there could be different set of \"acq/proc\" across different modalities, but I would expect it to be consistent across files within  subject/session (e.g. not to have in one `_ecephys-acq` and another `_ecephys-acq-proc`).\n  - So it feels natural to me for us to **extend with *support* of `_proc` and `_desc-` entities to allow users to describe difference between two files in case of files containing derived/processed data** and for `dandi organize` make use of either or both of them instead of coming up with an additional ad-hoc way to extend \"_\u003cmodalities\u003e\" to become a composite \"modalities and processing hints\" (wherever we use `_entity-value` for the \"hints\"). Adding yet another \"ad-hoc\" annotation within  \n\nIf we agree that we should just add `_proc-` and/or `_desc-`, then we can discuss on how `organize` could/should make use of them (e.g. may be via looking at the unique sets of \"modality + processing\" parametrizations etc).","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1681744298,"metadata":{"github-id":"UCE_lALODBZtRc5aGIHYzjAKs-c"},"target":"1ce521cc97fa2c79c47d6b40883545259e8e8138901030a603ea205b3427c035","message":"Sorry for the delay in joining this conversation -- was traveling etc.  My thinking:\n\n- `dandi organize` if first of all just a helper!  If it does not arrive to the final desired file naming, there is always a simple `mv` (or other command/interface) to rename file(s) further to the desired naming without needing to await for some logic to be implemented in `dandi organize`.  Moreover, since dandi archive is smart enough to not require reupload of the same blobs, and there is even `dandi move` command to rename files locally/in the archive -- I would recommend to proceed with upload of the .nwb files the content of which you do not expect to change as soon as you have a viable naming convention (and possibly improve/rename later). Relevant here:\n   - As discussed above -- it is recommended to store \"raw\" data in separate files or even dandisets.\n- the main body of discussion seems to be about filename annotation of having processed data in the dandiset/files.  @bendichter and @satra exercised above ideas on extending our ad-hoc DANDI filenaming/layout with placing more of metadata into \"modalities\" suffix of the filename.  Although looks nice on the initial sight, I do not think it is a good idea:\n  - DANDI layout (result of `dandi organize`) was introduced to largely mimic BIDS layout. \n  - **DANDI layout is not really \"standardized\" beyond what is implemented in `dandi organize` and as an archive we should avoid leading a standardization effort, and should strive the balance between remaining flexible to suite users' use cases while adopting (and influencing) standards which exist or being developed**\n  - DANDI layout filename is `sub-\u003clabel\u003e_ses-\u003clabel\u003e[_\u003centity\u003e-\u003cvalue\u003e]*_\u003cmodalities\u003e.\u003cext\u003e` at large mimics BIDS and `entities` we accept do largely come from BIDS and we work with BIDS and  BEPs to introduce new. \n  - `_\u003centity\u003e-\u003cvalue\u003e` are there to be added to filename to provide extra \"hints\" (not really to encode metadata values) on how two or more files differ. `dandi organize` logic \"automates\" and \"accents\" on above - it adds an `_\u003centity\u003e-\u003cvalue\u003e` (unless explicitly listed in `--required-field`) only if such \"entity\" differs across files  \n  - `_\u003cmodalities\u003e` was introduced also to mimic BIDS suffixies but since NWB allow for multiple modalities to be present in the same file, we allowed for composition (thus `+` between different modalities)\n  - BIDS has a number of \"entities\" which were introduced to \"hint\" that/how files differ based on processing done\n     - during data acquisition (by hardware) , e.g. `_rec-` (for \"reconstruction\") and/or `_proc-`  (for \"processing\").  Also `_acq-` is there to \"note\" on differences in acquisition (e.g. parameters). Again, just a \"note\" and details should be contained inside files, not filenames\n     - after having being acquired: [`_desc-` (for \"description\")](https://bids-specification.readthedocs.io/en/stable/appendices/entities.html#desc) as a generic entity to apply to derived data files.\n  - I would not expect two files which would have somewhat duplicate/conflicting combinations like  `_ecephys-acq` and `_ecephys-acq-proc`. Indeed there could be different set of \"acq/proc\" across different modalities, but I would expect it to be consistent across files within  subject/session (e.g. not to have in one `_ecephys-acq` and another `_ecephys-acq-proc`).\n  - So it feels natural to me for us to **extend with *support* of `_proc` and `_desc-` entities to allow users to describe difference between two files in case of files containing derived/processed data** and for `dandi organize` make use of either or both of them instead of coming up with an additional ad-hoc way to extend `_\u003cmodalities\u003e` to become a composite \"modalities and processing hints\" (wherever we use `_entity-value` for the \"hints\"). Adding yet another \"ad-hoc\" annotation within  \n\nIf we agree that we should just add `_proc-` and/or `_desc-`, then we can discuss on how `organize` could/should make use of them (e.g. may be via looking at the unique sets of \"modality + processing\" parametrizations etc).","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1681744317,"metadata":{"github-id":"UCE_lALODBZtRc5aGIHYzjAKtWs"},"target":"1ce521cc97fa2c79c47d6b40883545259e8e8138901030a603ea205b3427c035","message":"Sorry for the delay in joining this conversation -- was traveling etc.  My thinking:\n\n- `dandi organize` if first of all just a helper!  If it does not arrive to the final desired file naming, there is always a simple `mv` (or other command/interface) to rename file(s) further to the desired naming without needing to await for some logic to be implemented in `dandi organize`.  Moreover, since dandi archive is smart enough to not require reupload of the same blobs, and there is even `dandi move` command to rename files locally/in the archive -- I would recommend to proceed with upload of the .nwb files the content of which you do not expect to change as soon as you have a viable naming convention (and possibly improve/rename later). Relevant here:\n   - As discussed above -- it is recommended to store \"raw\" data in separate files or even dandisets.\n- the main body of discussion seems to be about filename annotation of having processed data in the dandiset/files.  @bendichter and @satra exercised above ideas on extending our ad-hoc DANDI filenaming/layout with placing more of metadata into \"modalities\" suffix of the filename.  Although looks nice on the initial sight, I do not think it is a good idea:\n  - DANDI layout (result of `dandi organize`) was introduced to largely mimic BIDS layout. \n  - **DANDI layout is not really \"standardized\" beyond what is implemented in `dandi organize` and as an archive we should avoid leading a standardization effort, and should strive the balance between remaining flexible to suite users' use cases while adopting (and influencing) standards which exist or being developed**\n  - DANDI layout filename is `sub-\u003clabel\u003e_ses-\u003clabel\u003e[_\u003centity\u003e-\u003cvalue\u003e]*_\u003cmodalities\u003e.\u003cext\u003e` at large mimics BIDS and `entities` we accept do largely come from BIDS and we work with BIDS and  BEPs to introduce new. \n  - `_\u003centity\u003e-\u003cvalue\u003e` are there to be added to filename to provide extra \"hints\" (not really to encode metadata values) on how two or more files differ. `dandi organize` logic \"automates\" and \"accents\" on above - it adds an `_\u003centity\u003e-\u003cvalue\u003e` (unless explicitly listed in `--required-field`) only if such \"entity\" differs across files  \n  - `_\u003cmodalities\u003e` was introduced also to mimic BIDS suffixies but since NWB allow for multiple modalities to be present in the same file, we allowed for composition (thus `+` between different modalities)\n  - BIDS has a number of \"entities\" which were introduced to \"hint\" that/how files differ based on processing done\n     - during data acquisition (by hardware) , e.g. `_rec-` (for \"reconstruction\") and/or `_proc-`  (for \"processing\").  Also `_acq-` is there to \"note\" on differences in acquisition (e.g. parameters). Again, just a \"note\" and details should be contained inside files, not filenames\n     - after having being acquired: [`_desc-` (for \"description\")](https://bids-specification.readthedocs.io/en/stable/appendices/entities.html#desc) as a generic entity to apply to derived data files.\n  - I would not expect two files which would have somewhat duplicate/conflicting combinations like  `_ecephys-acq` and `_ecephys-acq-proc`. Indeed there could be different set of \"acq/proc\" across different modalities, but I would expect it to be consistent across files within  subject/session (e.g. not to have in one `_ecephys-acq` and another `_ecephys-acq-proc`).\n  - So it feels natural to me for us to **extend with *support* of `_proc` and `_desc-` entities to allow users to describe difference between two files in case of files containing derived/processed data** and for `dandi organize` make use of either or both of them instead of coming up with an additional ad-hoc way to extend `_\u003cmodalities\u003e` to become a composite \"modalities and processing hints\" (wherever we use `_entity-value` for the \"hints\").\n\nIf we agree that we should just add `_proc-` and/or `_desc-`, then we can discuss on how `organize` could/should make use of them (e.g. may be via looking at the unique sets of \"modality + processing\" parametrizations etc).","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1681744340,"metadata":{"github-id":"UCE_lALODBZtRc5aGIHYzjAKtwE"},"target":"1ce521cc97fa2c79c47d6b40883545259e8e8138901030a603ea205b3427c035","message":"Sorry for the delay in joining this conversation -- was traveling etc.  My thinking:\n\n- `dandi organize` if first of all just a helper!  If it does not arrive to the final desired file naming, there is always a simple `mv` (or other command/interface) to rename file(s) further to the desired naming without needing to await for some logic to be implemented in `dandi organize`.  Moreover, since dandi archive is smart enough to not require reupload of the same blobs, and there is even `dandi move` command to rename files locally/in the archive -- I would recommend to proceed with upload of the .nwb files the content of which you do not expect to change as soon as you have a viable naming convention (and possibly improve/rename later). Relevant here:\n   - As discussed above -- it is recommended to store \"raw\" data in separate files or even dandisets.\n- the main body of discussion seems to be about filename annotation of having processed data in the dandiset/files.  @bendichter and @satra exercised above ideas on extending our ad-hoc DANDI filenaming/layout with placing more of metadata into \"modalities\" suffix of the filename.  Although looks nice on the initial sight, I do not think it is a good idea:\n  - DANDI layout (result of `dandi organize`) was introduced to largely mimic BIDS layout. \n  - **DANDI layout is not really \"standardized\" beyond what is implemented in `dandi organize` and as an archive we should avoid leading a standardization effort, and should strive the balance between remaining flexible to suite users' use cases while adopting (and influencing) standards which exist or being developed**\n  - DANDI layout filename is `sub-\u003clabel\u003e_ses-\u003clabel\u003e[_\u003centity\u003e-\u003cvalue\u003e]*_\u003cmodalities\u003e.\u003cext\u003e` at large mimics BIDS and `entities` we accept do largely come from BIDS and we work with BIDS and  BEPs to introduce new. \n  - `_\u003centity\u003e-\u003cvalue\u003e` are there to be added to filename to provide extra \"hints\" (not really to encode metadata values) on how two or more files differ. `dandi organize` logic \"automates\" and \"accents\" on above - it adds an `_\u003centity\u003e-\u003cvalue\u003e` (unless explicitly listed in `--required-field`) only if such \"entity\" differs across files  \n  - `_\u003cmodalities\u003e` was introduced also to mimic BIDS suffixies but since NWB allow for multiple modalities to be present in the same file, we allowed for composition (thus `+` between different modalities)\n  - BIDS has a number of \"entities\" which were introduced to \"hint\" that/how files differ based on processing done\n     - during data acquisition (by hardware) , e.g. `_rec-` (for \"reconstruction\") and/or `_proc-`  (for \"processing\").  Also `_acq-` is there to \"note\" on differences in acquisition (e.g. parameters). Again, just a \"note\" and details should be contained inside files, not filenames\n     - after having being acquired: [`_desc-` (for \"description\")](https://bids-specification.readthedocs.io/en/stable/appendices/entities.html#desc) as a generic entity to apply to derived data files.\n  - I would not expect two files which would have somewhat duplicate/conflicting combinations like  `_ecephys-acq` and `_ecephys-acq-proc`. Indeed there could be different set of \"acq/proc\" across different modalities, but I would expect it to be consistent across files within  subject/session (e.g. not to have in one `_ecephys-acq` and another `_ecephys-acq-proc`).\n  - So it feels natural to me for us to **extend with *support* of `_proc` and `_desc-` entities to allow users to describe difference between files in case of files containing derived/processed data** and for `dandi organize` make use of either or both of them instead of coming up with an additional ad-hoc way to extend `_\u003cmodalities\u003e` to become a composite \"modalities and processing hints\" (wherever we use `_entity-value` for the \"hints\").\n\nIf we agree that we should just add `_proc-` and/or `_desc-`, then we can discuss on how `organize` could/should make use of them (e.g. may be via looking at the unique sets of \"modality + processing\" parametrizations etc).","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681744696,"metadata":{"github-id":"IC_kwDODBZtRc5aGM8b","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1511575323"},"message":"@yarikoptic Note that the code for validating that asset paths conform to `dandi organize` currently requires modalities to consist of just lowercase ASCII letters, so a user trying to upload a file with a `+` or `-` in the modality would currently get an error.  (They would still be able to use `dandi move` to rename assets on the server to use their desired format, as validation is not run on `move`.)","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681745079,"metadata":{"github-id":"UCE_lALODBZtRc5aGM8bzjAK6-I"},"target":"c7f4699df4f2720593b1033e96e34fa15bee4a6f6853770540e2185ae774be4a","message":"@yarikoptic Note that the code for validating that asset paths conform to `dandi organize` currently requires modalities to consist of just lowercase ASCII letters, so a user trying to upload a file with a `-` in the modality would currently get an error.  (They would still be able to use `dandi move` to rename assets on the server to use their desired format, as validation is not run on `move`.)","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1681744981,"metadata":{"github-id":"IC_kwDODBZtRc5aGPL7","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1511584507"},"message":"@yarikoptic - i think the challenge here that is different from bids is that the same file may have multiple types of data, each in raw + proc form, so there is still a need of modality specific information, which i don't think can easily be added in `_proc` or `_desc`. i agree with the notion that we shouldn't have `acq+proc` and `acq` or `proc` in the same file.\n\nwhat happens with datasets that only share spike times? are you going to disallow it? \n\n\u003e he code for validating that asset paths conform to dandi organize currently requires modalities to consist of just lowercase ASCII letters\n\n@jwodder - if that's the case dandi cli has changed. as we currently should have support for `+` in the modalities string. there are many dandisets with multiple modalities in nwb files that create that suffix.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681745070,"metadata":{"github-id":"IC_kwDODBZtRc5aGPxV","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1511586901"},"message":"@satra The `+` is a modality separator, which the validation supports.  The restriction to lowercase letters applies to the individual modalities.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1681754518,"metadata":{"github-id":"IC_kwDODBZtRc5aHOQs","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1511842860"},"message":"\u003e @yarikoptic - i think the challenge here that is different from bids is that the same file may have multiple types of data, each in raw + proc form, so there is still a need of modality specific information, which i don't think can easily be added in `_proc` or `_desc`.\n\nwhy? `_desc-raw+proc`, if not as descriptive in some cases, is just fine/valid.  If you want to become more specific, can even be `_desc-allphys-rawproc+beh-deeplabcut` or alike, or in other words -- as descriptive as you like if you really want to annotate per modality, or just `_desc-preproc+curated1` as a description of preprocessed and curated data for all modalities present etc.\n\nThe point IMHO is that if we really want to add per-modality entities, we need to come up with some generic approach generalizing to other entities etc, not just limit it only for `proc` and `raw` \"hints\".  But as it would render filenames likely unusable, IMHO `_desc-` is just fine.\n\n\u003e  i agree with the notion that we shouldn't have `acq+proc` and `acq` or `proc` in the same file.\n\u003e \n\u003e what happens with datasets that only share spike times? are you going to disallow it?\n\nnope -- are those spikes for ecephys? then `_desc-spikes_ecephys` or alike.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1681754702,"metadata":{"github-id":"IC_kwDODBZtRc5aHP1M","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1511849292"},"message":"\u003e @yarikoptic Note that the code for validating that asset paths conform to `dandi organize` currently requires modalities to consist of just lowercase ASCII letters, so a user trying to upload a file with a `-` in the modality would currently get an error.\n\nThanks!  happy to hear that we made it so specific -- it is easier to relax than to make more restrictive.","files":null},{"type":3,"author":{"id":"f068089e26f6e2e30d51a62e75a77af7d9dc5ecb"},"timestamp":1681761811,"metadata":{"github-id":"IC_kwDODBZtRc5aH2Sz","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1512006835"},"message":"\u003e I agree with the notion that we shouldn't have acq+proc and acq or proc in the same file.\n\n@satra just so I understand, are you talking here about the file naming convention, or are you saying that you think the NWB file itself should not mix acquisition and processing data between modalities?\n\n@yarik, Let me see if I understand some of your points:\n1. Putting DANDI-specific naming conventions inside of `_desc-` allows us to better comply with existing BIDS naming convention rules.\n\nI don't see a problem with that on my end. \n\n2. These `_desc-` fields do not need to follow any strict automatically applied rules and could be manually specified by the user. \n\nI don't see a problem with that from my end either. In fact, this alone could solve @colleenjg's problem, as she would be able to add `_desc-` to modify the filenames of all NWB files and include the \"acquired\"/\"processed\" metadata she desires. It would require a little custom script that parses the NWB files and adds `_desc-` the way she wants. This approach is flexible enough to handle lots of other types of problems we may run up against and could provide a good compromise between standardization (the existing modalities string) and customization (in desc) while providing better compliance with BIDS. One downside of this approach is that this would be user-specific and would therefore not be generally applied to all DANDI users. It would also require an additional step. Perhaps a compromise would be to be able to optionally supply a user-specified function for the desc string. Something like:\n\n```\ndandi -desc extract_desc.py organize .\n```\n\n```python\n#  extract_desc.py\nfunction extract_desc(nwbfile):\n    if ...:\n        return \"acq\"\n    elif ...:\n        return \"proc\"\n    else:\n        return \"\"\n```\n\nbut I'm not sure if this would really be better than simply creating a custom script that iterates over and renames the files. Something like:\n\n```python\nfrom glob import glob\nfrom pynwb import NWBHDF5IO\nimport os\n\ndesc_dict = dict()\n\nfor nwb_filepath in glob(\"data/*/**.nwb\", recursive=True):\n    with NWBHDF5IO(nwb_filepath, mode=\"r\", load_namespaces=True) as io:\n        nwbfile = io.read()\n        desc_dict[nwb_filepath] = extract_desc(nwbfile)\n\nfor src, desc in desc_dict.items():\n    os.rename(src, src[:-4] + desc + \".nwb\")\n```\n\nIs this along the lines of what you were thinking?","files":null},{"type":3,"author":{"id":"23a0b490ec2a3aef708e5bf3548508030984c8bd"},"timestamp":1681765159,"metadata":{"github-id":"IC_kwDODBZtRc5aIHZa","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1512076890"},"message":"@satra and @yarikoptic  Given the time crunch I'm under, I think I'll go ahead and tag the files with the raw ophys data with something like `+raw`. Then my download code can identify files that have that string in their names and selectively download or ignore them.\n\nDo you have a preference for what string I might use? I aim to do this tomorrow (Tuesday) at the latest.","files":null},{"type":6,"author":{"id":"23a0b490ec2a3aef708e5bf3548508030984c8bd"},"timestamp":1681765210,"metadata":{"github-id":"UCE_lALODBZtRc5aIHZazjAPmyY"},"target":"1bfcba7e8a83b2270c65108b554b6ca9e24a7b6e1980d993a6e2cf048a546073","message":"@satra and @yarikoptic  Given the time crunch I'm under, I think I'll go ahead and tag the files with the raw ophys data with something like `+raw`. This would fit well with my download code for the dataset that looks to identify files based on whether certain strings occur or do not occur in their names.\n\nDo you have a preference for what string I might use? I aim to do this tomorrow (Tuesday) at the latest.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1681775248,"metadata":{"github-id":"IC_kwDODBZtRc5aIwME","github-url":"https://github.com/dandi/dandi-cli/issues/1265#issuecomment-1512243972"},"message":"quick one for @colleenjg : if you just use `_obj-raw` for those (in place of what actual object id `organize` uses there) now, it would even make it legit for DANDI validation etc.","files":null},{"type":4,"author":{"id":"23a0b490ec2a3aef708e5bf3548508030984c8bd"},"timestamp":1732405127,"metadata":{"github-id":"CE_lADODBZtRc5iqzFFzwAAAAOWcSMw"},"status":2}]}