{"version":2,"ops":[{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1589299162,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDYyNzQzNTQwNQ==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-627435405"},"message":"yes. Let's initiate may be `dandi_nwb.json` (to complement `dandiset_nwb.json`). https://github.com/dandi/dandiarchive/tree/master/web/src/assets/schema is the origin to https://github.com/dandi/dandi-core/tree/master/assets/schema ATM, but with git powers I think I will be able to still sync with changes to other files there in dandiarchive which we probably want to keep as origin for a bit longer, while all of those schemas related to web ui are still in flux.\n\nWhat specific fields you had in mind? if you could express that in json schema - would be great ;-)","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1589299359,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDYyNzQzNzMzNw==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-627437337"},"message":"for nwb level, we will likely be using this: https://nih-cfde.org/product/cfde-c2m2/ but we need to map these fields onto nwb fields or functions that can operate on nwb files.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1596471265,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODExMDU3Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668110577"},"message":"Now schema being developed in https://github.com/dandi/schema .  There are schemas for json-validator but also pydantic (original) models which could be used for validation in https://github.com/dandi/schema/blob/master/scripts/models.py . So the plan would be\n\n1. adjust our metadata extraction to extract relevant metadata in the desired structure (this function is the \"entry\" point to extract metadatahttps://github.com/dandi/dandi-cli/blob/master/dandi/metadata.py#L17:5).  If desired metadata is missing - we need to identify which metadata within nwb is the one to get: This is the function which \"interrogates\" .nwb file and should also harmonize metadata https://github.com/dandi/dandi-cli/blob/master/dandi/pynwb_utils.py#L170 .\n\n2. use `pydantic` to validate (or would you @satra recommend using produced json schemas?)\n\nFor inclusion of schema: lets wait for https://github.com/dandi/schema/issues/9 and proceed accordingly (module, git submodule, or git subtree as in https://github.com/dandi/dandi-core#assets)\n\nReferences:\n-  https://github.com/NeurodataWithoutBorders/nwb-schema","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1596473793,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODEzMDYyNw==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668130627"},"message":"@yarikoptic - i think all the python code for models/validation should move into dandi-cli (i.e. that will be our dandi python library). the schema repo should be used for making releases and as such it may have a couple of python scripts for processing but not a library.\n\nthe closest example at this point is [reproschema](https://github.com/ReproNim/reproschema) - but there we use jsonld and shacl directly instead of pydantic. i think for dandi pydantic makes sense, especially since we may be able to deliver it as the models for the api as well.\n\nthere are many ways to create models and schemas. the choice to use pydantic was motivated by its ability to generate json schema automatically, which is what the dandi frontend was using for validation. \n\nso i would be ok with moving the models code to dandi-cli and continue the development of the code there. that way it can be used for asset metadata validation. i think we should just use the models code for validation. would give us a lot more flexibility over json schema presently.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1596474000,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODEzMjI3Mg==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668132272"},"message":"Wouldn't it be desirable to eventually run validation server-side as part of dandiarchive?  (Or is bad data on dandiarchive not a problem?)  If so, it seems like the validation code should be a separate library that can be used by both dandi-cli and dandiarchive.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1596481871,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODE5MzgyNQ==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668193825"},"message":"\u003e @yarikoptic - i think all the python code for models/validation should move into dandi-cli (i.e. that will be our dandi python library). the schema repo should be used for making releases and as such it may have a couple of python scripts for processing but not a library.\n\nyeap, my thinking as well, thus relates to @jwodder question:\n\n\u003e Wouldn't it be desirable to eventually run validation server-side as part of dandiarchive?\n\ndandiarchive will just pip install dandi-cli (`dandi` in pip and module) and use it to validate.\n\n\u003e there are many ways to create models and schemas. the choice to use pydantic was motivated by its ability to generate json schema automatically, which is what the dandi frontend was using for validation.\n\u003e \n\u003e so i would be ok with moving the models code to dandi-cli and continue the development of the code there. that way it can be used for asset metadata validation. i think we should just use the models code for validation. would give us a lot more flexibility over json schema presently.\n\nbut pydantic model (besides being in Python) is the model defining the schema, right? thus I thought it would make sense to have it in `dandi/schema`, and for dandi-cli to just use it for validation.  Moving the model inside dandi-cli would \"bind\" them too much IMHO. I.e., in the long(er) run `dandi-cli` could just fetch (or ask to upgrade if it is a pypi module) to the most up-to-date dandi-schema without requiring re-release of the client. And versioning of `dandi-cli` releases would somewhat obscure versioning of the model.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1596488668,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODI0MTI1Mw==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668241253"},"message":"@yarikoptic I'm still not 100% clear on how the metadata extraction should be rewritten.  The goal is to rearrange the data in an NWB so that it matches one of the structures from dandi/schema, yes?  Let's consider `sub-1001658946_ses-1003020741_icephys.nwb` from [DANDI 000020](https://gui.dandiarchive.org/#/dandiset/5eaa216d1fe11561ec13e9ed).  This NWB file has the following fields:\n\n* `acquisition`\n* `analysis`\n* `scratch`\n* `stimulus`\n* `stimulus_template`\n* `processing`\n* `devices`\n* `electrode_groups`\n* `imaging_planes`\n* `icephys_electrodes`\n* `ogen_sites`\n* `intervals`\n* `lab_meta_data`\n* `session_description`\n* `identifier`\n* `session_start_time`\n* `timestamps_reference_time`\n* `file_create_date`\n* `epoch_tags`\n* `subject`; subfields:\n    * `age`\n    * `date_of_birth`\n    * `description`\n    * `genotype`\n    * `sex`\n    * `species`\n    * `subject_id`\n* `sweep_table`\n* `session_id`\n* `institution`\n* `data_collection`\n* `source_script`\n* `source_script_file_name`\n\nThese are the mappings I see to fields defined in https://github.com/dandi/schema/blob/master/scripts/models.py (I assume `subject` maps to `BioSample`, which implies that an NWB file is supposed to map to an `Asset`, as that's the only thing that contains `BioSample`):\n\n* `subject.age` → `Asset.wasDerivedFrom.age`\n* `subject.sex` → `Asset.wasDerivedFrom.sex`\n* `subject.species` → `Asset.wasDerivedFrom.taxonomy` ?\n* `subject.subject_id` → `Asset.wasDerivedFrom.identifier`\n* `subject.genotype` → `Asset.wasDerivedFrom.strain` ???\n* `session_description` → `Asset.description`?\n\nWhat else should there be?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1596499291,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODI5Nzk4OQ==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668297989"},"message":"@satra how in main record for the asset do you see to say that it is derived from a BioSample, or is it assumed to be of type BioSample (thus no explicit type, sounds suboptimal)","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1596499291,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6MzkwNDg4Nzgw"},"target":"180b50f4d3fcf0e5addb5b3c00518a7e609984273657862d8c7304186b1d8d98","message":"@satra how in main record for the asset do you see to say that it is derived from a BioSample, or is it assumed to be of type BioSample (thus no explicit type, sounds suboptimal)?","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1596499305,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6MzkwNDg4ODE3"},"target":"180b50f4d3fcf0e5addb5b3c00518a7e609984273657862d8c7304186b1d8d98","message":"@satra how in (json) record for the asset do you see to say that it is derived from a BioSample, or is it assumed to be of type BioSample (thus no explicit type, sounds suboptimal)?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1596500272,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODMwMjQyMg==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668302422"},"message":"you can use this to be explicit\n\n```\n{ \"@type\", \"dandi:Asset\"\n   ...\n   \"wasDerivedFrom\": { \"@type\": \"dandi:Biosample\",\n                       \"age\": \u003cvalue\u003e, \n                       ...},\n   \"...\": ...\n}\n```","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1596500459,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODMwMzI4Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668303287"},"message":"in general it's not suboptimal since the jsonld context would handle it and augment the type information. however the above explicit notion is still as valid.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1596501660,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODMwODg0Mw==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668308843"},"message":"Thanks, looks nice ;-)","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1596545619,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODU3ODM1OQ==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668578359"},"message":"@jwodder - other metadata to extract from the nwb files are `modality`, `measurementTechnique`, and `variableMeasured`.\n\n```\nmodality: (e.g., optical electrophysiology)\nmeasurementTechnique: (e.g., patch clamp)\nvariableMeasured: (e.g., voltage, current, etc.,. with other properties)\n```\n\nfor modality and measurementTechnique, we may have to create a custom heuristic that maps nwb values to these. here is a presentation from @tgbugs  that lists the modality and technique valuees: https://docs.google.com/presentation/d/1OzLDeTvebHXvn07_oagUcSH0xOuxzrP2GSKTdt5jJlg/edit#slide=id.g8daabd40c5_0_6\n\nthe `variableMeasured` list should come from the different data arrays of the nwb file. perhaps @bendichter can provide some guidance about extracting `variableMeasured`","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1596557097,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY2ODY4NDcxNg==","github-url":"https://github.com/dandi/dandi-cli/issues/101#issuecomment-668684716"},"message":"As the first step let's do those basic attributes which we already somewhat extract and then jump into heuristics and additional fields.\n\nre modality (note: list since could be multiple):  we already have some machinery...:\n\n- we already have some heuristic to \"harvest\" basic neural data types (ndtypes) from the file already:\n\n\u003cdetails\u003e\n\u003csummary\u003e`git grep -p nd_*type` to see related code with inconsistent namings ;) \u003c/summary\u003e \n\n```shell\n$\u003e git grep -p nd_*type\ndandi/consts.py=metadata_nwb_computed_fields = (\ndandi/consts.py:    \"nd_types\",\ndandi/metadata.py=def get_metadata(path):\ndandi/metadata.py:    ndtypes_registry = {\ndandi/metadata.py:            ndtype = res.groups()[0]\ndandi/metadata.py:            if ndtype not in ndtypes_registry:\ndandi/metadata.py:                    \"Original exception was: %s. \" % (ndtype, exc)\ndandi/metadata.py:            import_mod = ndtypes_registry[ndtype]\ndandi/metadata.py:            lgr.debug(\"Importing %r which should provide %r\", import_mod, ndtype)\ndandi/metadata.py:                    % (import_mod, ndtype)\ndandi/metadata.py:    meta[\"nd_types\"] = get_neurodata_types(path)\ndandi/organize.py=def _populate_modalities(metadata):\ndandi/organize.py:    ndtypes_to_modalities = get_neurodata_types_to_modalities_map()\ndandi/organize.py:    ndtypes_unassigned = set()\ndandi/organize.py:        nd_types = r.get(\"nd_types\", [])\ndandi/organize.py:        if isinstance(nd_types, str):\ndandi/organize.py:            nd_types = nd_types.split(\",\")\ndandi/organize.py:        for nd_rec in nd_types:\ndandi/organize.py:            ndtype = nd_rec.split()[0]\ndandi/organize.py:            mod = ndtypes_to_modalities.get(ndtype, None)\ndandi/organize.py:                ndtypes_unassigned.add(ndtype)\ndandi/pynwb_utils.py=def get_neurodata_types_to_modalities_map():\ndandi/pynwb_utils.py:    ndtypes = {}\ndandi/pynwb_utils.py:                ndtype = v_.__name__\ndandi/pynwb_utils.py:                if ndtype in ndtypes:\ndandi/pynwb_utils.py:                    if ndtypes[ndtype] == modality:\ndandi/pynwb_utils.py:                        % (ndtype, ndtypes[ndtype], modality)\ndandi/pynwb_utils.py:                ndtypes[ndtype] = modality\ndandi/pynwb_utils.py:    return ndtypes\ndandi/pynwb_utils.py=def _get_pynwb_metadata(path):\ndandi/pynwb_utils.py:            if f in (\"nwb_version\", \"nd_types\"):\ndandi/tests/test_pynwb_utils.py=def test_get_metadata(simple1_nwb, simple1_nwb_metadata):\ndandi/tests/test_pynwb_utils.py:    # We also populate with nd_types now, although here they would be empty\ndandi/tests/test_pynwb_utils.py:    target_metadata[\"nd_types\"] = []\n\n```\n\u003c/details\u003e\n\n- and then `get_neurodata_types_to_modalities_map` helper to map those basic types to modalities which we use in `organize` to establish the (abbreviated) suffixes such as `_icephys` in the original example here\n- and then those I guess should be mapped into proper ontological ones from pointers in the aforementioned presentation.","files":null}]}