{"version":2,"ops":[{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1679951362,"metadata":{"github-id":"IC_kwDODBZtRc5YkIK1","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1485865653"},"message":"also pinging @AlmightyYakob as there may be both server and cli interactions at play.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1679952314,"metadata":{"github-id":"IC_kwDODBZtRc5YkMTm","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1485882598"},"message":"could you please share the log file which should have been mentioned at the end of the process?  May be would help to see what is not implemented and thus causing 501.","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1680180648,"metadata":{"github-id":"IC_kwDODBZtRc5Y02cQ","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1490249488"},"message":"I can't see any meaningful information in the log file besides the snippets I have pasted above, I'm attaching it here anyways!\n[20230327134937Z-3822266.log.gz](https://github.com/dandi/dandi-cli/files/11111613/20230327134937Z-3822266.log.gz)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1680187983,"metadata":{"github-id":"IC_kwDODBZtRc5Y1m9X","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1490448215"},"message":"I see two types of errors in the log file:\n\n* 501 errors of the kind seen previously in #1033.  As before, I maintain that this can only be caused by a faulty filesystem.\n* 400 errors caused by a digest mismatch when uploading.  Unless something was modifying the local files during the upload process, I also blame this on the file system.\n\n@gmazzamuto What type of filesystem are you uploading from?  Do the errors happen again if you upload again?","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1680188357,"metadata":{"github-id":"IC_kwDODBZtRc5Y1pcb","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1490458395"},"message":"The files are not modified during the upload. I am uploading from an NFS share. When I reuploaded, there were no errors. The upload I am doing right now is showing two 400 errors and one 501 error.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1680198329,"metadata":{"github-id":"IC_kwDODBZtRc5Y2h3F","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1490689477"},"message":"NFS can be fun, depending on the enthusiasm (or lack of such) of admins ;)  At some point even, to reduce \"fun\", I just made it all plain `sync` (read -- slow) to avoid any kind of surprises which I had of various kinds. How \"recent\" the data files you are uploading were created/modified in respect to their time point of upload?  may be we should start recording `mtime` of files we upload so if smth goes wrong we could check if mtime did not change but I am neither sure it is the whole situation here.","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1680281163,"metadata":{"github-id":"IC_kwDODBZtRc5Y8f45","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1492254265"},"message":"Hi Yarik, the files where created several hours before the upload. They are stored on a NAS server with RAID and accessed through an NFS share. Do you think the problems I am seeing are due to the network share? I've never noticed data corruption before. \n\nDuring the last upload I got several errors, including a 501 but with a slightly different stack trace:\n\n```\nTraceback (most recent call last):                                                                                                                                                                                                           \n  File \"/opt/bin/dandi\", line 8, in \u003cmodule\u003e                                                                                                                                                                           \n    sys.exit(main())                                                                                                                                                                                                                         \n  File \"/opt/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__                                                                                                                                        \n    return self.main(*args, **kwargs)                                                                                                                                                                                                        \n  File \"/opt/lib/python3.8/site-packages/click/core.py\", line 1055, in main                                                                                                                                            \n    rv = self.invoke(ctx)                                                                                                                                                                                                                    \n  File \"/opt/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke                                                                                                                                          \n    return _process_result(sub_ctx.command.invoke(sub_ctx))                                                                                                                                                                                  \n  File \"/opt/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke                                                                                                                                          \n    return ctx.invoke(self.callback, **ctx.params)                                                                                                                                                                                           \n  File \"/opt/lib/python3.8/site-packages/click/core.py\", line 760, in invoke                                                                                                                                           \n    return __callback(*args, **kwargs)                                                                                                                                                                                                       \n  File \"/opt/lib/python3.8/site-packages/click/decorators.py\", line 38, in new_func                                                                                                                                    \n    return f(get_current_context().obj, *args, **kwargs)                                                                                                                                                                                     \n  File \"/opt/lib/python3.8/site-packages/dandi/cli/base.py\", line 102, in wrapper                                                                                                                                      \n    return f(*args, **kwargs)                                                                                                                                                                                                                \n  File \"/opt/lib/python3.8/site-packages/dandi/cli/cmd_upload.py\", line 98, in upload                                                                                                                                  \n    upload(                                                                                                                                                                                                                                  \n  File \"/opt/lib/python3.8/site-packages/dandi/upload.py\", line 343, in upload                                                                                                                                         \n    raise upload_err                                                                                                                                                                                                                         \n  File \"/opt/lib/python3.8/site-packages/dandi/upload.py\", line 240, in process_path\n    for r in dfile.iter_upload(                            \n  File \"/opt/lib/python3.8/site-packages/dandi/files/zarr.py\", line 470, in iter_upload\n    size = fut.result()                                    \n  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 437, in result                                          \n    return self.__get_result()                             \n  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result                                    \n    raise self._exception                                  \n  File \"/usr/lib/python3.8/concurrent/futures/thread.py\", line 57, in run                                             \n    result = self.fn(*self.args, **self.kwargs)            \n  File \"/opt/lib/python3.8/site-packages/dandi/files/zarr.py\", line 544, in _upload_zarr_file\n    storage_session.put(                                   \n  File \"/opt/lib/python3.8/site-packages/dandi/dandiapi.py\", line 311, in put\n    return self.request(\"PUT\", path, **kwargs)             \n  File \"/opt/lib/python3.8/site-packages/dandi/dandiapi.py\", line 210, in request\n    for i, attempt in enumerate(                           \n  File \"/opt/lib/python3.8/site-packages/tenacity/__init__.py\", line 347, in __iter__\n    do = self.iter(retry_state=retry_state)                \n  File \"/opt/lib/python3.8/site-packages/tenacity/__init__.py\", line 325, in iter\n    raise retry_exc.reraise()                              \n  File \"/opt/lib/python3.8/site-packages/tenacity/__init__.py\", line 158, in reraise\n    raise self.last_attempt.result()                       \n  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 437, in result                                          \n    return self.__get_result()                             \n  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result                                    \n    raise self._exception                                  \n  File \"/opt/lib/python3.8/site-packages/dandi/dandiapi.py\", line 240, in request\n    result.raise_for_status()                              \n  File \"/opt/lib/python3.8/site-packages/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)                                                                    \nrequests.exceptions.HTTPError: 501 Server Error: Not Implemented for url: https://dandiarchive.s3.amazonaws.com/zarr/d5f67f3d-144c-447e-aae0-ca5b979c648d/0/1/22/58?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F\n20230330%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230330T134214Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=e65355bb3725ca73f8d313d9e907ddbbeb00c3e7868d392b233b8eaf2eb7b1b5\n\n```","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1680285468,"metadata":{"github-id":"IC_kwDODBZtRc5Y9ACO","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1492385934"},"message":"@jwodder what about those \"not implemented\" 501s coming from AWS -- what are those exactly about and isn't it us who submit those requests?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1680286059,"metadata":{"github-id":"IC_kwDODBZtRc5Y9CUx","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1492395313"},"message":"@yarikoptic As before, it's the same problem as #1033.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1680292437,"metadata":{"github-id":"IC_kwDODBZtRc5Y9g13","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1492520311"},"message":"\u003e Hi Yarik, the files where created several hours before the upload. They are stored on a NAS server with RAID and accessed through an NFS share. Do you think the problems I am seeing are due to the network share?\n\nIn that #1033 it also was NFS over beegfs, so so far it is some common aspect here.  Do you know your underlying NAS server filesystem?\n\n\u003e  I've never noticed data corruption before.\n\nit might be not necessarily \"corruption\" but some other odd behavior/delayed metadata propagation. E.g. in https://github.com/dandi/dandi-cli/issues/1033#issuecomment-1176447518 @jwodder hypothesizes that may be some inability to get file size reported forces requests  to\n\n\u003e sends a \"Transfer-Encoding\" (with value \"chunked\") when it's unable to determine the size of the request payload .. The only reason I can think of as to why getting the file size should fail would be due to some sort of filesystem hiccup.\n\n\n\u003e During the last upload I got several errors, including a 501 but with a slightly different stack trace:\n\u003e ...\n\u003e requests.exceptions.HTTPError: 501 Server Error: Not Implemented for url: https://dandiarchive.s3.amazonaws.com/zarr/d5f67f3d-144c-447e-aae0-ca5b979c648d/0/1/22/58?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F\n\u003e 20230330%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230330T134214Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=e65355bb3725ca73f8d313d9e907ddbbeb00c3e7868d392b233b8eaf2eb7b1b5\n\u003e ```\n\n@jwodder believes (and I trust him on that) that it is likely that issue of chunked encoding described in https://github.com/dandi/dandi-cli/issues/1033#issuecomment-1176447518 and cited above.  \n\n @jwodder - do you mean this https://github.com/psf/requests/blob/HEAD/requests/models.py#L549  or some other location?  how could we instrument in our code to discover more about the situation better?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1680528003,"metadata":{"github-id":"IC_kwDODBZtRc5ZEW2H","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1494314375"},"message":"@yarikoptic That's one of the location involved, yes.  What exactly do you want to discover?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1680544161,"metadata":{"github-id":"IC_kwDODBZtRc5ZF9Tf","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1494734047"},"message":"\u003e @yarikoptic That's one of the location involved, yes. What exactly do you want to discover?\n\nultimately -- resolution to this issue so that there is no error during uploading.  For that IMHO we need to troubleshoot further and instrument to gather\n- some evidence that your idea on the underlying issue is right on. If it is (refusal to stat a size) it sounds really odd that size would not be reported, and IMHO then should be reported to the admins of those systems\n- workaround: presumably (double check for this particular case in the logs, report back) we are retrying.  It sounds really unlikely that the same filesystem fluke happens reproducibly for an extended period of time.  If it happens at requests level reproducibly -- we should be able to detect it at dandi-cli level when catching that exception -- instrument there to confirm that it is indeed about inability to request the size","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1680544178,"metadata":{"github-id":"IC_kwDODBZtRc5ZF9YM","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1494734348"},"message":"Found another old NFS related issue: https://github.com/dandi/dandi-cli/issues/764","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1680624339,"metadata":{"github-id":"IC_kwDODBZtRc5ZLsg0","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1496238132"},"message":"@yarikoptic Could you be more precise about exactly what sort of instrumentation you want?  Your first bullet point sounds like you want to detect \u0026 report when `requests` fails to get a file's size, and your second bullet point also sounds like you want to detect \u0026 report when `requests` fails to get a file's size (except that the second bullet says to do this by inspecting some exception, but the only exception dandi-cli sees is raised in response to S3 returning a 501 error).  Am I misinterpreting what you're saying, or are you asking for the same thing twice with completely different words?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1680624348,"metadata":{"github-id":"UCE_lALODBZtRc5ZLsg0zi-Dt6A"},"target":"99f42aa7032bd9f61a51b64a1d920acd34b26007dab7f8264497e8908027e634","message":"@yarikoptic Could you be more precise about exactly what sort of instrumentation you want?  Your first bullet point sounds like you want to detect \u0026 report when `requests` fails to get a file's size, and your second bullet point also sounds like you want to detect \u0026 report when `requests` fails to get a file's size (except that the second bullet says to do this by inspecting some exception, but the only exception dandi-cli sees is raised in response to S3 returning a 501 error).  Am I misinterpreting what you're saying, or are you asking for the exact same thing twice with completely different words?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1680624877,"metadata":{"github-id":"IC_kwDODBZtRc5ZLv2J","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1496251785"},"message":"\u003e Your first bullet point sounds like you want to detect \u0026 report when `requests` fails to get a file's size\n\ncorrect\n\n\u003e and your second bullet point also ... Am I misinterpreting what you're saying, or are you asking for the exact same thing twice with completely different words?\n\nin the 2nd point I\n\n- asked to double check if we are retrying to send a file on that particular `501` error from AWS\n- continued arguing that if we get to that 501 exception handling after initial error we must fail to get the size the way `requests` fails to get the size if on retry `requests` also fails to get the size.  If we do not fail to get the size, but requests again leads us to 501 then it is either\n   - it is not the \"size getting\" which flips it and we better provision more,\n   - we do get size somehow differently\n   - some other factors within `requests` code which cause it to fail there.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1680626215,"metadata":{"github-id":"IC_kwDODBZtRc5ZL2uW","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1496279958"},"message":"@yarikoptic Yes, we are retrying on the 501 errors.  For example, grepping for one of the failed upload URLs from the logs in [this comment](https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1490249488) gives:\n\n```\n2023-03-27T17:00:04+0200 [DEBUG   ] dandi 3822266:140508050728704 PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:11+0200 [DEBUG   ] urllib3.connectionpool 3822266:140508050728704 https://dandiarchive.s3.amazonaws.com:443 \"PUT /zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228 HTTP/1.1\" 500 None\n2023-03-27T17:00:13+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:15+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:18+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:21+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:24+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:29+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:34+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:41+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:49+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:00:59+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:01:11+0200 [WARNING ] dandi 3822266:140508050728704 Retrying PUT https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\nrequests.exceptions.HTTPError: 501 Server Error: Not Implemented for url: https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n2023-03-27T17:01:12+0200 [DEBUG   ] dandi 3822266:140509742302976 Error uploading zarr: HTTPError: 501 Server Error: Not Implemented for url: https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\nrequests.exceptions.HTTPError: 501 Server Error: Not Implemented for url: https://dandiarchive.s3.amazonaws.com/zarr/0205ef2e-71ba-4ba1-a4b3-489ee5efe02b/0/0/8/42?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20230327%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20230327T145955Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=70c3f2c08fc29ea209c8a60fb366fa4619cb2d7190b9b5030c82552cad51a228\n```\n\n\u003e continued arguing that if we get to that 501 exception handling after initial error we must fail to get the size the way `requests` fails to get the size if on retry `requests` also fails to get the size.\n\nSo, if \u0026 when the 501 error occurs (only for the first time for a given uploaded file?), you want dandi-cli to try to get the filesize the same way `requests` does it, and then what?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1680638278,"metadata":{"github-id":"IC_kwDODBZtRc5ZMw6m","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1496518310"},"message":"\u003cdetails\u003e\n\u003csummary\u003eNB I had a question are we \"retrying at a proper level\" or just sending the \"bad\" `Content-Length` over and over again? Seems to  be yes:\u003c/summary\u003e\n\nhere is the stack for the point where `Content-Length` is assigned:\n\n```\n(Pdb)   /usr/lib/python3.11/threading.py(995)_bootstrap()\n-\u003e self._bootstrap_inner()\n  /usr/lib/python3.11/threading.py(1038)_bootstrap_inner()\n-\u003e self.run()\n  /usr/lib/python3.11/threading.py(975)run()\n-\u003e self._target(*self._args, **self._kwargs)\n  /usr/lib/python3.11/concurrent/futures/thread.py(83)_worker()\n-\u003e work_item.run()\n  /usr/lib/python3.11/concurrent/futures/thread.py(58)run()\n-\u003e result = self.fn(*self.args, **self.kwargs)\n  /home/yoh/proj/dandi/dandi-cli-master/dandi/files/zarr.py(544)_upload_zarr_file()\n-\u003e storage_session.put(\n  /home/yoh/proj/dandi/dandi-cli-master/dandi/dandiapi.py(311)put()\n-\u003e return self.request(\"PUT\", path, **kwargs)\n  /home/yoh/proj/dandi/dandi-cli-master/dandi/dandiapi.py(227)request()\n-\u003e result = self.session.request(\n  /home/yoh/proj/misc/requests/requests/sessions.py(573)request()\n-\u003e prep = self.prepare_request(req)\n  /home/yoh/proj/misc/requests/requests/sessions.py(484)prepare_request()\n-\u003e p.prepare(\n  /home/yoh/proj/misc/requests/requests/models.py(371)prepare()\n-\u003e self.prepare_body(data, files, json)\n\u003e /home/yoh/proj/misc/requests/requests/models.py(550)prepare_body()\n-\u003e if length:\n```\n\nso the `sessions.py(573)request()` is the request we are retrying with tenacity (in the original traceback) so I guess should be all good,.\n\n\u003c/details\u003e\n\n\u003e So, if \u0026 when the 501 error occurs (only for the first time for a given uploaded file?), you want dandi-cli to try to get the filesize the same way requests does it, and then what?\n\nif we can get the file size there then it is one of those 3 cases I listed which could be at play here.  Most likely imho is that it is some other cause than size getting. But to entirely rule it out -- we would need to overload\n\n```\n❯ grep 'def super' requests/utils.py\ndef super_len(o):\n```\nwhich is what is used by requests, and instrument to log the \"body\" (what file it is) and returned size.  This way we would discover for sure if that is somehow size to blame.  What if we start from there and add instrumentation of that function to be triggered by e.g. `DANDI_DEVEL_INSTRUMENT_REQUESTS_SUPERLEN` env var?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1680656107,"metadata":{"github-id":"IC_kwDODBZtRc5ZNw8m","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1496780582"},"message":"@gmazzamuto would you be so kind to try with fresh 0.52.0 release which would log more information about failing requests -- may be it would give us more clue on what is going on.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1680717925,"metadata":{"github-id":"IC_kwDODBZtRc5ZSDoZ","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1497905689"},"message":"@gmazzamuto , if you could install from GitHub master branch, and run with `DANDI_DEVEL_INSTRUMENT_REQUESTS_SUPERLEN=1` environment variable set, would be even better and logs could help us to figure out more","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1680782350,"metadata":{"github-id":"IC_kwDODBZtRc5ZWCjK","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1498949834"},"message":"Sure, I can give it a try. But I have completed my uploads for now, there are only some small files left (photos and json sidecar files). I guess it's more likely to happen with larger files, but I can try anyways","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1681736244,"metadata":{"github-id":"IC_kwDODBZtRc5aFG8D","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1511288579"},"message":"any odd behaviors you were (un)lucky to run into @gmazzamuto ?","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1681746312,"metadata":{"github-id":"IC_kwDODBZtRc5aGZR3","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1511625847"},"message":"Hi Yarik, I haven't had the chance to upload new data yet, I will try in the coming days then I'll let you know how it goes!","files":null},{"type":5,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1681835307,"metadata":{"github-id":"LE_lADODBZtRc5h6XKnzwAAAAIatQF3"},"added":["awaiting-user-response"],"removed":[]},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1682957696,"metadata":{"github-id":"IC_kwDODBZtRc5bMJU1","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1529910581"},"message":"@gmazzamuto we are holding our breath! ;-)","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1683013857,"metadata":{"github-id":"IC_kwDODBZtRc5bQbyY","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1531034776"},"message":"Hi Yarik, I haven't forgotten about this, but I am just waiting for a bunch of new data to upload to see if the error can be triggered again. It should be ready soon","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1687419229,"metadata":{"github-id":"IC_kwDODBZtRc5ffu8_","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1602154303"},"message":"Hi Yarik, I am getting ready for new uploads next week. Are the instructions still valid? Install from GitHub master branch and run with `DANDI_DEVEL_INSTRUMENT_REQUESTS_SUPERLEN=1`","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1687433027,"metadata":{"github-id":"IC_kwDODBZtRc5fg7_e","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1602469854"},"message":"yes, besides that pypi release (since 0.53.0) would be as good too now.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1697825205,"metadata":{"github-id":"IC_kwDODBZtRc5psIpO","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1773177422"},"message":"@gmazzamuto did you get a chance to gather more debugging information on this issue?","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1698158867,"metadata":{"github-id":"IC_kwDODBZtRc5p8OJf","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1777394271"},"message":"Yes I have tried recently and I still got the same errors. I had to restart the upload a couple of times. I have the log file but it's really huge, so I haven't uploaded it. I will soon try again, maybe with a smaller payload","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1700678623,"metadata":{"github-id":"IC_kwDODBZtRc5srULN","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1823294157"},"message":"Hi folks,\n\nI am still getting those errors. Here attached is a heavily redacted log file with many lines deleted, I've left the lines around where the error happened.\n\nI've deleted all the lines of this kind:\n```\nThe `/NIH/DANDI/000026/rawdata_biolab/I60/20220829_I60_49_LeftDet_638_RightDet_488/tiff_right/x_132.00000_y_099.60000_z_000.00000__cam_r.ome.tif` file was not matched by any regex schema entry.\n\nEstimating digests for /NIH/DANDI/000026/sub-I59/ses-SPIM/micr/sub-I59_ses-SPIM_sample-BrocaAreaS27_stain-Calretinin_SPIM.ome.zarr/0/0/84/55\n```\nand the successful transfers.\n\n[20231122142221Z-639852.log.gz](https://github.com/dandi/dandi-cli/files/13443386/20231122142221Z-639852.log.gz)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1700679453,"metadata":{"github-id":"IC_kwDODBZtRc5srZ4g","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1823317536"},"message":"I found the problem:\n\n\u003e 2023-11-22T17:44:16+0100 [DEBUG   ] dandi 639852:140007712995072 requests.utils.super_len() reported 0 for \u003c_io.BufferedReader name='/NIH/DANDI/000026/sub-I59/ses-SPIM/micr/sub-I59_ses-SPIM_sample-BrocaAreaS28_stain-Somatostatin_SPIM.ome.zarr/0/1/6/12'\u003e\n\nIf `super_len()` returns 0, [`requests` will use \"chunked\" encoding](https://github.com/psf/requests/blob/0b4d494192de489701d3a2e32acef8fb5d3f042e/src/requests/models.py#L548-L551), which AWS apparently doesn't support.  It's my understanding that uploading an empty file to AWS (and thus to Dandi Archive) isn't something that can be done in the first place, so (aside from the UX around the error message) the upload is correct to error.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1700682334,"metadata":{"github-id":"IC_kwDODBZtRc5sryjK","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1823418570"},"message":"@gmazzamuto can you confirm that `/NIH/DANDI/000026/sub-I59/ses-SPIM/micr/sub-I59_ses-SPIM_sample-BrocaAreaS28_stain-Somatostatin_SPIM.ome.zarr/0/1/6/12` is an empty (0 length) file?  \n\n\u003e It's my understanding that uploading an empty file to AWS (and thus to Dandi Archive) isn't something that can be done in the first place\n\nWhat do you mean exactly? For DANDI we indeed disallow empty blobs, but S3 does support empty files [*] so if zarr needs (yet to be checked) an empty file we should be able to do it on S3.\n\nSince I am not an expert in zarr -- does having an empty file within zarr makes sense. @satra do you know?  so far chatgpt says \"No\" to me, but there could be empty folders! (for empty datasets, not sure if they could safely be omitted but we can't have empty folder on S3)\n\nif it does -- I guess we would need to make sure we could \"upload\" 0-length file to S3 (might be some custom/`touch` call without actual content transfer since none to be done).  If it doesn't -- we should add to zarr validation and fail informatively if such file is found within a zarr.\n\n[*] https://aws.amazon.com/s3/faqs/ says \"Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. \"","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1700682491,"metadata":{"github-id":"IC_kwDODBZtRc5srzOg","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1823421344"},"message":"Previous issue on uploading empty files to the Archive: https://github.com/dandi/dandi-archive/issues/168","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1700682684,"metadata":{"github-id":"IC_kwDODBZtRc5sr0AX","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1823424535"},"message":"@yarikoptic No, it's not an empty file. It's 965KiB. Indeed, I reuploaded the whole .ome.zarr that errored and now it worked.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1700692122,"metadata":{"github-id":"IC_kwDODBZtRc5ssbPS","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1823585234"},"message":"\u003e Previous issue on uploading empty files to the Archive: [dandi/dandi-archive#168](https://github.com/dandi/dandi-archive/issues/168)\n\nyeap, no blobs of size 0 are still allowed although indeed it is somewhat of a superficial restriction. And apparently the case here is also that we do not have a 0-length file, but rather that `requests.utils.super_len()` managed to report `0` on the file which is not empty.  From looking at the function and https://github.com/psf/requests/blob/main/src/requests/utils.py#L143\n\n```python\n        try:\n            fileno = o.fileno()\n        except (io.UnsupportedOperation, AttributeError):\n            # AttributeError is a surprising exception, seeing as how we've just checked\n            # that `hasattr(o, 'fileno')`.  It happens for objects obtained via\n            # `Tarfile.extractfile()`, per issue 5229.\n            pass\n        else:\n            total_length = os.fstat(fileno).st_size\n```\n\nit is not 100% clear on either it was some io.UnsupportedOperation for some reason or indeed `os.fstat(fileno).st_size` returned 0.\n\nI found nothing in requests [to mention NFS](https://github.com/search?q=repo%3Apsf%2Frequests%20NFS\u0026type=code), but I do know that \"NFS is *special*\" and all kinds of oddities could happen there.\n\n@gmazzamuto -- would you mind sharing output of \n\n`stat /NIH/DANDI/000026/sub-I59/ses-SPIM/micr/sub-I59_ses-SPIM_sample-BrocaAreaS28_stain-Somatostatin_SPIM.ome.zarr/0/1/6/12` \n\n?\n\n@jwodder -- any ideas? if not -- we should  instrument your instrumentation further to clarify where the gotcha and either there is some sensible workaround (e.g. sleep). e.g. instead of just printing when `super_len` returns 0 - do os.fstat on that `.fileno` and then possibly even smth as calling `stat ` on the `.name` from outside the python and checking that information -- if both are 0s, then \"all cool\", must be empty on that round.  But if not -- we caught NFS/Python interplay and might be worth reporting to requests depending on what we find.  Also do loop few times with a sleep to ensure that value doesn't change within e.g. 5 seconds, if changes -- just NFS to blame, and for us to workaround unless we could unveil some indicator -- please log full output of `fstat` in such cases, may be mtime/ctime or some other attribute would also be degenerate.","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1700725355,"metadata":{"github-id":"IC_kwDODBZtRc5stu1Y","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1823927640"},"message":"Here is the output of stat:\n\n```\nFile: /NIH/DANDI/000026/sub-I59/ses-SPIM/micr/sub-I59_ses-SPIM_sample-BrocaAreaS28_stain-Somatostatin_SPIM.ome.zarr/0/1/6/12\n  Size: 987680          Blocks: 1936       IO Block: 1048576 regular file\nDevice: 3dh/61d Inode: 244535389962  Links: 1\nAccess: (0644/-rw-r--r--)  Uid: ( 1000/user)   Gid: ( 1000/group)\nAccess: 2023-11-22 17:01:35.752336409 +0100\nModify: 2023-11-22 10:14:49.966501429 +0100\nChange: 2023-11-22 10:14:49.966501429 +0100\n Birth: -\n```","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701197144,"metadata":{"github-id":"IC_kwDODBZtRc5tGtEn","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1830474023"},"message":"@yarikoptic \n\n\u003e possibly even smth as calling `stat ` on the `.name` from outside the python and checking that information\n\nDo you mean invoking an external program that does `stat`?  I don't believe there's a portable way to do that, and I'd rather not blindly try to figure out how to do such a thing in Windows.\n\n\u003e But if not -- we caught NFS/Python interplay and might be worth reporting to requests depending on what we find.\n\nI doubt the `requests` maintainers are going to be willing or able to do anything about NFS being a liar.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1701200007,"metadata":{"github-id":"IC_kwDODBZtRc5tG99u","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1830543214"},"message":"\u003e @yarikoptic\n\u003e \n\u003e \u003e possibly even smth as calling `stat ` on the `.name` from outside the python and checking that information\n\u003e \n\u003e Do you mean invoking an external program that does `stat`? I don't believe there's a portable way to do that, and I'd rather not blindly try to figure out how to do such a thing in Windows.\n\nAFAIK there no NFS on windows we need to troubleshoot here, so we should be fine.  Sure thing if you see a better, non-stat way, ok with me too. The idea was to get \"third-party non Python assessment\" since after all Python is also not god's given and I personally do not know if it would lead to the same syscall as of `stat` and no other conditioning. \n\n\u003e I doubt the requests maintainers are going to be willing or able to do anything about NFS being a liar.\n\nwell, it might at least be worth raising their awareness that NFS is a liar and workarounds might be needed.  But first we really need to catch it \"red handed\".","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701202069,"metadata":{"github-id":"IC_kwDODBZtRc5tHVza","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1830640858"},"message":"@yarikoptic Problem: I don't believe there's a portable way to call `stat` via an external program on Unix either (unless you want to invoke another programming language like Perl).  The best candidate — the `stat(1)` command — isn't part of POSIX, so the GNU version (used on Linux) and the BSD version (used on macOS) emit different formats and take different options.  Also, I strongly suspect that `stat(1)`, Perl, and Python all call [`stat(2)`](https://pubs.opengroup.org/onlinepubs/9699919799/).","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701202103,"metadata":{"github-id":"UCE_lALODBZtRc5tHVzazjuhSLE"},"target":"012a0a766cfa4cefe611020591e1b7591369690fe4c74d85689b07b3c8f41792","message":"@yarikoptic Problem: I don't believe there's a portable way to call `stat` via an external program on Unix either (unless you want to invoke another programming language like Perl).  The best candidate — the `stat(1)` command — isn't part of POSIX, so the GNU version (used on Linux) and the BSD version (used on macOS) emit different formats and take different options.  Also, I strongly suspect that `stat(1)`, Perl, and Python all call [`stat(2)`](https://pubs.opengroup.org/onlinepubs/9699919799/functions/fstatat.html).","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701203077,"metadata":{"github-id":"IC_kwDODBZtRc5tHcKw","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1830666928"},"message":"@yarikoptic Setting aside the external program issue for now, what exactly is the flow you want to be added around `super_len()`?  [Your comment](https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1823585234) implies the following:\n\n- If `super_len()` returns 0 on a file-like object:\n    - Call `os.stat()` on the file path\n    - Call `os.fstat()` on the file object's fileno\n    - If at least one stat was successful and all successful stats reported a size of zero, then assume the file is genuinely empty and return\n    - Otherwise, sleep for five(?) seconds and then try again\n        - How many tries should there be in total?\n        - Should we ever sleep \u0026 retry when the stats agree the file is empty?\n        - What size value should be returned to `requests`?\n- Should these checks also be run if `super_len()` raises an exception?\n\nIs this the sort of thing you want?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1701209405,"metadata":{"github-id":"IC_kwDODBZtRc5tIGSJ","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1830839433"},"message":"Since we do not expect 0 length files at all:\n\n- If `super_len()` returns 0 on a file-like object:\n   - for 10 trials:\n     -  Call `super_len()` on the file path\n     - Call `os.stat()` on the file path\n     - Call `os.fstat()` on the file object's fileno\n     - If any of them fails -- log that ERROR, might be the culprit\n     - If any of them non-0 value -- raise Exception with detail (original super_len was 0 but now `stat` X is not after Y seconds)\n     - sleep for 1 second\n  - DEBUG log that we are returning 0 for the file path after 10 rounds of stats which all were 0s\n  - return 0\n- Should these checks also be run if `super_len()` raises an exception?\n  - so far we didn't get any error from super_len, right? so let's just keep it as is since then we would not get just 0 returned , right?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1701316799,"metadata":{"github-id":"IC_kwDODBZtRc5tQm4s","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1833070124"},"message":"@gmazzamuto when will you be uploading again?  we now have merged #1370 so if you would use master version of dandi-cli with `DANDI_DEVEL_INSTRUMENT_REQUESTS_SUPERLEN=1` we should do even more checks/logging at debug level to hopefully pin down erroneous behavior of NFS here.","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1701440528,"metadata":{"github-id":"IC_kwDODBZtRc5tci6P","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1836199567"},"message":"Here is the latest log (again, where I've deleted  many lines) with several exceptions of different kinds.\n\n[log.txt](https://github.com/dandi/dandi-cli/files/13527585/log.txt)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701440852,"metadata":{"github-id":"IC_kwDODBZtRc5tck23","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1836207543"},"message":"@gmazzamuto Are you sure you were using commit b9a1099 of dandi-cli?  I see a log message about a zero-length Zarr file but none of the messages added in #1370.","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1701441386,"metadata":{"github-id":"IC_kwDODBZtRc5tcoM8","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1836221244"},"message":"no sorry, my fault: I saw a new available version of dandi-cli and installed that without checking if it contained the relevant commit. I'll send the log using the correct version of the client","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1701679711,"metadata":{"github-id":"IC_kwDODBZtRc5tjvrX","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1838086871"},"message":"Hi all, here is the latest log:\n\n[log.txt](https://github.com/dandi/dandi-cli/files/13543834/log.txt)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701709948,"metadata":{"github-id":"IC_kwDODBZtRc5tnnfX","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839101911"},"message":"I see the following problems in the log:\n\n```\n2023-12-02T02:15:59+0100 [DEBUG   ] dandi 3030432:140064092833536 requests.utils.super_len() reported 0 for \u003c_io.BufferedReader name='/NIH/DANDI/000026/sub-I62/ses-SPIM/micr/sub-I62_ses-SPIM_sample-BrocaAreaS13_stain-Calretinin_SPIM.ome.zarr/0/1/82/89'\u003e\n2023-12-02T02:15:59+0100 [DEBUG   ] dandi 3030432:140064092833536 - Size of 0 is suspicious; double-checking that NFS isn't lying\n2023-12-02T02:15:59+0100 [DEBUG   ] dandi 3030432:140064092833536 - stat('/NIH/DANDI/000026/sub-I62/ses-SPIM/micr/sub-I62_ses-SPIM_sample-BrocaAreaS13_stain-Calretinin_SPIM.ome.zarr/0/1/82/89') = os.stat_result(st_mode=33188, st_ino=248398250076, st_dev=61, st_nlink=1, st_uid=5000, st_gid=7000, st_size=312461, st_atime=1701477007, st_mtime=1700979160, st_ctime=1700979160)\n2023-12-02T02:15:59+0100 [DEBUG   ] dandi 3030432:140064092833536 - fstat('/NIH/DANDI/000026/sub-I62/ses-SPIM/micr/sub-I62_ses-SPIM_sample-BrocaAreaS13_stain-Calretinin_SPIM.ome.zarr/0/1/82/89') = os.stat_result(st_mode=33188, st_ino=248398250076, st_dev=61, st_nlink=1, st_uid=5000, st_gid=7000, st_size=312461, st_atime=1701477007, st_mtime=1700979160, st_ctime=1700979160)\nRuntimeError: requests.utils.super_len() reported size of 0 for '/NIH/DANDI/000026/sub-I62/ses-SPIM/micr/sub-I62_ses-SPIM_sample-BrocaAreaS13_stain-Calretinin_SPIM.ome.zarr/0/1/82/89', but os.stat() reported size 312461 bytes 1 tries later\n```\n\n```\n2023-12-02T07:38:21+0100 [DEBUG   ] dandi 3030432:140061148436224 requests.utils.super_len() reported 0 for \u003c_io.BufferedReader name='/NIH/DANDI/000026/sub-I62/ses-SPIM/micr/sub-I62_ses-SPIM_sample-BrocaAreaS17_stain-NeuN_SPIM.ome.zarr/0/1/32/51'\u003e\n2023-12-02T07:38:21+0100 [DEBUG   ] dandi 3030432:140061148436224 - Size of 0 is suspicious; double-checking that NFS isn't lying\n2023-12-02T07:38:21+0100 [DEBUG   ] dandi 3030432:140061148436224 - stat('/NIH/DANDI/000026/sub-I62/ses-SPIM/micr/sub-I62_ses-SPIM_sample-BrocaAreaS17_stain-NeuN_SPIM.ome.zarr/0/1/32/51') = os.stat_result(st_mode=33188, st_ino=248625074703, st_dev=61, st_nlink=1, st_uid=5000, st_gid=7000, st_size=1355361, st_atime=1701494494, st_mtime=1700989187, st_ctime=1700989187)\n2023-12-02T07:38:21+0100 [DEBUG   ] dandi 3030432:140061148436224 - fstat('/NIH/DANDI/000026/sub-I62/ses-SPIM/micr/sub-I62_ses-SPIM_sample-BrocaAreaS17_stain-NeuN_SPIM.ome.zarr/0/1/32/51') = os.stat_result(st_mode=33188, st_ino=248625074703, st_dev=61, st_nlink=1, st_uid=5000, st_gid=7000, st_size=1355361, st_atime=1701494494, st_mtime=1700989187, st_ctime=1700989187)\nRuntimeError: requests.utils.super_len() reported size of 0 for '/NIH/DANDI/000026/sub-I62/ses-SPIM/micr/sub-I62_ses-SPIM_sample-BrocaAreaS17_stain-NeuN_SPIM.ome.zarr/0/1/32/51', but os.stat() reported size 1355361 bytes 1 tries later\n```\n\nConclusion: NFS is a liar.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1701712188,"metadata":{"github-id":"IC_kwDODBZtRc5tn3Bi","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839165538"},"message":"\"coolio!\"  another question before we proceed: @jwodder do you think it is  pertinent to only size 0 or might be for other occasions that NFS reports wrong size first -- would some check/code fail if file size changes during upload there from the moment of initial call to `super_len`?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701713245,"metadata":{"github-id":"IC_kwDODBZtRc5tn-Z4","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839195768"},"message":"@yarikoptic \n\n\u003e would some check/code fail if file size changes during upload there from the moment of initial call to super_len?\n\nThat depends on how the S3 server is implemented.  The HTTP standard doesn't seem to specify a behavior when a request has an inaccurate `Content-Length`, so the server could react however it likes, such as by storing a file with a size that matches the `Content-Length` but not the uploaded data.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1701715252,"metadata":{"github-id":"IC_kwDODBZtRc5toLzg","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839250656"},"message":"@jwodder \n\n- most likely it is a NFS behavior oddity specific for size 0, but let's add (at the end of `iter_upload`?) a check that file size didn't check from the point when it was initially requested to be uploaded. If changed -- raise an informative exception and blame the filesystem and/or user for having file changed under our \"feet\"\n- for files of size 0 -- add a loop alike we have but re-checking at smaller intervals and up to e.g. 2 seconds and if changes, log a warning and proceed with that new size.  That should mitigate this issue @gmazzamuto is experiencing and allow for uploads to become more robust.\n\n@gmazzamuto could you share details of kernel version (`uname -a`) and the mount options (output of `mount` grepped for that location) so we have a record of what scenario could potentially lead to such odd behavior.  Ideally someone should report/clear it up with NFS (kernel) developers.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701716413,"metadata":{"github-id":"IC_kwDODBZtRc5toTMr","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839280939"},"message":"@yarikoptic \n\n\u003e let's add (at the end of iter_upload?) a check that file size didn't [change] from the point when it was initially requested to be uploaded.\n\nS3 already checks that the uploaded files match the MD5 checksums that the client provided before starting the upload.  If a `Content-Length` were to get set to an inaccurate value and S3 honored it, this check would fail, and the upload would fail with an HTTP error.\n\n\u003e for files of size 0 -- add a loop alike we have but re-checking at smaller intervals and up to e.g. 2 seconds and if changes, log a warning and proceed with that new size.\n\nIs this a change to the current loop when spying on `super_len()` or is this a new loop to be added somewhere else (and, if so, where)?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1701719695,"metadata":{"github-id":"IC_kwDODBZtRc5toqOI","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839375240"},"message":"\u003e \u003e let's add (at the end of iter_upload?) a check that file size didn't [change] from the point when it was initially requested to be uploaded.\n\u003e \n\u003e S3 already checks that the uploaded files match the MD5 checksums that the client provided before starting the upload. If a `Content-Length` were to get set to an inaccurate value and S3 honored it, this check would fail, and the upload would fail with an HTTP error.\n\nright, but if initial (before upload commenced) read `stat`s of size were wrong (incomplete) of value X bytes, file read only to X bytes, and upload stopped at `Content-Length` of X bytes -- from AWS perspective all would be good. With extra check we would just ensure that the size didn't silently change during upload.\n\n\u003e \u003e for files of size 0 -- add a loop alike we have but re-checking at smaller intervals and up to e.g. 2 seconds and if changes, log a warning and proceed with that new size.\n\u003e \n\u003e Is this a change to the current loop when spying on `super_len()` or is this a new loop to be added somewhere else (and, if so, where)?\n\nfor you to decide, but I think the `super_len` as common to all uploads (blobs or zarr files, right?) would be the best place albeit a fragile approach. Alternatively -- just the extra check for all places were we might be uploading a file.","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1701720464,"metadata":{"github-id":"IC_kwDODBZtRc5touqR","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839393425"},"message":"Yes there seems to be some problems with NFS... However only two of the exceptions seem to be related to the `super_len` thing. The other exceptions report \"The Content-MD5 you specified did not match what we received\"\n\n```\nuname -a\nLinux atlante 5.4.0-166-generic #183-Ubuntu SMP Mon Oct 2 11:28:33 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n\nmount | grep NIH\n192.168.2.200:/pool_1/DataVol1/Public on /mnt/NIH type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,soft,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=192.168.2.200,mountvers=3,mountport=20048,mountproto=udp,local_lock=none,addr=192.168.2.200,_netdev)\n\n```","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1701722642,"metadata":{"github-id":"IC_kwDODBZtRc5to7SG","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839445126"},"message":"\u003cdetails\u003e\n\u003csummary\u003eoh -- indeed! And 3 errors point to the same expected and calculated one\u003c/summary\u003e \n\n```shell\n❯ grep 'The Content-MD5 you specified did' log.txt | sort\n\u003cError\u003e\u003cCode\u003eBadDigest\u003c/Code\u003e\u003cMessage\u003eThe Content-MD5 you specified did not match what we received.\u003c/Message\u003e\u003cExpectedDigest\u003e4415dc11527da709bcb9729ce68c642b\u003c/ExpectedDigest\u003e\u003cCalculatedDigest\u003efQafc/jEmCnHsiz5Q1YzJw==\u003c/CalculatedDigest\u003e\u003cRequestId\u003eJ6XF28CCXZM6FBFP\u003c/RequestId\u003e\u003cHostId\u003eIzKVTFkEhPhHVH+EI5a73d+EnjZbHImw2EcCXgO8mdd8O20IDYdTf90/WdZqiz0AMHIUuCTFzVs=\u003c/HostId\u003e\u003c/Error\u003e\n\u003cError\u003e\u003cCode\u003eBadDigest\u003c/Code\u003e\u003cMessage\u003eThe Content-MD5 you specified did not match what we received.\u003c/Message\u003e\u003cExpectedDigest\u003e4415dc11527da709bcb9729ce68c642b\u003c/ExpectedDigest\u003e\u003cCalculatedDigest\u003efQafc/jEmCnHsiz5Q1YzJw==\u003c/CalculatedDigest\u003e\u003cRequestId\u003eJ6XF28CCXZM6FBFP\u003c/RequestId\u003e\u003cHostId\u003eIzKVTFkEhPhHVH+EI5a73d+EnjZbHImw2EcCXgO8mdd8O20IDYdTf90/WdZqiz0AMHIUuCTFzVs=\u003c/HostId\u003e\u003c/Error\u003e\n\u003cError\u003e\u003cCode\u003eBadDigest\u003c/Code\u003e\u003cMessage\u003eThe Content-MD5 you specified did not match what we received.\u003c/Message\u003e\u003cExpectedDigest\u003e4415dc11527da709bcb9729ce68c642b\u003c/ExpectedDigest\u003e\u003cCalculatedDigest\u003efQafc/jEmCnHsiz5Q1YzJw==\u003c/CalculatedDigest\u003e\u003cRequestId\u003eJ6XF28CCXZM6FBFP\u003c/RequestId\u003e\u003cHostId\u003eIzKVTFkEhPhHVH+EI5a73d+EnjZbHImw2EcCXgO8mdd8O20IDYdTf90/WdZqiz0AMHIUuCTFzVs=\u003c/HostId\u003e\u003c/Error\u003e\n\u003cError\u003e\u003cCode\u003eBadDigest\u003c/Code\u003e\u003cMessage\u003eThe Content-MD5 you specified did not match what we received.\u003c/Message\u003e\u003cExpectedDigest\u003e823a57a8016c1d44d24794c7399e0ca9\u003c/ExpectedDigest\u003e\u003cCalculatedDigest\u003egv2hsINH/+jlNy0QpKkeLw==\u003c/CalculatedDigest\u003e\u003cRequestId\u003eHDJQP9RNYP68WF4K\u003c/RequestId\u003e\u003cHostId\u003eSBMp22+m3Mh0pGpuZe0UXpZE8KmHWopgSNxgXMCOcFj1DciCu9R/6lx3RK8RdDZbz2EGLvxo/CU=\u003c/HostId\u003e\u003c/Error\u003e\n\u003cError\u003e\u003cCode\u003eBadDigest\u003c/Code\u003e\u003cMessage\u003eThe Content-MD5 you specified did not match what we received.\u003c/Message\u003e\u003cExpectedDigest\u003e8963fa526c5be1e0f57b95ffd2e9517c\u003c/ExpectedDigest\u003e\u003cCalculatedDigest\u003ez2Pn5UdgjoSjNHmg28/H9Q==\u003c/CalculatedDigest\u003e\u003cRequestId\u003e6B8D87Z9MP1E9PNX\u003c/RequestId\u003e\u003cHostId\u003ea8TVjSov3SapE8lhR3O3tSkXIrZ0EB6qJTCaCl/K4u2rKfySDjUey/txKBl1osjBUW85jKWDg3E=\u003c/HostId\u003e\u003c/Error\u003e\n\u003cError\u003e\u003cCode\u003eBadDigest\u003c/Code\u003e\u003cMessage\u003eThe Content-MD5 you specified did not match what we received.\u003c/Message\u003e\u003cExpectedDigest\u003eab1741f39527631526ea36d12924ae0c\u003c/ExpectedDigest\u003e\u003cCalculatedDigest\u003eyfoqltT4y8YiKeOtOkiPAg==\u003c/CalculatedDigest\u003e\u003cRequestId\u003e2CJE832QVMET5K1G\u003c/RequestId\u003e\u003cHostId\u003e2D6S5wTB4d459i2Nc2FW9kMa0T4Fm7JparIFluz1E+088UtUArc0Kush8QmvUKmjtnlccp6Pxtw=\u003c/HostId\u003e\u003c/Error\u003e\n\u003cError\u003e\u003cCode\u003eBadDigest\u003c/Code\u003e\u003cMessage\u003eThe Content-MD5 you specified did not match what we received.\u003c/Message\u003e\u003cExpectedDigest\u003efa994fde9c8f7863ecbdaa34cd9b52d4\u003c/ExpectedDigest\u003e\u003cCalculatedDigest\u003eJRpvUxoiyfX7dKr6pB3t3w==\u003c/CalculatedDigest\u003e\u003cRequestId\u003e7VE63P018K6BCNSC\u003c/RequestId\u003e\u003cHostId\u003eF4snlJfV9fA+3fOUE2ISiRIdxWSmGQGtfxV0z2dEzWQFnBFPZYfcl/ouwxPEdXFCQUWdG9MVgY0=\u003c/HostId\u003e\u003c/Error\u003e\n\n```\n\u003c/details\u003e\n\nnot sure why it reports calculated in base64 while original in hex, but here is detail on first instance\n\n```\n2023-12-02T03:10:46+0100 [ERROR   ] dandi 3030432:140067982329600 Error uploading /NIH/DANDI/000026/sub-I62/ses-SPIM/micr/sub-I62_ses-SPIM_sample-BrocaAreaS14_stain-NeuN_SPIM.ome.zarr:\nTraceback (most recent call last):\n  File \"/opt/lib/python3.8/site-packages/dandi/upload.py\", line 357, in process_path\n    for r in dfile.iter_upload(   \n  File \"/opt/lib/python3.8/site-packages/dandi/files/zarr.py\", line 481, in iter_upload\n    size = fut.result()\n  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 437, in result\n    return self.__get_result()\n  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n    raise self._exception\n  File \"/usr/lib/python3.8/concurrent/futures/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/lib/python3.8/site-packages/dandi/files/zarr.py\", line 555, in _upload_zarr_file\n    storage_session.put( \n  File \"/opt/lib/python3.8/site-packages/dandi/dandiapi.py\", line 315, in put\n    return self.request(\"PUT\", path, **kwargs)\n  File \"/opt/lib/python3.8/site-packages/dandi/dandiapi.py\", line 275, in request\n    raise requests.HTTPError(msg, response=result)\nrequests.exceptions.HTTPError: Error 400 while sending PUT request to https://dandiarchive.s3.amazonaws.com/zarr/87c8b64e-6029-41fc-9640-85f85460fa1e/0/0/52/35?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=AKIAUBRWC5GAEKH3223E%2F20231202%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20231202T021034Z\u0026X-Amz-Expires=600\u0026X-Amz-SignedHeaders=content-md5%3Bhost%3Bx-amz-acl\u0026X-Amz-Signature=869bd8f5c70ccfb2f4dedf053ab228afc1ba0bc1d4bf9c8aaf4796752010821c: \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\n\u003cError\u003e\u003cCode\u003eBadDigest\u003c/Code\u003e\u003cMessage\u003eThe Content-MD5 you specified did not match what we received.\u003c/Message\u003e\u003cExpectedDigest\u003efa994fde9c8f7863ecbdaa34cd9b52d4\u003c/ExpectedDigest\u003e\u003cCalculatedDigest\u003eJRpvUxoiyfX7dKr6pB3t3w==\u003c/CalculatedDigest\u003e\u003cRequestId\u003e7VE63P018K6BCNSC\u003c/RequestId\u003e\u003cHostId\u003eF4snlJfV9fA+3fOUE2ISiRIdxWSmGQGtfxV0z2dEzWQFnBFPZYfcl/ouwxPEdXFCQUWdG9MVgY0=\u003c/HostId\u003e\u003c/Error\u003e\n```\n\nof such... I wonder if it could be for that reason of incorrect size reported first?  I guess could be tested by incrementally passing longer and longer portions of the file and see if we hit that \"ExpectedDigest\" (md5 since single part upload?)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701722805,"metadata":{"github-id":"IC_kwDODBZtRc5to8Oy","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839449010"},"message":"@yarikoptic \n\n\u003e right, but if initial (before upload commenced) read `stat`s of size were wrong (incomplete) of value X bytes, file read only to X bytes, and upload stopped at `Content-Length` of X bytes -- from AWS perspective all would be good. With extra check we would just ensure that the size didn't silently change during upload.\n\nI don't think the size reported by `stat` plays any part in locally calculating the MD5 digest.  The client just reads bytes from the file via Python's normal file-reading mechanisms.\n\n\u003e for you to decide, but I think the `super_len` as common to all uploads (blobs or zarr files, right?) would be the best place albeit a fragile approach. Alternatively -- just the extra check for all places were we might be uploading a file.\n\nThe `super_len` instrumentation (when enabled via the envvar) is currently applied to all HTTP requests with bodies (i.e., all uploads) made during a run of `dandi upload`.  With that in mind, is the only change you want for the sleep intervals to start out smaller and gradually increase to a maximum of two seconds?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1701725690,"metadata":{"github-id":"IC_kwDODBZtRc5tpMqj","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1839516323"},"message":"\u003e \u003e right, but if initial (before upload commenced) read `stat`s of size were wrong (incomplete) of value X bytes, file read only to X bytes, and upload stopped at `Content-Length` of X bytes -- from AWS perspective all would be good. With extra check we would just ensure that the size didn't silently change during upload.\n\u003e \n\u003e I don't think the size reported by `stat` plays any part in locally calculating the MD5 digest. The client just reads bytes from the file via Python's normal file-reading mechanisms.\n\nGiven that we see those other \"digest based\" errors too, I personally would not make any bets on interplay between stat's size and IO reads... adding a check is very cheap, so why not to do it?  By any chance do you see anything suggesting of size changes already in those logs we have associated with MD5 digests mismatch?\n\n\n\n\u003e \u003e for you to decide, but I think the `super_len` as common to all uploads (blobs or zarr files, right?) would be the best place albeit a fragile approach. Alternatively -- just the extra check for all places were we might be uploading a file.\n\u003e \n\u003e The `super_len` instrumentation (when enabled via the envvar) is currently applied to all HTTP requests with bodies (i.e., all uploads) made during a run of `dandi upload`. With that in mind, is the only change you want for the sleep intervals to start out smaller and gradually increase to a maximum of two seconds?\n\nI think so, and making it non-conditional on that dedicated env var `DANDI_DEVEL_INSTRUMENT_REQUESTS_SUPERLEN`","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701790492,"metadata":{"github-id":"IC_kwDODBZtRc5tu_wD","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1841036291"},"message":"@yarikoptic\n\n\u003e Given that we see those other \"digest based\" errors too, I personally would not make any bets on interplay between stat's size and IO reads... adding a check is very cheap, so why not to do it?\n\nSo what exactly is the client supposed to check after uploading a file?  Should it check that the size of the local file is the same as at the start of the upload?  I don't really see the value in that.\n\n\u003e By any chance do you see anything suggesting of size changes already in those logs we have associated with MD5 digests mismatch?\n\nOther than the digest mismatches themselves, no.\n\n\u003e I think so, and making it non-conditional on that dedicated env var `DANDI_DEVEL_INSTRUMENT_REQUESTS_SUPERLEN`\n\nDo you mean that you want `super_len()` to always be spied on, without the need to set the envvar?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1701800048,"metadata":{"github-id":"IC_kwDODBZtRc5twMUZ","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1841349913"},"message":"\u003e \u003e Given that we see those other \"digest based\" errors too, I personally would not make any bets on interplay between stat's size and IO reads... adding a check is very cheap, so why not to do it?\n\u003e \n\u003e So what exactly is the client supposed to check after uploading a file? Should it check that the size of the local file is the same as at the start of the upload? I don't really see the value in that.\n\nIt would have (without our extra checks for size 0) caught that size change (e.g. from 0) at the beginning to some other value.  And that is what I want us to do here -- to add such a trivial and basic check, so if we get md5 digest mismatch, depending on either we get this error too we would know if it was merely due to size change or something else. \n\n\u003e \u003e I think so, and making it non-conditional on that dedicated env var `DANDI_DEVEL_INSTRUMENT_REQUESTS_SUPERLEN`\n\u003e \n\u003e Do you mean that you want `super_len()` to always be spied on, without the need to set the envvar?\n\nyes, if that is the easiest way to provision check of the size to get consistent reading for `super_len`","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1701805062,"metadata":{"github-id":"IC_kwDODBZtRc5twwaq","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1841497770"},"message":"@yarikoptic \n\n\u003e It would have (without our extra checks for size 0) caught that size change (e.g. from 0) at the beginning to some other value. And that is what I want us to do here -- to add such a trivial and basic check, so if we get md5 digest mismatch, depending on either we get this error too we would know if it was merely due to size change or something else.\n\nSo, if \u0026 only if there's a digest mismatch (which currently already causes an exception), the upload code should stat the uploaded file and check whether its size is the same as at the beginning of the upload?  If it's not the same, should that result in a log message or an alternative exception or what?\n\nSide note: When uploading asset blobs (but not Zarr entries), the client computes a digest before uploading, sends the digest to the Archive, does the upload, and then compares the digest reported by S3 against the earlier digest, erroring if they differ.  I don't know whether the Archive or S3 would error before the client if they detect a digest mismatch, but if they do, should the client try to respond to those errors as well by checking for changes in file size?\n\n\u003e yes, if that is the easiest way to provision check of the size to get consistent reading for `super_len`\n\nI'm unclear what changes you want me to make around `super_len`.  Are you currently asking (a) for `super_len` to always be spied on, and (b) for the inter-check sleep for 0-sized files to start out at some small interval and increase up to a maximum of two seconds?  Something else?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1702073295,"metadata":{"github-id":"IC_kwDODBZtRc5uJMWb","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1847903643"},"message":"\u003e So, if \u0026 only if there's a digest mismatch (which currently already causes an exception), the upload code should stat the uploaded file and check whether its size is the same as at the beginning of the upload? \n\nnope, size check to be always done, regardless if there was an error with upload or not. \n\n\u003e If it's not the same, should that result in a log message or an alternative exception or what?\n\n If there is already some other problem with upload (md5 mismatch or another) - we can just log error about size difference. If there is no other error -- can raise exception about size change instead (or in addition) to logging.\n\n\u003e  I don't know whether the Archive or S3 would error before the client if they detect a digest mismatch, but if they do, should the client try to respond to those errors as well by checking for changes in file size?\n\nper above indeed let's check for size change always.\n\n\u003e \u003e \u003e \u003e I think so, and making it non-conditional on that dedicated env var `DANDI_DEVEL_INSTRUMENT_REQUESTS_SUPERLEN`\n\u003e \u003e \u003e \n\u003e \u003e \u003e \n\u003e \u003e \u003e Do you mean that you want `super_len()` to always be spied on, without the need to set the envvar?\n\u003e \u003e \n\u003e \u003e yes, if that is the easiest way to provision check of the size to get consistent reading for `super_len`\n\u003e\n\u003e I'm unclear what changes you want me to make around super_len. Are you currently asking (a) for super_len to always be spied on, and (b) for the inter-check sleep for 0-sized files to start out at some small interval and increase up to a maximum of two seconds? Something else?\n\nI was just replying that if you see spying on `super_len` to be the easiest way to provide desired \"no change in size\" checks -- do it that way. If spying is not needed/desired, implement it without spying. And yes -- for 0-sized files to check up to 2 seconds.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1702302936,"metadata":{"github-id":"IC_kwDODBZtRc5uRrOJ","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1850127241"},"message":"@yarikoptic These asides on what changes should be made have gone on too long and have become too hard to follow.  Please create a separate issue describing what changes I should make.  Please make the initial comment of the issue entirely \u0026 completely self-contained so that it is unnecessary to refer back to this thread to understand what is being asked.  If the change regarding looping for 0-sized files is supposed to be a separate thing from the check that file sizes didn't change, please create two issues instead.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1702498344,"metadata":{"github-id":"IC_kwDODBZtRc5ui39_","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1854635903"},"message":"@gmazzamuto please try current master version -- after https://github.com/dandi/dandi-cli/pull/1374 there should be even more checks/logs and it will be slower for anyone having 0-length files ;-)  no need for dedicated env var really but if you set -- should not hurt, and would add mystery if we again hit it at that level.","files":null},{"type":3,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1712138067,"metadata":{"github-id":"IC_kwDODBZtRc55PhUc","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-2034111772"},"message":"Dear all, \n\na quick update just to let you know that with version 0.61.2 I still get the very same errors. Attached is an excerpt from the log (which would be otherwise huge, btw).\n\n[20240402073313Z-1281068.log.gz](https://github.com/dandi/dandi-cli/files/14850238/20240402073313Z-1281068.log.gz)\n\nMaybe as you were suggesting the problem is linked to the data being on an NFS share? That's strange because I do all sorts of data processing on this data and I have never encountered any issue of this kind.\n\nI have a few more batches of data to upload, if you want me to try different things just let me know!\n\nCheers\nGiacomo","files":null},{"type":6,"author":{"id":"b47d70447f48e9670debced52b0983080ea7199d"},"timestamp":1712138092,"metadata":{"github-id":"UCE_lALODBZtRc55PhUczkNoxEU"},"target":"3b650f9805dd21c1d759716d7312c498d5eb743e5e8e5d428679b8ecb7c9c915","message":"Dear all, \n\na quick update just to let you know that with version 0.61.2 I still get the very same errors. Attached is an excerpt from the log (which would be otherwise huge, btw).\n\n[20240402073313Z-1281068.log.gz](https://github.com/dandi/dandi-cli/files/14850238/20240402073313Z-1281068.log.gz)\n\nMaybe as you were suggesting the problem is linked to the data being on an NFS share? That's strange because I do all sorts of processing on this data and I have never encountered any issue of this kind.\n\nI have a few more batches of data to upload, if you want me to try different things just let me know!\n\nCheers\nGiacomo","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1712149674,"metadata":{"github-id":"IC_kwDODBZtRc55ROlW","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-2034559318"},"message":"@jwodder please review the logs and see if you catch the reason for the problems","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1712331674,"metadata":{"github-id":"IC_kwDODBZtRc55mdTk","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-2040124644"},"message":"@yarikoptic Per the given logs:\n\n* 7 uploads failed with status 400 and the message \"The Content-MD5 you specified did not match what we received.\"\n* 4 uploads failed with status 501.\n* 3 uploads failed with status 403 and the message \"Request has expired\"\n\nFor the first two types, I stand by my analysis above in https://github.com/dandi/dandi-cli/issues/1257#issuecomment-1490448215.  The last type presumably happened due to something being slow.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1712625806,"metadata":{"github-id":"IC_kwDODBZtRc551IE9","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-2043969853"},"message":"For all of those I think we should try to just include those URLs into the next batch (or create a new batch if that was the last one) of upload URLs to try to reattempt with \"current\" opinion of the filesystem about them.  Let's permit to up to 3 such \"attempts\" (with re-minting upload URL).","files":null},{"type":5,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1716420035,"metadata":{"github-id":"LE_lADODBZtRc5h6XKnzwAAAAMA_rKU"},"added":["HIFNI"],"removed":[]},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1716482248,"metadata":{"github-id":"IC_kwDODBZtRc5-0Krb","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-2127604443"},"message":"@yarikoptic Exactly what errors should cause the Zarr entry upload to be re-batched (especially now that [the HIFNI thing may be solved](https://github.com/dandi/dandi-cli/pull/1444))?  For the Content-MD5 mismatches, should the client redigest the local file?\n\n@jjnesbitt If a Zarr asset upload to S3 fails with \"Request has expired,\" is resubmitting it to `/zarr/{zarr_id}/files/` the right way (or even a correct way) to retry it?  What if the client resubmits a file that wasn't successfully uploaded due to a digest mismatch?","files":null},{"type":3,"author":{"id":"a9e2633aeb6d7e2366a97d09712312c2442c6054"},"timestamp":1716482735,"metadata":{"github-id":"IC_kwDODBZtRc5-0OaR","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-2127619729"},"message":"\u003e @jjnesbitt If a Zarr asset upload to S3 fails with \"Request has expired,\" is resubmitting it to `/zarr/{zarr_id}/files/` the right way (or even a correct way) to retry it? What if the client resubmits a file that wasn't successfully uploaded due to a digest mismatch?\n\nYes that would be the correct way to retry it. Internally we track no state about a zarr asset, and so re-requesting the pre-signed upload URL for the same file multiple times is not an issue. The worst thing that would happen is an overwrite of the existing file, if it's already been successfully uploaded.","files":null},{"type":6,"author":{"id":"a9e2633aeb6d7e2366a97d09712312c2442c6054"},"timestamp":1716482768,"metadata":{"github-id":"UCE_lALODBZtRc5-0OaRzkb1BSc"},"target":"44c3f53351696169fab86d8f5abdd29c365183311a4b4df3007eeac5a623609e","message":"\u003e @jjnesbitt If a Zarr asset upload to S3 fails with \"Request has expired,\" is resubmitting it to `/zarr/{zarr_id}/files/` the right way (or even a correct way) to retry it? What if the client resubmits a file that wasn't successfully uploaded due to a digest mismatch?\n\nYes that would be the correct way to retry it. Internally we track no state about a zarr asset (that is, a file _within_ a zarr), and so re-requesting the pre-signed upload URL for the same file multiple times is not an issue. The worst thing that would happen is an overwrite of the existing file, if it's already been successfully uploaded.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1716491910,"metadata":{"github-id":"IC_kwDODBZtRc5-1Kbk","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-2127865572"},"message":"\u003e @yarikoptic Exactly what errors should cause the Zarr entry upload to be re-batched (especially now that [the HIFNI thing may be solved](https://github.com/dandi/dandi-cli/pull/1444))? For the Content-MD5 mismatches, should the client redigest the local file?\n\nI do not remember the complete list of problems -- I guess those which we coded for so far in the code. You reminded correctly on Content-MD5 mismatch -- since we would not know the reason for it, indeed would make sense to redigest local file as well.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1716494069,"metadata":{"github-id":"IC_kwDODBZtRc5-1Wzh","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-2127916257"},"message":"@yarikoptic \n\n\u003e I do not remember the complete list of problems\n\n[The comment before your last one might be a start.](https://github.com/dandi/dandi-cli/issues/1257#issuecomment-2040124644)\n\n\u003e I guess those which we coded for so far in the code.\n\nAside from the HIFNI stuff, the only such errors I can think of are the 5xx errors that we retry on, but if the repeated retrying fails, I don't think adding even more retrying is the best idea.","files":null},{"type":5,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1721326112,"metadata":{"github-id":"LE_lADODBZtRc5h6XKnzwAAAAMoRdh7"},"added":["zarr"],"removed":[]},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1755096098,"metadata":{"github-id":"IC_kwDODBZtRc69y1lM","github-url":"https://github.com/dandi/dandi-cli/issues/1257#issuecomment-3184220492"},"message":"\u003e * 400 errors caused by a digest mismatch when uploading.  Unless something was modifying the local files during the upload process, I also blame this on the file system.\n\nin later  https://github.com/dandi/dandi-cli/issues/1662#issuecomment-3179866481 we encountered the same but it was producing consistently different ETag across retries, so IMHO it shouldn't be filesystem... may be upload chunking varied or smth like that.\n\nmight be related or not but reducing `--jobs` to 5:5 from 15:15 also lead to successful upload later on.\n\n\u003e 3 uploads failed with status 403 and the message \"Request has expired\"\n\nfor that one we implemented and merged fix in #1674 \n\nOverall, I think, we still can encounter those 400s on mismatch, but this issue is old, so let's continue in #1662","files":null},{"type":4,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1755096098,"metadata":{"github-id":"CE_lADODBZtRc5h6XKnzwAAAAR0yFXG"},"status":2}]}