{"version":2,"ops":[{"type":1,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1607361159,"metadata":{"github-id":"MDU6SXNzdWU3NTg2OTEyNTU=","github-url":"https://github.com/dandi/dandi-cli/issues/309","origin":"github"},"title":"enable upload of files larger than current limit of 65G","message":"Underlying issue is multifaceted ATM, it is in \n- the girder S3 backend support: it does not handle pagination of responses from S3 for multi-part uploads (https://github.com/girder/girder/issues/3318), so handles only the first 1,000 of them (out of possible up to 10,000)\n- the girder client: hardcodes chunk size to `MAX_CHUNK_SIZE = 1024 * 1024 * 64`\n\nso we are ending up with about 64G (`1000!=1024` ;)) maximal size.  I am not sure how long it would take for both girder server **and** client to get fixed up, or for our API server (re)implementation to come to replace current girder based setup, so I think we should better just workaround to raise the limit of maximal file size users could upload *now* since we already do have such use cases.\n\nS3 allows for upload of chunks up to 5GB (current - 64GB), but we are working through nginx, so there is also a limit on the size of the PUT request (let's call it `NGINX_MAX_CHUNK_SIZE`).  Current limit is set to 100MB but probably could be raised to 400MB (@mgrauer will check if anything forbidding that).\n\nSo the plan would be\n- have a test in place to verify that we do fail ATM (requested as part of the https://github.com/dandi/dandi-cli/pull/306#issuecomment-740030234)\n- resurrect https://github.com/dandi/dandi-cli/pull/139 (or just reimplement more cleanly?) to be able to \n   - manually set the chunk size\n   - switch from default 64M (`GirderClient.MAX_CHUNK_SIZE`) size to  `max(64M, filesize/1000)` if we could tune CHUNK_SIZE on per file basis.  If that one exceeds `NGINX_MAX_CHUNK_SIZE` (we should add a constant I guess, although ideally server should provide such information -- added to now transferred https://github.com/dandi/dandi-cli/issues/308).  But if we could not base it on file-size, we could just go for `NGINX_MAX_CHUNK_SIZE` for the value of `GirderClient.MAX_CHUNK_SIZE`.  If with a given chunksize we need more than 1000 chunks -- we raise an exception (and remove our ad-hoc check on filesize ATM).  With 400M chunk we should be able to handle uploads of up to 400G which should cover current use cases.","files":null}]}