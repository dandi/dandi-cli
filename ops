{"version":2,"ops":[{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601651949,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlRWRpdDo0MzY2NTkxNTg="},"target":"9371a7c362700f7ab647e58a0339fcc99c865b0e692e4db84c30d801f6226d74","message":"Continuation to #243 which was addressed by providing/running https://github.com/dandi/dandi-cli/blob/master/tools/instantiate-dandisets.py .\n\nWhile API is still being cooking, I think it would be already beneficial to start providing proper DataLad datasets for all our dandisets.  \n\n# Short term implementation\n\nBuild on top of `instantiate-dandisets.py` so it would  (option names etc could be not exactly correct, just typing)\n\n- not just `mkdir` but use `ds = Dataset(path).create(cfg='text2git')` (if there is none) to establish a datalad dataset and configure text files, such as `dandiset.yaml` to go to git (everything is public anyways ATM and .nwb's are binary)\n  - by default datalad makes annex to use md5 based backend... our API ATM, and also git-annex by default uses SHA256.  So even though I would have liked shorter/faster SHA1, let's go with sha256e backend for annex, so create above should specify that one to use\n- after `cp`ing file from the local assetstore \n  - `ds.repo.add` it\n  - if metadata had digests\n  - `ds.repo.add_url_to_file` with URL pointing to redirected to location in the bucket , not girder API one so it still has a chance to work after girder is brought down but we still have assetstore in current shape (actually it would be even easier since script already works based on the path in the asset store!)\n- at the end -- just `ds.save` all the changes\n\nBut then add logic for proper updates \n- we should not re-cp/add file if did not change \n  - if metadata contains digest, we could base off that\n  - if not - we could use size/mtime to check if file was updated on dandi\n- if file (directory) is removed on dandi, we should remove locally\n\nCrawler (next section) has/uses https://github.com/datalad/datalad-crawler/blob/master/datalad_crawler/dbs/files.py#L89 to store/update information on each file status, but I guess we might just avoid all of that if we just store the full dump of assets listing metadata somewhere under `.datalad/dandi/` so that next time we run it we have a full list of files/assets from previous update to perform \"update\" actions listed above.\n\n# Alternative (a bit more involved, not sure if worthwhile ATM)\n\nCan be a \"datalad crawler\" pipeline (see https://github.com/datalad/datalad-crawler/), which would (internally) \n\n- take care about checking time/size \n   - might need to be extended to support check by digest\n- persistently storing that information per file (which it does already)\n\nBut since datalad-crawler, although functioning etc, is yet another thing to figure out and still uses older DataLad interfaces, primarily talking directly via GitRepo and AnnexRepo interfaces and without really taking advantage of higher level ones, I think it might be a bigger undertaking.\n\nNevertheless here are some pointers on that\n\n- very basic and scarce docs about \"crawler\" and its nodes and pipelines:  http://docs.datalad.org/projects/crawler/en/stable/basics.html  \n- https://github.com/datalad/datalad-crawler/blob/master/datalad_crawler/pipelines/xnat.py#L375 - a sample simple pipeline for XNAT.  It all boils down to just a few helpers to get listing, and pass it into annexificator\n- https://github.com/datalad/datalad-crawler/blob/master/datalad_crawler/pipelines/simple_with_archives.py#L28 -- one of the \"simple\" pipelines, which actually is not that simple since it implements 3-branch approach (`incoming` -\u003e `incoming-processed` -\u003e `master`) to support working with data from tarballs, and is \"reused\" in other pipelines I believe.\n- there was some thinking/idea about RFing or just providing a new (more modernly designed) pipeline which would \"integrate\" with `datalad addurls` -- some older thinking is at https://github.com/datalad/datalad-crawler/issues/22 .  So we might get there -- it could be just combining two calls - get assets list from dandi, tune them up just a bit, pass to the stock pipeline which would take care about doing all the time checks, removal of now obsolete files, and then passing the rest to addurls to do the rest (possibly splitting into subdatasets etc)\n\n## Edit 1: publishing\n\nUpon the end of update, script should `publish` all updated dandisets under https://github.com/dandisets organization (for testing, may be first create some throw away organization on github, e.g. dandisets-testing or alike).\n\nThere are `datalad` `publish` and `push` commands with slight differences, `push` is supposedly cleaner interface so probably use that one.  There is also `create-sibling-github` to initiate a github repository to `push` to.\n\n# Longer term\nAlso, I hope that would just start updating datalad datasets straight within API backend/workers reacting to API calls thus making datalad datasets immediately reflecting introduced changes. For that we would not need a dedicated script or a crawler.","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601653719,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlRWRpdDo0MzY2NzI2NTk="},"target":"9371a7c362700f7ab647e58a0339fcc99c865b0e692e4db84c30d801f6226d74","message":"Continuation to #243 which was addressed by providing/running https://github.com/dandi/dandi-cli/blob/master/tools/instantiate-dandisets.py .\n\nWhile API is still being cooking, I think it would be already beneficial to start providing proper DataLad datasets for all our dandisets.  \n\n# Short term implementation\n\nBuild on top of `instantiate-dandisets.py` so it would  (option names etc could be not exactly correct, just typing)\n\n- not just `mkdir` but use `ds = Dataset(path).create(cfg='text2git')` (if there is none) to establish a datalad dataset and configure text files, such as `dandiset.yaml` to go to git (everything is public anyways ATM and .nwb's are binary)\n  - by default datalad makes annex to use md5 based backend... our API ATM, and also git-annex by default uses SHA256.  So even though I would have liked shorter/faster SHA1, let's go with sha256e backend for annex, so create above should specify that one to use\n- after `cp`ing file from the local assetstore \n  - `ds.repo.add` it\n  - if metadata had digests - verify that annex computed checksum in its key is the same as the one we know in digests. If not -- crash, we better figure out what is going on\n  - `ds.repo.add_url_to_file` with URL pointing to redirected to location in the bucket , not girder API one so it still has a chance to work after girder is brought down but we still have assetstore in current shape (actually it would be even easier since script already works based on the path in the asset store!)\n- at the end -- just `ds.save` all the changes\n\nBut then add logic for proper updates \n- we should not re-cp/add file if did not change \n  - if metadata contains digest, we could base off that\n  - if not - we could use size/mtime to check if file was updated on dandi\n- if file (directory) is removed on dandi, we should remove locally\n\nCrawler (next section) has/uses https://github.com/datalad/datalad-crawler/blob/master/datalad_crawler/dbs/files.py#L89 to store/update information on each file status, but I guess we might just avoid all of that if we just store the full dump of assets listing metadata somewhere under `.datalad/dandi/` so that next time we run it we have a full list of files/assets from previous update to perform \"update\" actions listed above.\n\n# Alternative (a bit more involved, not sure if worthwhile ATM)\n\nCan be a \"datalad crawler\" pipeline (see https://github.com/datalad/datalad-crawler/), which would (internally) \n\n- take care about checking time/size \n   - might need to be extended to support check by digest\n- persistently storing that information per file (which it does already)\n\nBut since datalad-crawler, although functioning etc, is yet another thing to figure out and still uses older DataLad interfaces, primarily talking directly via GitRepo and AnnexRepo interfaces and without really taking advantage of higher level ones, I think it might be a bigger undertaking.\n\nNevertheless here are some pointers on that\n\n- very basic and scarce docs about \"crawler\" and its nodes and pipelines:  http://docs.datalad.org/projects/crawler/en/stable/basics.html  \n- https://github.com/datalad/datalad-crawler/blob/master/datalad_crawler/pipelines/xnat.py#L375 - a sample simple pipeline for XNAT.  It all boils down to just a few helpers to get listing, and pass it into annexificator\n- https://github.com/datalad/datalad-crawler/blob/master/datalad_crawler/pipelines/simple_with_archives.py#L28 -- one of the \"simple\" pipelines, which actually is not that simple since it implements 3-branch approach (`incoming` -\u003e `incoming-processed` -\u003e `master`) to support working with data from tarballs, and is \"reused\" in other pipelines I believe.\n- there was some thinking/idea about RFing or just providing a new (more modernly designed) pipeline which would \"integrate\" with `datalad addurls` -- some older thinking is at https://github.com/datalad/datalad-crawler/issues/22 .  So we might get there -- it could be just combining two calls - get assets list from dandi, tune them up just a bit, pass to the stock pipeline which would take care about doing all the time checks, removal of now obsolete files, and then passing the rest to addurls to do the rest (possibly splitting into subdatasets etc)\n\n## Edit 1: publishing\n\nUpon the end of update, script should `publish` all updated dandisets under https://github.com/dandisets organization (for testing, may be first create some throw away organization on github, e.g. dandisets-testing or alike).\n\nThere are `datalad` `publish` and `push` commands with slight differences, `push` is supposedly cleaner interface so probably use that one.  There is also `create-sibling-github` to initiate a github repository to `push` to.\n\n# Longer term\nAlso, I hope that would just start updating datalad datasets straight within API backend/workers reacting to API calls thus making datalad datasets immediately reflecting introduced changes. For that we would not need a dedicated script or a crawler.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601652205,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMjc5NjMzNw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-702796337"},"message":"* By \"Build on top of `instantiate-dandisets.py`,\" do you mean you want a new, separate script or that `instantiate-dandisets.py` should be modified?\n* It looks like you didn't finish writing the item that reads \"if metadata had digests\".","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601652205,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA0MTU1MjU2"},"target":"2cd97764e3a61c98c5fb84d82821652760454fbe98aac0fe36c5f56fb643a4f4","message":"* By \"Build on top of `instantiate-dandisets.py`,\" do you mean you want a new, separate script or that `instantiate-dandisets.py` should be modified?\n* It looks like you didn't finish writing the item that reads \"if metadata had digests\".\n* Is there an easy way to check whether a Datalad dataset has been created, or should I just check whether the directory is nonempty?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601653916,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMjgxMjM1OQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-702812359"},"message":"- up to you, either it is an option to existing script so we could still do without datalad or a new one based on it\n- thanks -- finished writing ;)\n- yes, `.is_installed()` http://docs.datalad.org/en/stable/generated/datalad.api.Dataset.html?highlight=is_installed#datalad.api.Dataset  . Pretty much\n\n```python\nds = DataLad(path)\nif not ds.is_installed():\n   ds.create(...)\n```\nI don't think we should bother adding them all into a \"super-dataset\" ATM, so should not worry if that path might be an uninstalled submodule or smth like that.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601658247,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMjg0ODA0MA==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-702848040"},"message":"@yarikoptic What is the proper way to set git-annex backend to SHA256E?  According to [this page](https://git-annex.branchable.com/backends/), it should be doable by just changing the `annex.backend` (singular) Git config setting, yet when I look in `.git/config` in a test dataset, I see an `annex.backends = MD5E` (plural) setting.  If I set just one or the other to `SHA256E` (and even if I set `backend` and delete `backends`) and then create \u0026 save a file, running `ds.repo.get_content_annexinfo([\"path/to/file\"])` shows that the file's backend is still MD5E.\n\n(Incidentally, is there a better way of getting a file's git-annex digest than splitting apart `ds.repo.get_content_annexinfo([\"file\"])[Path(\"file\")][\"key\"]`?)","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601658247,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA0MTgwNTU1"},"target":"13fe54ae83c424837691227e0214a78c0843ecf34bd258d3b5a7c1b44afde467","message":"@yarikoptic What is the proper way to set the git-annex backend to SHA256E?  According to [this page](https://git-annex.branchable.com/backends/), it should be doable by just changing the `annex.backend` (singular) Git config setting, yet when I look in `.git/config` in a test dataset, I see an `annex.backends = MD5E` (plural) setting.  If I set just one or the other to `SHA256E` (and even if I set `backend` and delete `backends`) and then create \u0026 save a file, running `ds.repo.get_content_annexinfo([\"path/to/file\"])` shows that the file's backend is still MD5E.\n\n(Incidentally, is there a better way of getting a file's git-annex digest than splitting apart `ds.repo.get_content_annexinfo([\"file\"])[Path(\"file\")][\"key\"]`?)","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601666881,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMjkxNzIwNA==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-702917204"},"message":"backend -- apparently no easy way! kheh kheh -- see https://github.com/datalad/datalad/issues/4978 for issue and my \"workaround\"\n\ndigest -- you could use `ds.repo.get_key(path)` to get annex key, and then indeed need to parse...  https://git-annex.branchable.com/internals/key_format/ describes the format just in case ;)  we have bunch of helpers but never cared about extracting checksum (could be also generally not a checksum for URL backend etc)","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601868518,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzM3NjY5Mg==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703376692"},"message":"forgot to add: if backup has no corresponding key (we are after all backing up only once a day, I will increase frequency) - file will need to be downloaded first e.g. using datalad's `download_url` (which will also add url to the file in annex upon download) or could download using dandi (or directly) and proceed to `add_url_to_file`.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601904589,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzYzMjkxNw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703632917"},"message":"@yarikoptic The Dandi asset metadata (as returned by `navigate_url`) contains two occurrences each of the SHA256, size, and mtime fields; should I use the ones in the `.metadata` sub-dict, or the `.sha256`, `.attrs.size`, \u0026 `.attrs.mtime` fields, or does it not matter?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601906265,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzY0OTk4Ng==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703649986"},"message":"Shouldn't matter. Probably use the ones from top level\n\nRe mtime - might be cool if you use the latest mtime of files to be committed as a commit date? (One of the two, iirc there is like changes date and commit date; you could check how datalad does it for fake-dates mode)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601909643,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzY4MzQ0NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703683445"},"message":"@yarikoptic `ds.repo.add_url_to_file()` is failing with \"while adding a new url to an already annexed file, failed to verify url exists\".  Is there a way to turn off the URL verification?\n\nIncidentally, should the query parameters be stripped from URLs before associating them with files?  They're rather long, and they seem superfluous.","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601909643,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA0NTYwNTI4"},"target":"289da28ebcc61773aba9cdae6575d7d4fa48051b32b87cb69712cc994cab42e2","message":"@yarikoptic `ds.repo.add_url_to_file()` is failing with \"while adding a new url to an already annexed file, failed to verify url exists\".  Is there a way to turn off the URL verification?\n\nIncidentally, should the query parameters be stripped from URLs before associating them with files?  They're rather long, and they seem superfluous.\n\nAlso, while testing, I need to be able to delete datalad datasets, but the git-annex files don't have write permission, so `rm -rf` fails unless I use sudo, which I'm not allowed to do on drogon.  Is there a clean way around this?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601911201,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA0NTY3NTY1"},"target":"289da28ebcc61773aba9cdae6575d7d4fa48051b32b87cb69712cc994cab42e2","message":"@yarikoptic `ds.repo.add_url_to_file()` is failing with \"while adding a new url to an already annexed file, failed to verify url exists\".  Is there a way to turn off the URL verification?\n\nIncidentally, should the query parameters be stripped from URLs before associating them with files?  They're rather long, and they seem superfluous.\n\n~Also, while testing, I need to be able to delete datalad datasets, but the git-annex files don't have write permission, so `rm -rf` fails unless I use sudo, which I'm not allowed to do on drogon.  Is there a clean way around this?~  (Nevermind, found a way around this.)","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601915760,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzc0NjE3NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703746175"},"message":"I am sorry, I am not exactly following on what query parameters (example of a the url could have helped).  My guess you followed my desire to store the URL on S3 as it was redirected to by girder and all those query parameters are the expiration etc?  Then yeah -- we do not want that and URL (since I believe versioning was turned off, right @satra?) should be just a url pointing to the asset store (so we could actually even just create it ourselves without asking girder I guess, since we already interrogate girder about that to find file in a local backup).","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601915760,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA0NTg3NjIz"},"target":"e31dadbaa769e16465bff89ab6951cb0e00a224ebaa3ba2d9762db84f9b8f1ec","message":"I am sorry, I am not exactly following on what query parameters (example of a the url could have helped).  My guess you followed my desire to store the URL on S3 as it was redirected to by girder and all those query parameters are the expiration etc?  Then yeah -- we do not want that and URL (since I believe versioning was turned off, right @satra?) should be just a url pointing to the asset store (so we could actually even just create it ourselves without asking girder I guess, since we already interrogate girder about that to find file in a local backup).\n\nedit 1: we should not skip validation of the URL, or we would not have any guarantee that we did use correct, at least currently, URL","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601915717,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzc0Njc1OA==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703746758"},"message":"re removing -- `remove(path, check=False)` command of datalad could be of help, or indeed just `chmod -R +w path \u0026\u0026 rm -rf path`","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601915779,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzc0NzI5Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703747297"},"message":"@yarikoptic Here's a URL that the script is currently trying to associate with a file:\n\n```\nhttps://dandiarchive.s3.amazonaws.com/girder-assetstore/f7/65/f765fb69bd7343798fd994103c1ff2eb?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=ASIA3GIMZPVVCZ6O3AUO%2F20201005%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20201005T151533Z\u0026X-Amz-Expires=3600\u0026X-Amz-SignedHeaders=host\u0026X-Amz-Security-Token=FwoGZXIvYXdzEMD%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDOu19%2BOH%2BIcKB%2BuVWSK%2FAfTWBMZ31yfaLgHKgQ8Dc8M4em39qRwOBgau0g15MrDMGJXzjyT2qQpr5yhbeVekWhAr7J2zoZqIkhTX3Fnpyn8TgO1AW%2FlzyMjNdVOYpzWkvTibOlWOQt0I%2B9Yjt3TzDQ%2BQcvV55SHbiQwgswPX6LY06tOlppex8hnCSWxLuaXVKl64Om2r8VCEmlqHJTd0yOCt6csY4vOKgr6K2RKoYJj5lXoX%2B%2BpQLBVDm2coFiBQmiX7n9dmHqugwEuHQGY%2FKJXc7PsFMi2hgBoOSTKeYlREtO136ewJJ5MoNIQuul4Sipl90GEFmmuW7tXC2P0eDweLuOo%3D\u0026X-Amz-Signature=1e3cb8d5fb3638259a1e03e439dcf4a4a14cb6bbd82ea677f136d12f995067c6\n```","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1601916980,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzc1ODQ1MA==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703758450"},"message":"@yarikoptic - versioning has not been turned off yet. if we get full datalad datasets and can maintain that, it will be easier to turn off versioning from that information and remove older versions of things.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601917711,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzc2NTI4Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703765287"},"message":"that is good (that it is not off yet).  Then our \"make a datalad\" dataset can become more robust and not puke if someone removes a file while we are \"crawling\" it if we used versioned URLs!  So, @jwodder - let's proceed with versioned URLs!  \nThe url for that example could be seen e.g. if you do\n```\n$\u003e datalad ls -aL s3://dandiarchive/girder-assetstore/f7/65/f765fb69bd7343798fd994103c1ff2eb\nConnecting to bucket: dandiarchive\n[INFO   ] S3 session: Connecting to the bucket dandiarchive anonymously \nBucket info:\n  Versioning: {'Versioning': 'Enabled'}\n     Website: dandiarchive.s3-website-us-east-1.amazonaws.com\n         ACL: \u003cPolicy: None (owner) = FULL_CONTROL\u003e\ngirder-assetstore/f7/65/f765fb69bd7343798fd994103c1ff2eb 2020-03-19T23:05:57.000Z 12887868734 ver:V5BK7tKKBlGCdAV75e_Q98xRAWvq5NCG  acl:\u003cPolicy: None (owner) = FULL_CONTROL\u003e  http://dandiarchive.s3.amazonaws.com/girder-assetstore/f7/65/f765fb69bd7343798fd994103c1ff2eb?versionId=V5BK7tKKBlGCdAV75e_Q98xRAWvq5NCG [OK]\n```\n\nso it is http://dandiarchive.s3.amazonaws.com/girder-assetstore/f7/65/f765fb69bd7343798fd994103c1ff2eb?versionId=V5BK7tKKBlGCdAV75e_Q98xRAWvq5NCG\n\nbut programmatically -- we have `datalad.support.s3.get_versioned_url` so you could use that one for now.  But in the long run -- we need to either RF it or add some caching so it would become more efficient and not redo \"get the bucket\" for every URL from the same bucket - waste of computing/traffic.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601917757,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzc2NTcxMg==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703765712"},"message":"alternatively - you could mint it in your code yourself reusing the same bucket instance etc - up to you","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601920885,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzc5NDA1NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703794055"},"message":"@yarikoptic Getting a versioned URL isn't working for me.  Using the file in Dandiset 000027 as an example (file ID `5f176584f63d62e1dbd06946`):\n\n* Making a HEAD request to `https://girder.dandiarchive.org/api/v1/file/5f176584f63d62e1dbd06946/download` returns the location:\n\n```\nhttps://dandiarchive.s3.amazonaws.com/girder-assetstore/74/0f/740feade0d784acc8ec76bb7834d80dc?X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Credential=ASIA3GIMZPVVHNVAWVMN%2F20201005%2Fus-east-2%2Fs3%2Faws4_request\u0026X-Amz-Date=20201005T175459Z\u0026X-Amz-Expires=3600\u0026X-Amz-SignedHeaders=host\u0026X-Amz-Security-Token=FwoGZXIvYXdzEMP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDL%2BSe47eGSV94y9NmiK%2FAQE1w7nr7B52xxI5XQli3z7HheHtfgFGvnMJe1PqCml4%2BqDepzZHa%2FkvP0Ltk6lnsWpVUAiga%2FZQOlChowhXkGQ7uLU36Coe6fntpsTj2hv4UCFqFtA5qaWSmXYHLBmnc9WV3%2Bd1sxjqoD%2FcBnJZe%2FGAwDijTIiDNgJvGL4nWNmYCDL5T2DmZo9KB0QLq17SO517RCRGwG%2BAsszpiAlgtAuO3wzSVnt9%2FFaR7iWWqR4uxlzHX55PlfGoNqDiQOLCKKS57fsFMi2YrZO1UPadlwGbC6jWoe%2BvZwmaVUr%2BikFvRx5iLwAIjthpwNvVJn16oHYJD1M%3D\u0026X-Amz-Signature=f198dfa71a0e16f9c715fb5ea121f11efb7fa65c715f882154ef3510132ad9ea\n```\n\n* If I pass the lengthy URL above to `datalad.support.s3.get_versioned_url()`, it fails with:\n\n```\nTraceback (most recent call last):\n  File \"q.py\", line 15, in \u003cmodule\u003e\n    print(get_versioned_url(url))\n  File \"/home/jwodder/dandi-cli/venv/lib/python3.8/site-packages/datalad/support/s3.py\", line 434, in get_versioned_url\n    if s3provider.authenticator.bucket is not None and s3provider.authenticator.bucket.name == s3_bucket:\nAttributeError: 'NoneType' object has no attribute 'bucket'\n```\n\n* If I remove the query parameters and pass the result to `get_versioned_url()`, the same error occurs.\n\n* If I remove the query parameters, change the scheme to `s3`, and change the host to `dandiarchive` to produce the URL `s3://dandiarchive/girder-assetstore/74/0f/740feade0d784acc8ec76bb7834d80dc`, then `get_versioned_url()` fails with:\n\n```\nTraceback (most recent call last):\n  File \"q.py\", line 25, in \u003cmodule\u003e\n    print(get_versioned_url(url3))\n  File \"/home/jwodder/dandi-cli/venv/lib/python3.8/site-packages/datalad/support/s3.py\", line 424, in get_versioned_url\n    raise NotImplementedError\nNotImplementedError\n```","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601923074,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzgxMzE1MA==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703813150"},"message":"crap, \"confirmed\", needs to be filed/fixed... if you could just get custom boto (or boto3) code to get bucket, get relevant key and its most recent version -- would be great. or just proceed without versioning and I will provide a fix on datalad end later on to just add that call.","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601923074,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA0NjE3ODA5"},"target":"71705d82948edfbe30283a85e86f3769de580755a40fd6741aac17952cd96489","message":"crap, \"confirmed\", needs to be filed/fixed... if you could just get custom boto (or boto3) code to get bucket, get relevant key and its most recent version -- would be great. or just proceed without versioning and I will provide a fix on datalad end later on to just add that call.\n\nedit: filed https://github.com/datalad/datalad/issues/4984","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1601924727,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzgyOTQ0NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703829445"},"message":"@yarikoptic It seems that AWS credentials are required for getting any information about objects via boto3.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1601927083,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwMzg0ODc3Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-703848777"},"message":"there should be some way for anonymous access.\n\nAnyways -- here is the fix for datalad: https://github.com/datalad/datalad/pull/4985  (against `maint` branch)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602012328,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNDUwMjk1NA==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-704502954"},"message":"@yarikoptic GitHub siblings are being created in https://github.com/dandisets-testing, but for some reason their default branch is set to git-annex instead of master.  How do I fix that?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1602013137,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNDUwOTc3NA==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-704509774"},"message":"i think it may be because github has moved away from master as default for new repositories: https://www.zdnet.com/article/github-to-replace-master-with-main-starting-next-month/","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602075676,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNDkxOTM2Ng==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-704919366"},"message":"@yarikoptic After running for almost seven hours, the script failed while trying to add a URL to a file in 000026 with the error message:\n\n```\ndatalad.support.exceptions.CommandError: CommandError: 'addurl' [Error, annex reported failure for addurl (url='https://dandiarchive.s3.amazonaws.com/girder-assetstore/dd/ac/ddac7448a1a847d5843078d1ec772dba?versionId=8j4PrdzTK9zmi_8yewcuLqOwHR2qyc19'): {'command': 'addurl', 'success': False, 'error-messages': ['  rawdata/sub-I46/ses-MRI/anat/sub-I46_echo-1_fa-1_VFA.json already exists; not overwriting'], 'file': 'rawdata/sub-I46/ses-MRI/anat/sub-I46_echo-1_fa-1_VFA.json'}]\n```\n\nThe error message seems to be implying that the file in question was somehow added to the repository twice, but the script output does not support that interpretation.\n\nAlso, the files in 000026 (and also 000025) are not arranged in the normal Dandiset manner; should the script do anything about this?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602076543,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNDkyODE3MQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-704928171"},"message":"@yarikoptic It appears that adding a file to git-annex updates its mtime, so using mtimes to determine whether a file has been modified won't work â€” or should the script try forcing the mtime back to the \"expected\" value after adding each file?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602077772,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNDk0MDY0Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-704940647"},"message":"@satra I don't believe that's it.  If I try creating \u0026 pushing a repository with two branches, one named \"master\", one named \"test\", it's whichever is pushed first that ends up as the default branch.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602164380,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTU2ODQxNQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705568415"},"message":"mtime: oh well, git does not store mtimes anyways so for the purpose of `dandi` client determining \"freshness\" indeed we would be out of luck I guess... will be a separate issue.    As for the script to determine either re-download/update is needed, I guess it is doomed to keep a \"registry\" of files and their mtime/size (as datalad-crawler does).","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602164380,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA1MzY2ODA1"},"target":"bff123628fa3c5fd30b8acdf019ed3d4c09d63409cba4e6aaee10304add42f34","message":"mtime: oh well, git does not store mtimes anyways so for the purpose of `dandi` client determining \"freshness\" indeed we would be out of luck I guess... will be a separate issue.    As for the script to determine either re-download/update is needed, I guess it is doomed to keep a \"registry\" of files and their mtime/size (as datalad-crawler does).\n\nedit: may be forget about mtime since you rely on hash to verify if re-download is needed... any recently uploaded file should have a hash, so it would only be needed for old ones to allow to download them once without hash known","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602163939,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTU3MDcwMw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705570703"},"message":"\u003e Also, the files in 000026 (and also 000025) are not arranged in the normal Dandiset manner; should the script do anything about this?\nnope... script should not be anyhow files layout specific (besides that there should be no dandiset.yaml file on s3/list of assets)","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602163939,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA1MzY1MTQ1"},"target":"8ae5d789b52787e07583d06056732f6820a9f6ba3622280bb4fb7e9aaa45b905","message":"\u003e Also, the files in 000026 (and also 000025) are not arranged in the normal Dandiset manner; should the script do anything about this?\n\nnope... script should not be anyhow files layout specific (besides that there should be no dandiset.yaml file on s3/list of assets)","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602165094,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTU4MzA1Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705583057"},"message":"re 000026 and addurl: where (on drogon?) those are (I thought to have a look into git history)? Ideally log from the script (per each dandiset) should be dumped into a file for introspection.  Looking at the code have not spotted anything obvious, and expect log to tell more.  I thought it might be that the file was \"broken\" in/copied from assetstore \n\n\u003cdetails\u003e\n\u003csummary\u003ebut annex issues another message then\u003c/summary\u003e \n\n```shell\n$\u003e datalad create /tmp/testdsss      \n[INFO   ] Creating a new annex repo at /tmp/testdsss \n[INFO   ] Scanning for unlocked files (this may take some time) \ncreate(ok): /tmp/testdsss (dataset)\n(dev3) 1 27849.....................................:Thu 08 Oct 2020 09:48:26 AM EDT:.\nlena:/tmp\n$\u003e cd testdsss\n(dev3) 1 27851.....................................:Thu 08 Oct 2020 09:48:32 AM EDT:.\n(git-annex)lena:/tmp/testdsss[master]\n$\u003e touch ddac7448a1a847d5843078d1ec772dba\\?versionId=8j4PrdzTK9zmi_8yewcuLqOwHR2qyc19        \n(dev3) 1 27852.....................................:Thu 08 Oct 2020 09:48:40 AM EDT:.\n(git-annex)lena:/tmp/testdsss[master]\n$\u003e git annex add ddac7448a1a847d5843078d1ec772dba\\?versionId=8j4PrdzTK9zmi_8yewcuLqOwHR2qyc19\nadd ddac7448a1a847d5843078d1ec772dba?versionId=8j4PrdzTK9zmi_8yewcuLqOwHR2qyc19 \nok\n(recording state in git...)\n(dev3) 1 27853.....................................:Thu 08 Oct 2020 09:48:44 AM EDT:.\n(git-annex)lena:/tmp/testdsss[master]\n$\u003e git annex addurl --file ddac7448a1a847d5843078d1ec772dba\\?versionId=8j4PrdzTK9zmi_8yewcuLqOwHR2qyc19 'https://dandiarchive.s3.amazonaws.com/girder-assetstore/dd/ac/ddac7448a1a847d5843078d1ec772dba?versionId=8j4PrdzTK9zmi_8yewcuLqOwHR2qyc19'\naddurl https://dandiarchive.s3.amazonaws.com/girder-assetstore/dd/ac/ddac7448a1a847d5843078d1ec772dba?versionId=8j4PrdzTK9zmi_8yewcuLqOwHR2qyc19 \n  while adding a new url to an already annexed file, url does not have expected file size (use --relaxed to bypass this check) https://dandiarchive.s3.amazonaws.com/girder-assetstore/dd/ac/ddac7448a1a847d5843078d1ec772dba?versionId=8j4PrdzTK9zmi_8yewcuLqOwHR2qyc19\nfailed\ngit-annex: addurl: 1 failed\n\n```\n\u003c/details\u003e\n\nyou could also enable/dump detailed log from datalad itself (`DATALAD_LOG_LEVEL=DEBUG DATALAD_LOG_TARGET=/tmp/mylog.log` going lower than DEBUG 10 to get even more info, and adding `DATALAD_LOG_OUTPUTS=1` to log detailed outputs from underlying commands) to see what is going one.  ATM I have no further ideas","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602165474,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTU4ODgzNg==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705588836"},"message":"re default `git-annex` branch: could you please come up with minimal reproducer (probably in python to stay close to your code) which would do `.create; .create_sibling_github; .push` and demonstrate that `git-annex` branch is default. If it does -- file an issue against datalad (with WTF information) -- I think it would be something to fixup on datalad end.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602167103,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTYwNDQ3MQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705604471"},"message":"re github: happens I had to do the same, so reproduced, will file and issue against datalad","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602167103,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA1Mzc3NTY5"},"target":"a3106d46a232eb0949a250b639d627e1ef0f39a358ae782a6d096d308daa064d","message":"re github: happens I had to do the same, so reproduced, ~~will file and issue against datalad~~ (done: https://github.com/datalad/datalad/issues/4997)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602167769,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTYxNDM0MQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705614341"},"message":"@yarikoptic The datasets are in /mnt/backup/dandi/dandiarchive-replica on drogon.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602168300,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTYxOTkzMg==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705619932"},"message":"@yarikoptic To be clear, regarding mtimes and hashes, your current recommendation is to only use hashes, and if a file doesn't have a hash in Dandiarchive, only copy it if it's not already in the dataset?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602169622,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTYzMzgyOA==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705633828"},"message":"\u003e ...  only copy it if it's not already in the dataset?\n\nyes, and fail if already in dataset and size differs (should not happen; but check on size at least will  provide some safety blanket)... (unless you do want to keep a record of mtimes in the dataset somewhere)","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602169650,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTYzNDEzNg==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705634136"},"message":"\u003e /mnt/backup/dandi/dandiarchive-replica\n\nI do not see the problematic 000026 there","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602169723,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTYzNDkwNg==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705634906"},"message":"@yarikoptic Oh, sorry, I deleted it because I planned on rerunning the script to see if the error would happen again, but then the script started copying everything again because of the mtime issue, so I killed it.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602171503,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTY1MzI5NQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705653295"},"message":"@yarikoptic I reran the script for just 000026, and it failed with the same error.  I'm leaving the directory there this time so you can inspect it.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602190468,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTgxNzc2Nw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705817767"},"message":"I've finished running the script for all Dandisets.  The only problems left should be the default branch issue and whatever's wrong with 000026.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1602200367,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNTg4NDE2Mw==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-705884163"},"message":"thanks! looking at 26 , I do not see `addurl` error you [mentioned](https://github.com/dandi/dandi-cli/issues/250#issuecomment-704919366) and the error I see\n```\n*$\u003e grep Error .dandi/logs/*\n.dandi/logs/sync-20201008191316Z-26083.log:    raise CommandError(\n.dandi/logs/sync-20201008191316Z-26083.log:datalad.support.exceptions.CommandError: CommandError: 'git-annex lookupkey -c annex.dotfiles=true -c annex.retry=3 -- rawdata/sub-I46/ses-MRI/anat/sub-I46_echo-1_fa-1_VFA.json' failed\n with exitcode 1 under /mnt/backup/dandi/dandiarchive-replica/000026\n.dandi/logs/sync-20201008191316Z-26083.log:    raise FileInGitError(cmd=cmd_str,\n.dandi/logs/sync-20201008191316Z-26083.log:datalad.support.exceptions.FileInGitError: FileInGitError: ''git annex lookupkey rawdata/sub-I46/ses-MRI/anat/sub-I46_echo-1_fa-1_VFA.json'' [File not in annex, but git: rawdata/sub-I4\n6/ses-MRI/anat/sub-I46_echo-1_fa-1_VFA.json]\n```\n\nmakes sense since we instantiate datasets with `-c text2git` so that file was added to git, and thus annex has no clue about it. For such files And since this is the first BIDS(-like) dataset, we did not encounter this in other datasets.  So just `AnnexRepo.add` it and then check using `.is_under_annex`, e.g.\n```shell\n*$\u003e python -c 'from datalad.support.annexrepo import AnnexRepo; r=AnnexRepo(\".\");print(r.is_under_annex(\"rawdata/sub-I46/ses-MRI/anat/sub-I46_echo-1_fa-1_VFA.json\"))'\nFalse\n```\nand if True -- only then use `add_url`... or just catch `FileInGitError` around that point and proceed ;)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1602247698,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcwNjE2MDgyOQ==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-706160829"},"message":"@yarikoptic I believe what you're seeing in the logs is due to a combination of the fact that fatal exceptions weren't logged at first combined with the fact that I later reran the script on the partially-populated dataset, at which point it failed with the error you see.  After adjusting the script to only call `add_url_to_file` if `.is_under_annex` and then deleting the dataset on disk, running the script for 000026 afresh fails with the same \"rawdata/sub-I46/ses-MRI/anat/sub-I46_echo-1_fa-1_VFA.json already exists; not overwriting\" addurl error from earlier.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1603473436,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcxNTQ2OTQwMg==","github-url":"https://github.com/dandi/dandi-cli/issues/250#issuecomment-715469402"},"message":"I will consider it done!","files":null},{"type":4,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1603473437,"metadata":{"github-id":"MDExOkNsb3NlZEV2ZW50MzkxNDkzMDYxOQ=="},"status":2}]}