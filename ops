{"version":2,"ops":[{"type":5,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1708722676,"metadata":{"github-id":"LE_lADODBZtRc6AQfCizwAAAALFyVka"},"added":["UX"],"removed":[]},{"type":5,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1708722676,"metadata":{"github-id":"LE_lADODBZtRc6AQfCizwAAAALFyVke"},"added":["performance"],"removed":[]},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1708955352,"metadata":{"github-id":"IC_kwDODBZtRc51Ey30","github-url":"https://github.com/dandi/dandi-cli/issues/1411#issuecomment-1964191220"},"message":"The client already batches Zarr entry deletions, 100 entries per request.  I doubt doing multiple batches in parallel is going to result in faster turnaround from the server.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1708970592,"metadata":{"github-id":"IC_kwDODBZtRc51HHTN","github-url":"https://github.com/dandi/dandi-cli/issues/1411#issuecomment-1964799181"},"message":"why? doesn't it handle requests in parallel?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1708971243,"metadata":{"github-id":"IC_kwDODBZtRc51HMKK","github-url":"https://github.com/dandi/dandi-cli/issues/1411#issuecomment-1964819082"},"message":"@yarikoptic I can't find why the exact value of 100 was chosen, but I believe the point of the limit is to avoid making the server do too much work on a Zarr at once.  Simultaneous requests would therefore mean too much work for the server.\n\n@jjnesbitt @mvandenburgh et alii: Can you confirm or deny that there's no efficiency gain to be had from parallelizing batched Zarr entry deletion requests?","files":null},{"type":3,"author":{"id":"a9e2633aeb6d7e2366a97d09712312c2442c6054"},"timestamp":1708977497,"metadata":{"github-id":"IC_kwDODBZtRc51IcEN","github-url":"https://github.com/dandi/dandi-cli/issues/1411#issuecomment-1965146381"},"message":"\u003e @jjnesbitt @mvandenburgh et alii: Can you confirm or deny that there's no efficiency gain to be had from parallelizing batched Zarr entry deletion requests?\n\nThat's correct, there would be no efficiency gain. This is for two reasons:\n\n1. The zarr is locked for update while processing the request, so if two were made in parallel, they would be processed sequentially.\n2. I believe the underlying reason why we limit the request size is because the API makes requests to the S3 API under the hood, in order to delete the requested zarrs files. Allowing for parallel invocation of this could cause transient errors from S3, which would complicate things very much.\n\nIs the current performance of the zarr deletion endpoint causing problems elsewhere?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1708985450,"metadata":{"github-id":"IC_kwDODBZtRc51JXcN","github-url":"https://github.com/dandi/dandi-cli/issues/1411#issuecomment-1965389581"},"message":"\u003e Is the current performance of the zarr deletion endpoint causing problems elsewhere?\n\nsomewhat. From the log of the issue referenced in the OP: https://github.com/dandi/dandi-cli/issues/1410\n\n```shell\n❯ zgrep -e 'Deleting.*files' -e 'DELETE.*zarr' 20240223192140Z-306046.log.gz | head -n 2\n2024-02-23T14:36:59-0500 [DEBUG   ] dandi 306046:140253474395840 sub-randomzarrlike/sub-randomzarrlike_junk.zarr: Deleting 226053 files in remote Zarr not present locally\n2024-02-23T14:36:59-0500 [DEBUG   ] dandi 306046:140253474395840 DELETE https://api.dandiarchive.org/api/zarr/fd6ab3ea-cff6-4006-a9bf-acfa5d983985/files/\n❯ zgrep 'DELETE.*zarr.*files/$' 20240223192140Z-306046.log.gz | tail -n 1\n2024-02-23T18:14:40-0500 [DEBUG   ] dandi 306046:140253474395840 DELETE https://api.dandiarchive.org/api/zarr/fd6ab3ea-cff6-4006-a9bf-acfa5d983985/files/\n```\n\nso I believe it took over 3 hours to \"merely\" to delete (lots of) files in ZARR having finished upload of other files.  Here such a drastic action was needed since I changed \"chunking\" strategy for a zarr, so would not be completely uncommon.  So I thought it might be nice to get it speedier.","files":null},{"type":5,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1721326115,"metadata":{"github-id":"LE_lADODBZtRc6AQfCizwAAAAMoRdrR"},"added":["zarr"],"removed":[]}]}