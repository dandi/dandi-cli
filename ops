{"version":2,"ops":[{"type":1,"author":{"id":"5b791aa6b01c2b9dfe9f033b09866230d6b477b7"},"timestamp":1753118436,"metadata":{"github-id":"I_kwDODBZtRc7Brp6g","github-url":"https://github.com/dandi/dandi-cli/issues/1662","origin":"github"},"title":"Allow Retrying when Uploading Big Assets","message":"\u003cimg width=\"1920\" height=\"1035\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eada8780-276a-47c0-aa56-76ba59ad274e\" /\u003e\nAt MGB we’re subject to a network filter that terminates any HTTP connection that either exceeds a certain duration or transfers a file above a threshold size. As a result, uploads of large assets to Dandi frequently fail midway through, even though the client itself is fully capable of completing the transfer if given another chance.\n\n# Current Behavior\n- Attempting to upload large datasets or assets via the Dandi client routinely triggers our network filter.\n\n- The HTTP connection is cut, and the client reports a failure without retrying.\n\n- Our network team has confirmed that they cannot adjust the filter parameters on their end.\n\n# Expected Behavior\nWhen an upload is interrupted by a network-level timeout or size-based cutoff, the Dandi client should automatically retry (or continue) that upload until it succeeds, rather than erroring out after a single attempt.\n\n# Proposed Solution\n- New CLI flag, e.g. --retry-on-fail or --max-retries=N, which automatically resumes or re‑issues the upload request up to N times (with sensible backoff).\n\n- Default behavior remains unchanged (no retries) to preserve backward compatibility.","files":null}]}