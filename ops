{"version":2,"ops":[{"type":5,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1723728471,"metadata":{"github-id":"LE_lADODBZtRc6TG4xqzwAAAAM8QZ0V"},"added":["tests"],"removed":[]},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1723728605,"metadata":{"github-id":"IC_kwDODBZtRc6IkglH","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2291272007"},"message":"@yarikoptic Those tests involve publishing a Dandiset, and it seems that the publication did not complete in time.  What happens if you rerun the tests?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1723739512,"metadata":{"github-id":"IC_kwDODBZtRc6ImDHz","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2291675635"},"message":"they were and still are (on master) failing consistently on my attempts. Full log http://www.oneukrainian.com/tmp/pytest-fail-20240815.txt \n\nI also note that there is 100% CPU celery (not sure what could keep it that busy) - do  you (or may be @jjnesbitt or @mvandenburgh ) have an idea what it could be doing or how to figure it out?\n\n\u003cdetails\u003e\n\u003csummary\u003ecelery container logs show that it completes its jobs in milliseconds\u003c/summary\u003e \n\n```shell\n[2024-08-15 15:51:30,720: INFO/MainProcess] Task dandiapi.api.tasks.calculate_sha256[47f0c15a-7d49-4208-aeda-9c8e01c18980] received: ((UUID('a33573ab-acc0-4d14-b823-91afa51b82b2'),), {})\n[2024-08-15 15:51:30,736: INFO/ForkPoolWorker-2] dandiapi.api.tasks.calculate_sha256[47f0c15a-7d49-4208-aeda-9c8e01c18980]: Calculating sha256 checksum for asset blob a33573ab-acc0-4d14-b823-91afa51b82b2\n[2024-08-15 15:51:30,742: INFO/ForkPoolWorker-2] Task dandiapi.api.tasks.calculate_sha256[47f0c15a-7d49-4208-aeda-9c8e01c18980] succeeded in 0.02144717297051102s: None\n[2024-08-15 15:51:30,909: INFO/MainProcess] Task dandiapi.api.tasks.calculate_sha256[b4f586df-dadf-4841-ae26-a7e733c4dd60] received: ((UUID('5bdd712a-04cc-40ac-b73f-98f614743b27'),), {})\n[2024-08-15 15:51:30,910: INFO/ForkPoolWorker-2] dandiapi.api.tasks.calculate_sha256[b4f586df-dadf-4841-ae26-a7e733c4dd60]: Calculating sha256 checksum for asset blob 5bdd712a-04cc-40ac-b73f-98f614743b27\n[2024-08-15 15:51:30,914: INFO/ForkPoolWorker-2] Task dandiapi.api.tasks.calculate_sha256[b4f586df-dadf-4841-ae26-a7e733c4dd60] succeeded in 0.005075494991615415s: None\n[2024-08-15 15:53:31,583: INFO/MainProcess] Task dandiapi.api.tasks.calculate_sha256[11a11275-94c1-40ff-adec-e87f012e4baa] received: ((UUID('091711e2-af2e-44f5-8fec-85eb49ce1ff9'),), {})\n[2024-08-15 15:53:31,585: INFO/ForkPoolWorker-2] dandiapi.api.tasks.calculate_sha256[11a11275-94c1-40ff-adec-e87f012e4baa]: Calculating sha256 checksum for asset blob 091711e2-af2e-44f5-8fec-85eb49ce1ff9\n[2024-08-15 15:53:31,588: INFO/ForkPoolWorker-2] Task dandiapi.api.tasks.calculate_sha256[11a11275-94c1-40ff-adec-e87f012e4baa] succeeded in 0.004132542992010713s: None\n[2024-08-15 15:53:31,758: INFO/MainProcess] Task dandiapi.api.tasks.calculate_sha256[9170e368-2588-4999-a1c4-4f2c6ce910ef] received: ((UUID('3b7be16d-38e1-4f8f-8046-36936890eac8'),), {})\n[2024-08-15 15:53:31,759: INFO/ForkPoolWorker-2] dandiapi.api.tasks.calculate_sha256[9170e368-2588-4999-a1c4-4f2c6ce910ef]: Calculating sha256 checksum for asset blob 3b7be16d-38e1-4f8f-8046-36936890eac8\n[2024-08-15 15:53:31,763: INFO/ForkPoolWorker-2] Task dandiapi.api.tasks.calculate_sha256[9170e368-2588-4999-a1c4-4f2c6ce910ef] succeeded in 0.004479025024920702s: None\n[2024-08-15 15:53:31,938: INFO/MainProcess] Task dandiapi.api.tasks.calculate_sha256[2da50b13-a501-4881-ad8d-0ccd1c6e1f8c] received: ((UUID('1e95e5b4-8fe2-4f6a-8d99-9fbf86646a72'),), {})\n[2024-08-15 15:53:31,939: INFO/ForkPoolWorker-2] dandiapi.api.tasks.calculate_sha256[2da50b13-a501-4881-ad8d-0ccd1c6e1f8c]: Calculating sha256 checksum for asset blob 1e95e5b4-8fe2-4f6a-8d99-9fbf86646a72\n[2024-08-15 15:53:31,943: INFO/ForkPoolWorker-2] Task dandiapi.api.tasks.calculate_sha256[2da50b13-a501-4881-ad8d-0ccd1c6e1f8c] succeeded in 0.004295543010812253s: None\n\n```\n\u003c/details\u003e","files":null},{"type":3,"author":{"id":"2db4f5da888e2bc30bfef7de9879ca52bb0fac9e"},"timestamp":1731629855,"metadata":{"github-id":"IC_kwDODBZtRc6Trllx","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2477676913"},"message":"I haven't gotten this to pass locally for me either. I can give full logs also if helpful. (FWIW there are other tests breaking as well, this is just the first one)\n\nI tried more than doubling the wait time, still not passing. I guess something in the environment is broken?\n\n```diff\ndiff --git a/dandi/dandiapi.py b/dandi/dandiapi.py\nindex 5ea504de..8b957c4f 100644\n--- a/dandi/dandiapi.py\n+++ b/dandi/dandiapi.py\n@@ -1086,7 +1086,7 @@ class RemoteDandiset:\n             json={\"metadata\": metadata, \"name\": metadata.get(\"name\", \"\")},\n         )\n \n-    def wait_until_valid(self, max_time: float = 120) -\u003e None:\n+    def wait_until_valid(self, max_time: float = 250) -\u003e None:\n         \"\"\"\n         Wait at most ``max_time`` seconds for the Dandiset to be valid for\n         publication.  If the Dandiset does not become valid in time, a\n```\n\n```\nduct tox -e py3 -- -s -v dandi/tests/test_dandiapi.py::test_publish_and_manipulate\n\n...\u003csnip\u003e...\nDEBUG    dandi:dandiapi.py:262 Response: 200\nDEBUG    dandi:dandiapi.py:198 GET http://localhost:8000/api/dandisets/000001/versions/draft/info/\nDEBUG    dandi:dandiapi.py:262 Response: 200\nDEBUG    dandi:dandiapi.py:198 GET http://localhost:8000/api/dandisets/000001/versions/draft/info/\nDEBUG    dandi:dandiapi.py:262 Response: 200\n============================= slowest 10 durations =============================\n251.62s call     dandi/tests/test_dandiapi.py::test_publish_and_manipulate\n45.01s setup    dandi/tests/test_dandiapi.py::test_publish_and_manipulate\n8.46s teardown dandi/tests/test_dandiapi.py::test_publish_and_manipulate\n=========================== short test summary info ============================\nFAILED dandi/tests/test_dandiapi.py::test_publish_and_manipulate - ValueError: Dandiset 000001 is Pending: {\n    \"asset_validation_errors\": [\n        {\n            \"field\": \"\",\n            \"message\": \"asset is currently being validated, please wait.\",\n            \"path\": \"subdir/doomed.txt\"\n        },\n        {\n            \"field\": \"\",\n            \"message\": \"asset is currently being validated, please wait.\",\n            \"path\": \"subdir/file.txt\"\n        }\n    ],\n    \"version_validation_errors\": []\n}\n======================== 1 failed in 305.86s (0:05:05) =========================\npy3: exit 1 (313.54 seconds) /home/austin/devel/dandi-cli\u003e coverage run -m pytest -v -s -v dandi/tests/test_dandiapi.py::test_publish_and_manipulate dandi pid=424363\n  py3: FAIL code 1 (320.60=setup[6.91]+cmd[0.15,313.54] seconds)\n  evaluation failed :( (320.68 seconds)\n2024-11-14T18:08:33-0600 [INFO    ] con-duct: Summary:\nExit Code: 1\nCommand: tox -e py3 -- -s -v dandi/tests/test_dandiapi.py::test_publish_and_manipulate\nLog files location: .duct/logs/2024.11.14T18.03.12-424275_\nWall Clock Time: 320.948 sec\nMemory Peak Usage (RSS): 311.4 MB\nMemory Average Usage (RSS): 234.0 MB\nVirtual Memory Peak Usage (VSZ): 7.7 GB\nVirtual Memory Average Usage (VSZ): 3.5 GB\nMemory Peak Percentage: 0.7%\nMemory Average Percentage: 0.599035369774919%\nCPU Peak Usage: 383.5%\nAverage CPU Usage: 26.813504823151113%\n```","files":null},{"type":6,"author":{"id":"2db4f5da888e2bc30bfef7de9879ca52bb0fac9e"},"timestamp":1731632339,"metadata":{"github-id":"UCE_lALODBZtRc6TrllxzlUlXWg"},"target":"db2f21189eccec207aaede5c346004a74a42c111f9c20878d42d4ac89a7c2625","message":"I haven't gotten this to pass locally for me either. I can give full logs also if helpful. (FWIW there are other tests breaking as well, this is just the first one)\n\nI tried more than doubling the wait time, still not passing. I guess something in the environment is broken?\n\n```diff\ndiff --git a/dandi/dandiapi.py b/dandi/dandiapi.py\nindex 5ea504de..8b957c4f 100644\n--- a/dandi/dandiapi.py\n+++ b/dandi/dandiapi.py\n@@ -1086,7 +1086,7 @@ class RemoteDandiset:\n             json={\"metadata\": metadata, \"name\": metadata.get(\"name\", \"\")},\n         )\n \n-    def wait_until_valid(self, max_time: float = 120) -\u003e None:\n+    def wait_until_valid(self, max_time: float = 250) -\u003e None:\n         \"\"\"\n         Wait at most ``max_time`` seconds for the Dandiset to be valid for\n         publication.  If the Dandiset does not become valid in time, a\n```\n\n```\nduct tox -e py3 -- -s -v dandi/tests/test_dandiapi.py::test_publish_and_manipulate\n\n...\u003csnip\u003e...\nDEBUG    dandi:dandiapi.py:262 Response: 200\nDEBUG    dandi:dandiapi.py:198 GET http://localhost:8000/api/dandisets/000001/versions/draft/info/\nDEBUG    dandi:dandiapi.py:262 Response: 200\nDEBUG    dandi:dandiapi.py:198 GET http://localhost:8000/api/dandisets/000001/versions/draft/info/\nDEBUG    dandi:dandiapi.py:262 Response: 200\n============================= slowest 10 durations =============================\n251.62s call     dandi/tests/test_dandiapi.py::test_publish_and_manipulate\n45.01s setup    dandi/tests/test_dandiapi.py::test_publish_and_manipulate\n8.46s teardown dandi/tests/test_dandiapi.py::test_publish_and_manipulate\n=========================== short test summary info ============================\nFAILED dandi/tests/test_dandiapi.py::test_publish_and_manipulate - ValueError: Dandiset 000001 is Pending: {\n    \"asset_validation_errors\": [\n        {\n            \"field\": \"\",\n            \"message\": \"asset is currently being validated, please wait.\",\n            \"path\": \"subdir/doomed.txt\"\n        },\n        {\n            \"field\": \"\",\n            \"message\": \"asset is currently being validated, please wait.\",\n            \"path\": \"subdir/file.txt\"\n        }\n    ],\n    \"version_validation_errors\": []\n}\n======================== 1 failed in 305.86s (0:05:05) =========================\npy3: exit 1 (313.54 seconds) /home/austin/devel/dandi-cli\u003e coverage run -m pytest -v -s -v dandi/tests/test_dandiapi.py::test_publish_and_manipulate dandi pid=424363\n  py3: FAIL code 1 (320.60=setup[6.91]+cmd[0.15,313.54] seconds)\n  evaluation failed :( (320.68 seconds)\n2024-11-14T18:08:33-0600 [INFO    ] con-duct: Summary:\nExit Code: 1\nCommand: tox -e py3 -- -s -v dandi/tests/test_dandiapi.py::test_publish_and_manipulate\nLog files location: .duct/logs/2024.11.14T18.03.12-424275_\nWall Clock Time: 320.948 sec\nMemory Peak Usage (RSS): 311.4 MB\nMemory Average Usage (RSS): 234.0 MB\nVirtual Memory Peak Usage (VSZ): 7.7 GB\nVirtual Memory Average Usage (VSZ): 3.5 GB\nMemory Peak Percentage: 0.7%\nMemory Average Percentage: 0.599035369774919%\nCPU Peak Usage: 383.5%\nAverage CPU Usage: 26.813504823151113%\n```\n\nThese others fail for me too, incase thats related \n\n```\n========================================================= short test summary info =========================================================\nFAILED dandi/tests/test_dandiapi.py::test_publish_and_manipulate - ValueError: Dandiset 000008 is Pending: {\nFAILED dandi/tests/test_dandiapi.py::test_get_dandiset_published[True] - ValueError: Dandiset 000021 is Pending: {\nFAILED dandi/tests/test_dandiapi.py::test_get_dandiset_published[False] - ValueError: Dandiset 000022 is Pending: {\nFAILED dandi/tests/test_dandiapi.py::test_get_dandiset_published_no_version_id[True] - ValueError: Dandiset 000023 is Pending: {\nFAILED dandi/tests/test_dandiapi.py::test_get_dandiset_published_no_version_id[False] - ValueError: Dandiset 000024 is Pending: {\n=============================== 5 failed, 707 passed, 1 skipped, 2 xfailed, 8 xpassed in 2413.00s (0:40:12) ===============================\npy3: exit 1 (2422.98 seconds) /home/austin/devel/dandi-cli\u003e coverage run -m pytest -v dandi pid=430147\n  py3: FAIL code 1 (2431.11=setup[7.95]+cmd[0.18,2422.98] seconds)\n  evaluation failed :( (2431.21 seconds)\n```","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732065748,"metadata":{"github-id":"IC_kwDODBZtRc6UPlzP","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2487114959"},"message":"digging through the history of issues, it seems we had similar issue before\n- https://github.com/dandi/dandi-archive/issues/567\nwhich was addressed in\n- https://github.com/dandi/dandi-cli/pull/957 describing changes as\n   - Limit celery concurrency to 1. Possibly having multiple workers running at the same time is causing a race condition.\n   - Upgrade the minio image version. The error might be happening when querying the minio container from the celery container, so possibly there is a minio bug that has been patched in the last 4 months.  \n\n\n\u003cdetails\u003e\n\u003csummary\u003eSince then a good number of changes to that compose\u003c/summary\u003e \n\n```diff\ndiff --git a/dandi/tests/data/dandiarchive-docker/docker-compose.yml b/dandi/tests/data/dandiarchive-docker/docker-compose.yml\nindex 9ce5e464..3f132d3e 100644\n--- a/dandi/tests/data/dandiarchive-docker/docker-compose.yml\n+++ b/dandi/tests/data/dandiarchive-docker/docker-compose.yml\n@@ -4,20 +4,7 @@\n # \u003chttps://github.com/dandi/dandi-api/blob/master/docker-compose.override.yml\u003e,\n # but using images uploaded to Docker Hub instead of building them locally.\n \n-version: '2.1'\n-\n services:\n-  redirector:\n-    image: dandiarchive/dandiarchive-redirector\n-    depends_on:\n-      - django\n-    ports:\n-      - \"8079:8080\"\n-    environment:\n-      #GUI_URL: http://localhost:8086\n-      ABOUT_URL: http://www.dandiarchive.org\n-      API_URL: http://localhost:8000/api\n-\n   django:\n     image: dandiarchive/dandiarchive-api\n     command: [\"./manage.py\", \"runserver\", \"--nothreading\", \"0.0.0.0:8000\"]\n@@ -30,23 +17,31 @@ services:\n         condition: service_healthy\n       rabbitmq:\n         condition: service_started\n-    environment:\n+    environment: \u0026django_env\n       DJANGO_CELERY_BROKER_URL: amqp://rabbitmq:5672/\n       DJANGO_CONFIGURATION: DevelopmentConfiguration\n       DJANGO_DANDI_DANDISETS_BUCKET_NAME: dandi-dandisets\n+      DJANGO_DANDI_DANDISETS_LOG_BUCKET_NAME: dandiapi-dandisets-logs\n       DJANGO_DANDI_DANDISETS_EMBARGO_BUCKET_NAME: dandi-embargoed-dandisets\n+      DJANGO_DANDI_DANDISETS_EMBARGO_LOG_BUCKET_NAME: dandiapi-embargo-dandisets-logs\n       DJANGO_DATABASE_URL: postgres://postgres:postgres@postgres:5432/django\n       DJANGO_MINIO_STORAGE_ACCESS_KEY: minioAccessKey\n       DJANGO_MINIO_STORAGE_ENDPOINT: minio:9000\n       DJANGO_MINIO_STORAGE_SECRET_KEY: minioSecretKey\n       DJANGO_STORAGE_BUCKET_NAME: django-storage\n-      DJANGO_MINIO_STORAGE_MEDIA_URL: http://localhost:9000/django-storage\n-      DJANGO_DANDI_SCHEMA_VERSION:\n+      # The Minio URL needs to use 127.0.0.1 instead of localhost so that blob\n+      # assets' \"S3 URLs\" will use 127.0.0.1, and thus tests that try to open\n+      # these URLs via fsspec will not fail on systems where localhost is both\n+      # 127.0.0.1 and ::1.\n+      DJANGO_MINIO_STORAGE_MEDIA_URL: http://127.0.0.1:9000/django-storage\n+      DJANGO_DANDI_SCHEMA_VERSION: ~\n       DJANGO_DANDI_WEB_APP_URL: http://localhost:8085\n       DJANGO_DANDI_API_URL: http://localhost:8000\n+      DJANGO_DANDI_JUPYTERHUB_URL: https://hub.dandiarchive.org\n+      DJANGO_DANDI_DEV_EMAIL: \"test@example.com\"\n       DANDI_ALLOW_LOCALHOST_URLS: \"1\"\n     ports:\n-      - \"8000:8000\"\n+      - \"127.0.0.1:8000:8000\"\n \n   celery:\n     image: dandiarchive/dandiarchive-api\n@@ -70,21 +65,8 @@ services:\n       rabbitmq:\n         condition: service_started\n     environment:\n-      DJANGO_CELERY_BROKER_URL: amqp://rabbitmq:5672/\n-      DJANGO_CONFIGURATION: DevelopmentConfiguration\n-      DJANGO_DANDI_DANDISETS_BUCKET_NAME: dandi-dandisets\n-      DJANGO_DANDI_DANDISETS_EMBARGO_BUCKET_NAME: dandi-embargoed-dandisets\n-      DJANGO_DATABASE_URL: postgres://postgres:postgres@postgres:5432/django\n-      DJANGO_MINIO_STORAGE_ACCESS_KEY: minioAccessKey\n-      DJANGO_MINIO_STORAGE_ENDPOINT: minio:9000\n-      DJANGO_MINIO_STORAGE_SECRET_KEY: minioSecretKey\n-      DJANGO_STORAGE_BUCKET_NAME: django-storage\n-      DJANGO_MINIO_STORAGE_MEDIA_URL: http://localhost:9000/django-storage\n-      DJANGO_DANDI_SCHEMA_VERSION:\n+      \u003c\u003c : *django_env\n       DJANGO_DANDI_VALIDATION_JOB_INTERVAL: \"5\"\n-      DJANGO_DANDI_WEB_APP_URL: http://localhost:8085\n-      DJANGO_DANDI_API_URL: http://localhost:8000\n-      DANDI_ALLOW_LOCALHOST_URLS: \"1\"\n \n   minio:\n     image: minio/minio:RELEASE.2022-04-12T06-55-35Z\n@@ -92,7 +74,7 @@ services:\n     tty: true\n     command: [\"server\", \"/data\"]\n     ports:\n-      - \"9000:9000\"\n+      - \"127.0.0.1:9000:9000\"\n     environment:\n       MINIO_ACCESS_KEY: minioAccessKey\n       MINIO_SECRET_KEY: minioSecretKey\n@@ -107,8 +89,8 @@ services:\n       POSTGRES_DB: django\n       POSTGRES_PASSWORD: postgres\n     image: postgres\n-    ports:\n-      - \"5432:5432\"\n+    expose:\n+      - \"5432\"\n     healthcheck:\n       test: [\"CMD\", \"pg_isready\", \"-U\", \"postgres\"]\n       interval: 7s\n@@ -117,5 +99,5 @@ services:\n \n   rabbitmq:\n     image: rabbitmq:management\n-    ports:\n-      - \"5672:5672\"\n+    expose:\n+      - \"5672\"\n\n```\n\u003c/details\u003e\n\nbut neither minio version nor `-c`  concurrency setting...","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732072701,"metadata":{"github-id":"UCE_lALODBZtRc6UPlzPzlWNxjY"},"target":"f48144e356d9d37d15791e862ca7dc6e4d29c4c08e158b82fd6f247fdd79bc94","message":"digging through the history of issues, it seems we had similar issue before\n- https://github.com/dandi/dandi-archive/issues/567\nwhich was addressed in\n- https://github.com/dandi/dandi-cli/pull/957 describing changes as\n   - Limit celery concurrency to 1. Possibly having multiple workers running at the same time is causing a race condition.\n   - Upgrade the minio image version. The error might be happening when querying the minio container from the celery container, so possibly there is a minio bug that has been patched in the last 4 months.  \n\n\n\u003cdetails\u003e\n\u003csummary\u003eSince then a good number of changes to that compose\u003c/summary\u003e \n\n```diff\ndiff --git a/dandi/tests/data/dandiarchive-docker/docker-compose.yml b/dandi/tests/data/dandiarchive-docker/docker-compose.yml\nindex 9ce5e464..3f132d3e 100644\n--- a/dandi/tests/data/dandiarchive-docker/docker-compose.yml\n+++ b/dandi/tests/data/dandiarchive-docker/docker-compose.yml\n@@ -4,20 +4,7 @@\n # \u003chttps://github.com/dandi/dandi-api/blob/master/docker-compose.override.yml\u003e,\n # but using images uploaded to Docker Hub instead of building them locally.\n \n-version: '2.1'\n-\n services:\n-  redirector:\n-    image: dandiarchive/dandiarchive-redirector\n-    depends_on:\n-      - django\n-    ports:\n-      - \"8079:8080\"\n-    environment:\n-      #GUI_URL: http://localhost:8086\n-      ABOUT_URL: http://www.dandiarchive.org\n-      API_URL: http://localhost:8000/api\n-\n   django:\n     image: dandiarchive/dandiarchive-api\n     command: [\"./manage.py\", \"runserver\", \"--nothreading\", \"0.0.0.0:8000\"]\n@@ -30,23 +17,31 @@ services:\n         condition: service_healthy\n       rabbitmq:\n         condition: service_started\n-    environment:\n+    environment: \u0026django_env\n       DJANGO_CELERY_BROKER_URL: amqp://rabbitmq:5672/\n       DJANGO_CONFIGURATION: DevelopmentConfiguration\n       DJANGO_DANDI_DANDISETS_BUCKET_NAME: dandi-dandisets\n+      DJANGO_DANDI_DANDISETS_LOG_BUCKET_NAME: dandiapi-dandisets-logs\n       DJANGO_DANDI_DANDISETS_EMBARGO_BUCKET_NAME: dandi-embargoed-dandisets\n+      DJANGO_DANDI_DANDISETS_EMBARGO_LOG_BUCKET_NAME: dandiapi-embargo-dandisets-logs\n       DJANGO_DATABASE_URL: postgres://postgres:postgres@postgres:5432/django\n       DJANGO_MINIO_STORAGE_ACCESS_KEY: minioAccessKey\n       DJANGO_MINIO_STORAGE_ENDPOINT: minio:9000\n       DJANGO_MINIO_STORAGE_SECRET_KEY: minioSecretKey\n       DJANGO_STORAGE_BUCKET_NAME: django-storage\n-      DJANGO_MINIO_STORAGE_MEDIA_URL: http://localhost:9000/django-storage\n-      DJANGO_DANDI_SCHEMA_VERSION:\n+      # The Minio URL needs to use 127.0.0.1 instead of localhost so that blob\n+      # assets' \"S3 URLs\" will use 127.0.0.1, and thus tests that try to open\n+      # these URLs via fsspec will not fail on systems where localhost is both\n+      # 127.0.0.1 and ::1.\n+      DJANGO_MINIO_STORAGE_MEDIA_URL: http://127.0.0.1:9000/django-storage\n+      DJANGO_DANDI_SCHEMA_VERSION: ~\n       DJANGO_DANDI_WEB_APP_URL: http://localhost:8085\n       DJANGO_DANDI_API_URL: http://localhost:8000\n+      DJANGO_DANDI_JUPYTERHUB_URL: https://hub.dandiarchive.org\n+      DJANGO_DANDI_DEV_EMAIL: \"test@example.com\"\n       DANDI_ALLOW_LOCALHOST_URLS: \"1\"\n     ports:\n-      - \"8000:8000\"\n+      - \"127.0.0.1:8000:8000\"\n \n   celery:\n     image: dandiarchive/dandiarchive-api\n@@ -70,21 +65,8 @@ services:\n       rabbitmq:\n         condition: service_started\n     environment:\n-      DJANGO_CELERY_BROKER_URL: amqp://rabbitmq:5672/\n-      DJANGO_CONFIGURATION: DevelopmentConfiguration\n-      DJANGO_DANDI_DANDISETS_BUCKET_NAME: dandi-dandisets\n-      DJANGO_DANDI_DANDISETS_EMBARGO_BUCKET_NAME: dandi-embargoed-dandisets\n-      DJANGO_DATABASE_URL: postgres://postgres:postgres@postgres:5432/django\n-      DJANGO_MINIO_STORAGE_ACCESS_KEY: minioAccessKey\n-      DJANGO_MINIO_STORAGE_ENDPOINT: minio:9000\n-      DJANGO_MINIO_STORAGE_SECRET_KEY: minioSecretKey\n-      DJANGO_STORAGE_BUCKET_NAME: django-storage\n-      DJANGO_MINIO_STORAGE_MEDIA_URL: http://localhost:9000/django-storage\n-      DJANGO_DANDI_SCHEMA_VERSION:\n+      \u003c\u003c : *django_env\n       DJANGO_DANDI_VALIDATION_JOB_INTERVAL: \"5\"\n-      DJANGO_DANDI_WEB_APP_URL: http://localhost:8085\n-      DJANGO_DANDI_API_URL: http://localhost:8000\n-      DANDI_ALLOW_LOCALHOST_URLS: \"1\"\n \n   minio:\n     image: minio/minio:RELEASE.2022-04-12T06-55-35Z\n@@ -92,7 +74,7 @@ services:\n     tty: true\n     command: [\"server\", \"/data\"]\n     ports:\n-      - \"9000:9000\"\n+      - \"127.0.0.1:9000:9000\"\n     environment:\n       MINIO_ACCESS_KEY: minioAccessKey\n       MINIO_SECRET_KEY: minioSecretKey\n@@ -107,8 +89,8 @@ services:\n       POSTGRES_DB: django\n       POSTGRES_PASSWORD: postgres\n     image: postgres\n-    ports:\n-      - \"5432:5432\"\n+    expose:\n+      - \"5432\"\n     healthcheck:\n       test: [\"CMD\", \"pg_isready\", \"-U\", \"postgres\"]\n       interval: 7s\n@@ -117,5 +99,5 @@ services:\n \n   rabbitmq:\n     image: rabbitmq:management\n-    ports:\n-      - \"5672:5672\"\n+    expose:\n+      - \"5672\"\n\n```\n\u003c/details\u003e\n\nbut neither minio version nor `-c`  concurrency setting...\n\n\u003cdetails\u003e\n\u003csummary\u003eedit 1:  celery is \"busy\" trying to close some incrementing handle and that is what it is wasting cpu on\u003c/summary\u003e \n\n```shell\nroot@d49aa92590f7:/opt/django# strace -f -p 8 2\u003e\u00261 | head\nstrace: Process 8 attached\nclose(928972594)                        = -1 EBADF (Bad file descriptor)\nclose(928972593)                        = -1 EBADF (Bad file descriptor)\nclose(928972592)                        = -1 EBADF (Bad file descriptor)\nclose(928972591)                        = -1 EBADF (Bad file descriptor)\nclose(928972590)                        = -1 EBADF (Bad file descriptor)\nclose(928972589)                        = -1 EBADF (Bad file descriptor)\nclose(928972588)                        = -1 EBADF (Bad file descriptor)\nclose(928972587)                        = -1 EBADF (Bad file descriptor)\nclose(928972586)                        = -1 EBADF (Bad file descriptor)\nroot@d49aa92590f7:/opt/django# ls -l /proc/8/fd/\ntotal 0\nlrwx------ 1 root root 64 Nov 20 03:06 0 -\u003e /dev/null\nl-wx------ 1 root root 64 Nov 20 03:06 1 -\u003e 'pipe:[5581143]'\nl-wx------ 1 root root 64 Nov 20 03:06 2 -\u003e 'pipe:[5581144]'\nlrwx------ 1 root root 64 Nov 20 03:06 3 -\u003e 'socket:[5575530]'\nlrwx------ 1 root root 64 Nov 20 03:06 4 -\u003e 'socket:[5574189]'\nlrwx------ 1 root root 64 Nov 20 03:06 5 -\u003e 'socket:[5576134]'\nlrwx------ 1 root root 64 Nov 20 03:06 6 -\u003e 'anon_inode:[eventpoll]'\nlr-x------ 1 root root 64 Nov 20 03:06 7 -\u003e /dev/null\nl-wx------ 1 root root 64 Nov 20 03:06 8 -\u003e 'pipe:[5578287]'\n\n```\n\nwhenever in logs it seems to be ok\n\n```\ncelery-1  | [2024-11-20 03:04:27,590: INFO/MainProcess] Task dandiapi.api.tasks.calculate_sha256[281efa6f-f660-4603-a661-f11e7ebcc220] received: ((UUID('13745690-aebd-4812-a1bd-aecb374bb83b'),), {})\ncelery-1  | [2024-11-20 03:04:27,590: DEBUG/MainProcess] TaskPool: Apply \u003cfunction fast_trace_task at 0x7fbb89a50360\u003e (args:('dandiapi.api.tasks.calculate_sha256', '281efa6f-f660-4603-a661-f11e7ebcc220', {'argsrepr': \"(UUID('13745690-aebd-4812-a1bd-aecb374bb83b'),)\", 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'id': '281efa6f-f660-4603-a661-f11e7ebcc220', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen7@e29931eba4e7', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '281efa6f-f660-4603-a661-f11e7ebcc220', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'dandiapi.api.tasks.calculate_sha256', 'timelimit': [None, 86400], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"(UUID('13745690-aebd-4812-a1bd-aecb374bb83b'),)\", 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'id': '281efa6f-f660-4603-a661-f11e7ebcc220', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen7@e29931eba4e7', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\ncelery-1  | [2024-11-20 03:04:27,606: DEBUG/ForkPoolWorker-2] (0.001) SELECT t.oid, typarray FROM pg_type t JOIN pg_namespace ns ON typnamespace = ns.oid WHERE typname = 'hstore'; args=None; alias=default\ncelery-1  | [2024-11-20 03:04:27,606: DEBUG/ForkPoolWorker-2] (0.000) SELECT typarray FROM pg_type WHERE typname = 'citext'; args=None; alias=default\ncelery-1  | [2024-11-20 03:04:27,607: DEBUG/ForkPoolWorker-2] (0.000) SELECT \"api_assetblob\".\"id\", \"api_assetblob\".\"created\", \"api_assetblob\".\"modified\", \"api_assetblob\".\"embargoed\", \"api_assetblob\".\"blob\", \"api_assetblob\".\"blob_id\", \"api_assetblob\".\"sha256\", \"api_assetblob\".\"etag\", \"api_assetblob\".\"size\", \"api_assetblob\".\"download_count\" FROM \"api_assetblob\" WHERE \"api_assetblob\".\"blob_id\" = '13745690-aebd-4812-a1bd-aecb374bb83b'::uuid LIMIT 21; args=(UUID('13745690-aebd-4812-a1bd-aecb374bb83b'),); alias=default\ncelery-1  | [2024-11-20 03:04:27,607: INFO/ForkPoolWorker-2] dandiapi.api.tasks.calculate_sha256[281efa6f-f660-4603-a661-f11e7ebcc220]: Calculating sha256 checksum for asset blob 13745690-aebd-4812-a1bd-aecb374bb83b\ncelery-1  | [2024-11-20 03:04:27,610: DEBUG/ForkPoolWorker-2] http://minio:9000 \"GET /dandi-dandisets/blobs/137/456/13745690-aebd-4812-a1bd-aecb374bb83b HTTP/11\" 200 22\ncelery-1  | [2024-11-20 03:04:27,611: DEBUG/ForkPoolWorker-2] (0.001) UPDATE \"api_assetblob\" SET \"sha256\" = '458eeae191657906bb1fefdca3619f35820263fd9d429e35688de62cf7a8c10b' WHERE \"api_assetblob\".\"blob_id\" = '13745690-aebd-4812-a1bd-aecb374bb83b'::uuid; args=('458eeae191657906bb1fefdca3619f35820263fd9d429e35688de62cf7a8c10b', UUID('13745690-aebd-4812-a1bd-aecb374bb83b')); alias=default\ncelery-1  | [2024-11-20 03:04:27,612: INFO/ForkPoolWorker-2] Task dandiapi.api.tasks.calculate_sha256[281efa6f-f660-4603-a661-f11e7ebcc220] succeeded in 0.02052633202401921s: None\ncelery-1  | [2024-11-20 03:04:27,817: INFO/MainProcess] Task dandiapi.api.tasks.calculate_sha256[77c1f8a3-93e7-4b46-9e3e-73e3db11409e] received: ((UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed'),), {})\ncelery-1  | [2024-11-20 03:04:27,817: DEBUG/MainProcess] TaskPool: Apply \u003cfunction fast_trace_task at 0x7fbb89a50360\u003e (args:('dandiapi.api.tasks.calculate_sha256', '77c1f8a3-93e7-4b46-9e3e-73e3db11409e', {'argsrepr': \"(UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed'),)\", 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'id': '77c1f8a3-93e7-4b46-9e3e-73e3db11409e', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen7@e29931eba4e7', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '77c1f8a3-93e7-4b46-9e3e-73e3db11409e', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'dandiapi.api.tasks.calculate_sha256', 'timelimit': [None, 86400], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"(UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed'),)\", 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'id': '77c1f8a3-93e7-4b46-9e3e-73e3db11409e', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen7@e29931eba4e7', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\ncelery-1  | [2024-11-20 03:04:27,818: DEBUG/ForkPoolWorker-2] (0.000) SELECT \"api_assetblob\".\"id\", \"api_assetblob\".\"created\", \"api_assetblob\".\"modified\", \"api_assetblob\".\"embargoed\", \"api_assetblob\".\"blob\", \"api_assetblob\".\"blob_id\", \"api_assetblob\".\"sha256\", \"api_assetblob\".\"etag\", \"api_assetblob\".\"size\", \"api_assetblob\".\"download_count\" FROM \"api_assetblob\" WHERE \"api_assetblob\".\"blob_id\" = '7a7988d0-01ed-4bfb-8fa3-28741ced67ed'::uuid LIMIT 21; args=(UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed'),); alias=default\ncelery-1  | [2024-11-20 03:04:27,818: INFO/ForkPoolWorker-2] dandiapi.api.tasks.calculate_sha256[77c1f8a3-93e7-4b46-9e3e-73e3db11409e]: Calculating sha256 checksum for asset blob 7a7988d0-01ed-4bfb-8fa3-28741ced67ed\ncelery-1  | [2024-11-20 03:04:27,820: DEBUG/ForkPoolWorker-2] http://minio:9000 \"GET /dandi-dandisets/blobs/7a7/988/7a7988d0-01ed-4bfb-8fa3-28741ced67ed HTTP/11\" 200 19\ncelery-1  | [2024-11-20 03:04:27,821: DEBUG/ForkPoolWorker-2] (0.001) UPDATE \"api_assetblob\" SET \"sha256\" = '6fef386efa7208eaf1c596b6ab2f8a5a3583696ef8649be0552ab3effad1e191' WHERE \"api_assetblob\".\"blob_id\" = '7a7988d0-01ed-4bfb-8fa3-28741ced67ed'::uuid; args=('6fef386efa7208eaf1c596b6ab2f8a5a3583696ef8649be0552ab3effad1e191', UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed')); alias=default\ncelery-1  | [2024-11-20 03:04:27,821: INFO/ForkPoolWorker-2] Task dandiapi.api.tasks.calculate_sha256[77c1f8a3-93e7-4b46-9e3e-73e3db11409e] succeeded in 0.00400476303184405s: None\n\n\n\n\ncelery-1  | [2024-11-20 03:04:33,334: DEBUG/MainProcess] heartbeat_tick : for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:33,334: DEBUG/MainProcess] heartbeat_tick : Prev sent/recv: None/None, now - 42/43, monotonic - 330934.476212158, last_heartbeat_sent - 330934.47621111, heartbeat int. - 20.0 for connection 9884df1b2b3c481899da38f31ce36670\n\n\ncelery-1  | [2024-11-20 03:04:40,002: DEBUG/MainProcess] heartbeat_tick : for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:40,002: DEBUG/MainProcess] heartbeat_tick : Prev sent/recv: 42/43, now - 42/44, monotonic - 330941.144286589, last_heartbeat_sent - 330934.47621111, heartbeat int. - 20.0 for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:40,003: DEBUG/MainProcess] heartbeat_tick: sending heartbeat for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:46,670: DEBUG/MainProcess] heartbeat_tick : for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:46,670: DEBUG/MainProcess] heartbeat_tick : Prev sent/recv: 42/44, now - 43/45, monotonic - 330947.812269588, last_heartbeat_sent - 330947.812268985, heartbeat int. - 20.0 for connection 9884df1b2b3c481899da38f31ce36670\n```\n\n\u003c/details\u003e","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732073110,"metadata":{"github-id":"UCE_lALODBZtRc6UPlzPzlWN3OE"},"target":"f48144e356d9d37d15791e862ca7dc6e4d29c4c08e158b82fd6f247fdd79bc94","message":"digging through the history of issues, it seems we had similar issue before\n- https://github.com/dandi/dandi-archive/issues/567\nwhich was addressed in\n- https://github.com/dandi/dandi-cli/pull/957 describing changes as\n   - Limit celery concurrency to 1. Possibly having multiple workers running at the same time is causing a race condition.\n   - Upgrade the minio image version. The error might be happening when querying the minio container from the celery container, so possibly there is a minio bug that has been patched in the last 4 months.  \n\n\n\u003cdetails\u003e\n\u003csummary\u003eSince then a good number of changes to that compose\u003c/summary\u003e \n\n```diff\ndiff --git a/dandi/tests/data/dandiarchive-docker/docker-compose.yml b/dandi/tests/data/dandiarchive-docker/docker-compose.yml\nindex 9ce5e464..3f132d3e 100644\n--- a/dandi/tests/data/dandiarchive-docker/docker-compose.yml\n+++ b/dandi/tests/data/dandiarchive-docker/docker-compose.yml\n@@ -4,20 +4,7 @@\n # \u003chttps://github.com/dandi/dandi-api/blob/master/docker-compose.override.yml\u003e,\n # but using images uploaded to Docker Hub instead of building them locally.\n \n-version: '2.1'\n-\n services:\n-  redirector:\n-    image: dandiarchive/dandiarchive-redirector\n-    depends_on:\n-      - django\n-    ports:\n-      - \"8079:8080\"\n-    environment:\n-      #GUI_URL: http://localhost:8086\n-      ABOUT_URL: http://www.dandiarchive.org\n-      API_URL: http://localhost:8000/api\n-\n   django:\n     image: dandiarchive/dandiarchive-api\n     command: [\"./manage.py\", \"runserver\", \"--nothreading\", \"0.0.0.0:8000\"]\n@@ -30,23 +17,31 @@ services:\n         condition: service_healthy\n       rabbitmq:\n         condition: service_started\n-    environment:\n+    environment: \u0026django_env\n       DJANGO_CELERY_BROKER_URL: amqp://rabbitmq:5672/\n       DJANGO_CONFIGURATION: DevelopmentConfiguration\n       DJANGO_DANDI_DANDISETS_BUCKET_NAME: dandi-dandisets\n+      DJANGO_DANDI_DANDISETS_LOG_BUCKET_NAME: dandiapi-dandisets-logs\n       DJANGO_DANDI_DANDISETS_EMBARGO_BUCKET_NAME: dandi-embargoed-dandisets\n+      DJANGO_DANDI_DANDISETS_EMBARGO_LOG_BUCKET_NAME: dandiapi-embargo-dandisets-logs\n       DJANGO_DATABASE_URL: postgres://postgres:postgres@postgres:5432/django\n       DJANGO_MINIO_STORAGE_ACCESS_KEY: minioAccessKey\n       DJANGO_MINIO_STORAGE_ENDPOINT: minio:9000\n       DJANGO_MINIO_STORAGE_SECRET_KEY: minioSecretKey\n       DJANGO_STORAGE_BUCKET_NAME: django-storage\n-      DJANGO_MINIO_STORAGE_MEDIA_URL: http://localhost:9000/django-storage\n-      DJANGO_DANDI_SCHEMA_VERSION:\n+      # The Minio URL needs to use 127.0.0.1 instead of localhost so that blob\n+      # assets' \"S3 URLs\" will use 127.0.0.1, and thus tests that try to open\n+      # these URLs via fsspec will not fail on systems where localhost is both\n+      # 127.0.0.1 and ::1.\n+      DJANGO_MINIO_STORAGE_MEDIA_URL: http://127.0.0.1:9000/django-storage\n+      DJANGO_DANDI_SCHEMA_VERSION: ~\n       DJANGO_DANDI_WEB_APP_URL: http://localhost:8085\n       DJANGO_DANDI_API_URL: http://localhost:8000\n+      DJANGO_DANDI_JUPYTERHUB_URL: https://hub.dandiarchive.org\n+      DJANGO_DANDI_DEV_EMAIL: \"test@example.com\"\n       DANDI_ALLOW_LOCALHOST_URLS: \"1\"\n     ports:\n-      - \"8000:8000\"\n+      - \"127.0.0.1:8000:8000\"\n \n   celery:\n     image: dandiarchive/dandiarchive-api\n@@ -70,21 +65,8 @@ services:\n       rabbitmq:\n         condition: service_started\n     environment:\n-      DJANGO_CELERY_BROKER_URL: amqp://rabbitmq:5672/\n-      DJANGO_CONFIGURATION: DevelopmentConfiguration\n-      DJANGO_DANDI_DANDISETS_BUCKET_NAME: dandi-dandisets\n-      DJANGO_DANDI_DANDISETS_EMBARGO_BUCKET_NAME: dandi-embargoed-dandisets\n-      DJANGO_DATABASE_URL: postgres://postgres:postgres@postgres:5432/django\n-      DJANGO_MINIO_STORAGE_ACCESS_KEY: minioAccessKey\n-      DJANGO_MINIO_STORAGE_ENDPOINT: minio:9000\n-      DJANGO_MINIO_STORAGE_SECRET_KEY: minioSecretKey\n-      DJANGO_STORAGE_BUCKET_NAME: django-storage\n-      DJANGO_MINIO_STORAGE_MEDIA_URL: http://localhost:9000/django-storage\n-      DJANGO_DANDI_SCHEMA_VERSION:\n+      \u003c\u003c : *django_env\n       DJANGO_DANDI_VALIDATION_JOB_INTERVAL: \"5\"\n-      DJANGO_DANDI_WEB_APP_URL: http://localhost:8085\n-      DJANGO_DANDI_API_URL: http://localhost:8000\n-      DANDI_ALLOW_LOCALHOST_URLS: \"1\"\n \n   minio:\n     image: minio/minio:RELEASE.2022-04-12T06-55-35Z\n@@ -92,7 +74,7 @@ services:\n     tty: true\n     command: [\"server\", \"/data\"]\n     ports:\n-      - \"9000:9000\"\n+      - \"127.0.0.1:9000:9000\"\n     environment:\n       MINIO_ACCESS_KEY: minioAccessKey\n       MINIO_SECRET_KEY: minioSecretKey\n@@ -107,8 +89,8 @@ services:\n       POSTGRES_DB: django\n       POSTGRES_PASSWORD: postgres\n     image: postgres\n-    ports:\n-      - \"5432:5432\"\n+    expose:\n+      - \"5432\"\n     healthcheck:\n       test: [\"CMD\", \"pg_isready\", \"-U\", \"postgres\"]\n       interval: 7s\n@@ -117,5 +99,5 @@ services:\n \n   rabbitmq:\n     image: rabbitmq:management\n-    ports:\n-      - \"5672:5672\"\n+    expose:\n+      - \"5672\"\n\n```\n\u003c/details\u003e\n\nbut neither minio version nor `-c`  concurrency setting...\n\n\u003cdetails\u003e\n\u003csummary\u003eedit 1:  celery is \"busy\" trying to close some incrementing handle and that is what it is wasting cpu on\u003c/summary\u003e \n\n```shell\nroot@d49aa92590f7:/opt/django# strace -f -p 8 2\u003e\u00261 | head\nstrace: Process 8 attached\nclose(928972594)                        = -1 EBADF (Bad file descriptor)\nclose(928972593)                        = -1 EBADF (Bad file descriptor)\nclose(928972592)                        = -1 EBADF (Bad file descriptor)\nclose(928972591)                        = -1 EBADF (Bad file descriptor)\nclose(928972590)                        = -1 EBADF (Bad file descriptor)\nclose(928972589)                        = -1 EBADF (Bad file descriptor)\nclose(928972588)                        = -1 EBADF (Bad file descriptor)\nclose(928972587)                        = -1 EBADF (Bad file descriptor)\nclose(928972586)                        = -1 EBADF (Bad file descriptor)\nroot@d49aa92590f7:/opt/django# ls -l /proc/8/fd/\ntotal 0\nlrwx------ 1 root root 64 Nov 20 03:06 0 -\u003e /dev/null\nl-wx------ 1 root root 64 Nov 20 03:06 1 -\u003e 'pipe:[5581143]'\nl-wx------ 1 root root 64 Nov 20 03:06 2 -\u003e 'pipe:[5581144]'\nlrwx------ 1 root root 64 Nov 20 03:06 3 -\u003e 'socket:[5575530]'\nlrwx------ 1 root root 64 Nov 20 03:06 4 -\u003e 'socket:[5574189]'\nlrwx------ 1 root root 64 Nov 20 03:06 5 -\u003e 'socket:[5576134]'\nlrwx------ 1 root root 64 Nov 20 03:06 6 -\u003e 'anon_inode:[eventpoll]'\nlr-x------ 1 root root 64 Nov 20 03:06 7 -\u003e /dev/null\nl-wx------ 1 root root 64 Nov 20 03:06 8 -\u003e 'pipe:[5578287]'\n\n```\n\nwhenever in logs it seems to be ok\n\n```\ncelery-1  | [2024-11-20 03:04:27,590: INFO/MainProcess] Task dandiapi.api.tasks.calculate_sha256[281efa6f-f660-4603-a661-f11e7ebcc220] received: ((UUID('13745690-aebd-4812-a1bd-aecb374bb83b'),), {})\ncelery-1  | [2024-11-20 03:04:27,590: DEBUG/MainProcess] TaskPool: Apply \u003cfunction fast_trace_task at 0x7fbb89a50360\u003e (args:('dandiapi.api.tasks.calculate_sha256', '281efa6f-f660-4603-a661-f11e7ebcc220', {'argsrepr': \"(UUID('13745690-aebd-4812-a1bd-aecb374bb83b'),)\", 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'id': '281efa6f-f660-4603-a661-f11e7ebcc220', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen7@e29931eba4e7', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '281efa6f-f660-4603-a661-f11e7ebcc220', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'dandiapi.api.tasks.calculate_sha256', 'timelimit': [None, 86400], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"(UUID('13745690-aebd-4812-a1bd-aecb374bb83b'),)\", 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'id': '281efa6f-f660-4603-a661-f11e7ebcc220', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen7@e29931eba4e7', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\ncelery-1  | [2024-11-20 03:04:27,606: DEBUG/ForkPoolWorker-2] (0.001) SELECT t.oid, typarray FROM pg_type t JOIN pg_namespace ns ON typnamespace = ns.oid WHERE typname = 'hstore'; args=None; alias=default\ncelery-1  | [2024-11-20 03:04:27,606: DEBUG/ForkPoolWorker-2] (0.000) SELECT typarray FROM pg_type WHERE typname = 'citext'; args=None; alias=default\ncelery-1  | [2024-11-20 03:04:27,607: DEBUG/ForkPoolWorker-2] (0.000) SELECT \"api_assetblob\".\"id\", \"api_assetblob\".\"created\", \"api_assetblob\".\"modified\", \"api_assetblob\".\"embargoed\", \"api_assetblob\".\"blob\", \"api_assetblob\".\"blob_id\", \"api_assetblob\".\"sha256\", \"api_assetblob\".\"etag\", \"api_assetblob\".\"size\", \"api_assetblob\".\"download_count\" FROM \"api_assetblob\" WHERE \"api_assetblob\".\"blob_id\" = '13745690-aebd-4812-a1bd-aecb374bb83b'::uuid LIMIT 21; args=(UUID('13745690-aebd-4812-a1bd-aecb374bb83b'),); alias=default\ncelery-1  | [2024-11-20 03:04:27,607: INFO/ForkPoolWorker-2] dandiapi.api.tasks.calculate_sha256[281efa6f-f660-4603-a661-f11e7ebcc220]: Calculating sha256 checksum for asset blob 13745690-aebd-4812-a1bd-aecb374bb83b\ncelery-1  | [2024-11-20 03:04:27,610: DEBUG/ForkPoolWorker-2] http://minio:9000 \"GET /dandi-dandisets/blobs/137/456/13745690-aebd-4812-a1bd-aecb374bb83b HTTP/11\" 200 22\ncelery-1  | [2024-11-20 03:04:27,611: DEBUG/ForkPoolWorker-2] (0.001) UPDATE \"api_assetblob\" SET \"sha256\" = '458eeae191657906bb1fefdca3619f35820263fd9d429e35688de62cf7a8c10b' WHERE \"api_assetblob\".\"blob_id\" = '13745690-aebd-4812-a1bd-aecb374bb83b'::uuid; args=('458eeae191657906bb1fefdca3619f35820263fd9d429e35688de62cf7a8c10b', UUID('13745690-aebd-4812-a1bd-aecb374bb83b')); alias=default\ncelery-1  | [2024-11-20 03:04:27,612: INFO/ForkPoolWorker-2] Task dandiapi.api.tasks.calculate_sha256[281efa6f-f660-4603-a661-f11e7ebcc220] succeeded in 0.02052633202401921s: None\ncelery-1  | [2024-11-20 03:04:27,817: INFO/MainProcess] Task dandiapi.api.tasks.calculate_sha256[77c1f8a3-93e7-4b46-9e3e-73e3db11409e] received: ((UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed'),), {})\ncelery-1  | [2024-11-20 03:04:27,817: DEBUG/MainProcess] TaskPool: Apply \u003cfunction fast_trace_task at 0x7fbb89a50360\u003e (args:('dandiapi.api.tasks.calculate_sha256', '77c1f8a3-93e7-4b46-9e3e-73e3db11409e', {'argsrepr': \"(UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed'),)\", 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'id': '77c1f8a3-93e7-4b46-9e3e-73e3db11409e', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen7@e29931eba4e7', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0, 'root_id': '77c1f8a3-93e7-4b46-9e3e-73e3db11409e', 'shadow': None, 'stamped_headers': None, 'stamps': {}, 'task': 'dandiapi.api.tasks.calculate_sha256', 'timelimit': [None, 86400], 'properties': {'content_type': 'application/json', 'content_encoding': 'utf-8', 'application_headers': {'argsrepr': \"(UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed'),)\", 'eta': None, 'expires': None, 'group': None, 'group_index': None, 'id': '77c1f8a3-93e7-4b46-9e3e-73e3db11409e', 'ignore_result': False, 'kwargsrepr': '{}', 'lang': 'py', 'origin': 'gen7@e29931eba4e7', 'parent_id': None, 'replaced_task_nesting': 0, 'retries': 0,... kwargs:{})\ncelery-1  | [2024-11-20 03:04:27,818: DEBUG/ForkPoolWorker-2] (0.000) SELECT \"api_assetblob\".\"id\", \"api_assetblob\".\"created\", \"api_assetblob\".\"modified\", \"api_assetblob\".\"embargoed\", \"api_assetblob\".\"blob\", \"api_assetblob\".\"blob_id\", \"api_assetblob\".\"sha256\", \"api_assetblob\".\"etag\", \"api_assetblob\".\"size\", \"api_assetblob\".\"download_count\" FROM \"api_assetblob\" WHERE \"api_assetblob\".\"blob_id\" = '7a7988d0-01ed-4bfb-8fa3-28741ced67ed'::uuid LIMIT 21; args=(UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed'),); alias=default\ncelery-1  | [2024-11-20 03:04:27,818: INFO/ForkPoolWorker-2] dandiapi.api.tasks.calculate_sha256[77c1f8a3-93e7-4b46-9e3e-73e3db11409e]: Calculating sha256 checksum for asset blob 7a7988d0-01ed-4bfb-8fa3-28741ced67ed\ncelery-1  | [2024-11-20 03:04:27,820: DEBUG/ForkPoolWorker-2] http://minio:9000 \"GET /dandi-dandisets/blobs/7a7/988/7a7988d0-01ed-4bfb-8fa3-28741ced67ed HTTP/11\" 200 19\ncelery-1  | [2024-11-20 03:04:27,821: DEBUG/ForkPoolWorker-2] (0.001) UPDATE \"api_assetblob\" SET \"sha256\" = '6fef386efa7208eaf1c596b6ab2f8a5a3583696ef8649be0552ab3effad1e191' WHERE \"api_assetblob\".\"blob_id\" = '7a7988d0-01ed-4bfb-8fa3-28741ced67ed'::uuid; args=('6fef386efa7208eaf1c596b6ab2f8a5a3583696ef8649be0552ab3effad1e191', UUID('7a7988d0-01ed-4bfb-8fa3-28741ced67ed')); alias=default\ncelery-1  | [2024-11-20 03:04:27,821: INFO/ForkPoolWorker-2] Task dandiapi.api.tasks.calculate_sha256[77c1f8a3-93e7-4b46-9e3e-73e3db11409e] succeeded in 0.00400476303184405s: None\n\n\n\n\ncelery-1  | [2024-11-20 03:04:33,334: DEBUG/MainProcess] heartbeat_tick : for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:33,334: DEBUG/MainProcess] heartbeat_tick : Prev sent/recv: None/None, now - 42/43, monotonic - 330934.476212158, last_heartbeat_sent - 330934.47621111, heartbeat int. - 20.0 for connection 9884df1b2b3c481899da38f31ce36670\n\n\ncelery-1  | [2024-11-20 03:04:40,002: DEBUG/MainProcess] heartbeat_tick : for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:40,002: DEBUG/MainProcess] heartbeat_tick : Prev sent/recv: 42/43, now - 42/44, monotonic - 330941.144286589, last_heartbeat_sent - 330934.47621111, heartbeat int. - 20.0 for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:40,003: DEBUG/MainProcess] heartbeat_tick: sending heartbeat for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:46,670: DEBUG/MainProcess] heartbeat_tick : for connection 9884df1b2b3c481899da38f31ce36670\ncelery-1  | [2024-11-20 03:04:46,670: DEBUG/MainProcess] heartbeat_tick : Prev sent/recv: 42/44, now - 43/45, monotonic - 330947.812269588, last_heartbeat_sent - 330947.812268985, heartbeat int. - 20.0 for connection 9884df1b2b3c481899da38f31ce36670\n```\n\n\u003c/details\u003e\n\nedit2: I verified that we do run this test in dandi-api tests: https://github.com/dandi/dandi-archive/actions/runs/11925359257/job/33237382168?pr=2076#step:8:30","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732072942,"metadata":{"github-id":"IC_kwDODBZtRc6UQKz-","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2487266558"},"message":"just now spotted in the django outputs\n\n```\ndjango-1  |            INFO     \"GET /api/auth/token/ HTTP/1.1\" 200 42       basehttp.py:187\ndjango-1  | /usr/local/lib/python3.11/site-packages/pydantic/main.py:390: UserWarning: Pydantic serializer warnings:\ndjango-1  |   Failed to get discriminator value for tagged union serialization with value `{'schemaKey': 'Person', '... 'dcite:ContactPerson']}` - defaulting to left to right union serialization.\ndjango-1  |   PydanticSerializationUnexpectedValue: Expected `Person` but got `dict` with value `{'schemaKey': 'Person', '... 'dcite:ContactPerson']}` - serialized value may not be as expected\ndjango-1  |   PydanticSerializationUnexpectedValue: Expected `Organization` but got `dict` with value `{'schemaKey': 'Person', '... 'dcite:ContactPerson']}` - serialized value may not be as expected\ndjango-1  |   Expected `enum` but got `str` with value `'spdx:CC0-1.0'` - serialized value may not be as expected\ndjango-1  |   return self.__pydantic_serializer__.to_python(\ndjango-1  |            INFO     \"POST /api/dandisets/?embargo=false          basehttp.py:187\ndjango-1  |                     HTTP/1.1\" 200 614                                           \n\n```\n\nso may be that is what undermines validation somewhere ?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732113751,"metadata":{"github-id":"IC_kwDODBZtRc6UV5tw","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2488769392"},"message":"ok, I think that celery in our docker compose instance is not properly initialized and never registers for periodic tasks.\nI base it on the fact that adding the additional logging in\n\n- https://github.com/dandi/dandi-archive/pull/2079/files#diff-12445f54eb725ed385017501c3c849c6422c4541850cbf7185ca85729f537ed7R132\n\ndoes not show up anywhere in the logs for the `docker compose logs` run even if I made DJANGO and celery log level debug.  @jjnesbitt @mvandenburgh @jwodder @candleindark -- any ideas on how that could happen and why https://github.com/dandi/dandi-archive/blob/HEAD/dandiapi/celery.py#L26 might not be in effect?","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732113838,"metadata":{"github-id":"UCE_lALODBZtRc6UV5twzlWbUiQ"},"target":"abcec844c48ef7d76e88459069b8bbbccf3d8ad9cb086554b1578684bc2d1603","message":"ok, I think that celery in our docker compose instance is not properly initialized and never registers for periodic tasks.\nI base it on the fact that adding the additional logging in\n\n- https://github.com/dandi/dandi-archive/pull/2079/files#diff-12445f54eb725ed385017501c3c849c6422c4541850cbf7185ca85729f537ed7R132\n\ndoes not show up anywhere in the logs for the `docker compose logs` run even if I made DJANGO and celery log level debug.  @jjnesbitt @mvandenburgh @jwodder @candleindark -- any ideas on how that could happen and why https://github.com/dandi/dandi-archive/blob/HEAD/dandiapi/celery.py#L26 might not be in effect?\n\n\u003cdetails\u003e\n\u003csummary\u003eFTR: here is the patch to docker compose in our tests to enable more logging, and bind mount dandi-archive/dandiapi so I could use \"development\" version\u003c/summary\u003e \n\n```patch\ndiff --git a/dandi/tests/data/dandiarchive-docker/docker-compose.yml b/dandi/tests/data/dandiarchive-docker/docker-compose.yml\nindex cc37b5c5..9108cd5c 100644\n--- a/dandi/tests/data/dandiarchive-docker/docker-compose.yml\n+++ b/dandi/tests/data/dandiarchive-docker/docker-compose.yml\n@@ -24,6 +24,8 @@ services:\n       DJANGO_DANDI_DANDISETS_LOG_BUCKET_NAME: dandiapi-dandisets-logs\n       DJANGO_DANDI_DANDISETS_EMBARGO_BUCKET_NAME: dandi-embargoed-dandisets\n       DJANGO_DANDI_DANDISETS_EMBARGO_LOG_BUCKET_NAME: dandiapi-embargo-dandisets-logs\n+      # Pending https://github.com/dandi/dandi-archive/pull/2078\n+      DJANGO_DANDI_LOG_LEVEL: DEBUG\n       DJANGO_DATABASE_URL: postgres://postgres:postgres@postgres:5432/django\n       DJANGO_MINIO_STORAGE_ACCESS_KEY: minioAccessKey\n       DJANGO_MINIO_STORAGE_ENDPOINT: minio:9000\n@@ -42,6 +44,8 @@ services:\n       DANDI_ALLOW_LOCALHOST_URLS: \"1\"\n     ports:\n       - \"127.0.0.1:8000:8000\"\n+    volumes:\n+      - /home/yoh/proj/dandi/dandi-archive/dandiapi:/opt/django/dandiapi\n \n   celery:\n     image: dandiarchive/dandiarchive-api\n@@ -49,7 +53,7 @@ services:\n       \"celery\",\n       \"--app\", \"dandiapi.celery\",\n       \"worker\",\n-      \"--loglevel\", \"INFO\",\n+      \"--loglevel\", \"DEBUG\",\n       \"--without-heartbeat\",\n       \"-Q\",\"celery,calculate_sha256,ingest_zarr_archive,manifest-worker\",\n       \"-c\",\"1\",\n```\n\u003c/details\u003e","files":null},{"type":3,"author":{"id":"a9e2633aeb6d7e2366a97d09712312c2442c6054"},"timestamp":1732126255,"metadata":{"github-id":"IC_kwDODBZtRc6UXv1d","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489253213"},"message":"Ahh, @yarikoptic I think I see the issue. The way you tested your branch with the CLI docker compose file was to mount the `dandiapi` directory from your checked out git branch to the container directory. That works just fine, however, that log statement is run from the celery container, not the django one. It seems you only mounted the volume for the django container. Mounting it for both locally allows me to see that new log statement:\n\n```\ncelery-1    | [18:08:31] INFO     Registering scheduled tasks for \u003cCelery     scheduled.py:132\ncelery-1    |                     __main__ at 0x71ad933d2090\u003e.\ncelery-1    |                     DANDI_VALIDATION_JOB_INTERVAL is 5 seconds.\n```","files":null},{"type":6,"author":{"id":"a9e2633aeb6d7e2366a97d09712312c2442c6054"},"timestamp":1732126358,"metadata":{"github-id":"UCE_lALODBZtRc6UXv1dzlWh78A"},"target":"3cbeab079d2dffa58b4a086689a2013df83188e9e4436708b65676b7341fcb47","message":"Ahh, @yarikoptic I think I see the issue. The way you tested your branch with the CLI docker compose file was to mount the `dandiapi` directory from your checked out git branch to the container directory. That works just fine, however, that log statement is run from the celery container, not the django one. It seems you only mounted the volume for the django container. Mounting it for both locally allows me to see that new log statement:\n\n```\ncelery-1    | [18:08:31] INFO     Registering scheduled tasks for \u003cCelery     scheduled.py:132\ncelery-1    |                     __main__ at 0x71ad933d2090\u003e.\ncelery-1    |                     DANDI_VALIDATION_JOB_INTERVAL is 5 seconds.\n```\n\n\nSeeing that long with the scheduled tasks in the registered task printout, as well as seeing scheduled tasks running in the logs, seems to prove that scheduled tasks are being picked up just fine.\n\n```\ncelery-1    | [tasks]\n...\ncelery-1    |   . dandiapi.api.tasks.scheduled.aggregate_assets_summary_task\ncelery-1    |   . dandiapi.api.tasks.scheduled.refresh_materialized_view_search\ncelery-1    |   . dandiapi.api.tasks.scheduled.send_pending_users_email\ncelery-1    |   . dandiapi.api.tasks.scheduled.validate_draft_version_metadata\ncelery-1    |   . dandiapi.api.tasks.scheduled.validate_pending_asset_metadata\n...\n```","files":null},{"type":6,"author":{"id":"a9e2633aeb6d7e2366a97d09712312c2442c6054"},"timestamp":1732126373,"metadata":{"github-id":"UCE_lALODBZtRc6UXv1dzlWh8ZQ"},"target":"3cbeab079d2dffa58b4a086689a2013df83188e9e4436708b65676b7341fcb47","message":"Ahh, @yarikoptic I think I see the issue. The way you tested your branch with the CLI docker compose file was to mount the `dandiapi` directory from your checked out git branch to the container directory. That works just fine, however, that log statement is run from the celery container, not the django one. It seems you only mounted the volume for the django container. Mounting it for both locally allows me to see that new log statement:\n\n```\ncelery-1    | [18:08:31] INFO     Registering scheduled tasks for \u003cCelery     scheduled.py:132\ncelery-1    |                     __main__ at 0x71ad933d2090\u003e.\ncelery-1    |                     DANDI_VALIDATION_JOB_INTERVAL is 5 seconds.\n```\n\n\nSeeing that log with the scheduled tasks in the registered task printout, as well as seeing scheduled tasks running in the logs, seems to prove that scheduled tasks are being picked up just fine.\n\n```\ncelery-1    | [tasks]\n...\ncelery-1    |   . dandiapi.api.tasks.scheduled.aggregate_assets_summary_task\ncelery-1    |   . dandiapi.api.tasks.scheduled.refresh_materialized_view_search\ncelery-1    |   . dandiapi.api.tasks.scheduled.send_pending_users_email\ncelery-1    |   . dandiapi.api.tasks.scheduled.validate_draft_version_metadata\ncelery-1    |   . dandiapi.api.tasks.scheduled.validate_pending_asset_metadata\n...\n```","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732128188,"metadata":{"github-id":"IC_kwDODBZtRc6UX854","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489306744"},"message":"Thank you @jjnesbitt, makes total sense.  I will retry with bind mounting modified source tree there too to get logs.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732128908,"metadata":{"github-id":"IC_kwDODBZtRc6UYB1v","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489326959"},"message":"indeed, with bind mount also for celery I got the expected\n```\ncelery-1    |            INFO     Registering scheduled tasks for \u003cCelery     scheduled.py:132\ncelery-1    |                     __main__ at 0x7f0c08551d50\u003e.                                \ncelery-1    |                     DANDI_VALIDATION_JOB_INTERVAL is 5 seconds.  \n```\n\nbut no other logs on having e.g. `validate_draft_version_metadata` ever running","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732133976,"metadata":{"github-id":"IC_kwDODBZtRc6UYkuH","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489469831"},"message":"I have docker 26.1.5+dfsg1-4 from Debian\n\n@jjnesbitt has docker 27.3.1 (likely from upstream)\n\n@asmacdo what is your version of docker?","files":null},{"type":3,"author":{"id":"2db4f5da888e2bc30bfef7de9879ca52bb0fac9e"},"timestamp":1732134182,"metadata":{"github-id":"IC_kwDODBZtRc6UYmLC","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489475778"},"message":"Docker version 27.3.1, build ce12230 from fedora 40\nDocker Compose version v2.29.7","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732134368,"metadata":{"github-id":"IC_kwDODBZtRc6UYnWS","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489480594"},"message":"@asmacdo do you also observe 100% CPU of celery process while tests are waiting for dandiset to get validated.","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732134377,"metadata":{"github-id":"UCE_lALODBZtRc6UYnWSzlWlfus"},"target":"dda031e53ca09a990228fe846982c95ec0879f3bd1dc192ff577c6a65b3ca2b3","message":"@asmacdo do you also observe 100% CPU of celery process while tests are waiting for dandiset to get validated?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732134904,"metadata":{"github-id":"IC_kwDODBZtRc6UYqve","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489494494"},"message":"also both @asmacdo and @jjnesbitt what is output of \n\n```\n docker exec -it dandiarchive-docker-celery-1 bash -c 'ulimit -n'\n1073741816\n docker exec -it dandiarchive-docker-celery-1 python -c \"import os;print(os.sysconf('SC_OPEN_MAX'))\"\n1073741816\n```\nfor you while `docker compose` is running?   That is at least explains why celery (billiard) is busy for me due to this loop: https://github.com/celery/billiard/blob/main/billiard/compat.py#L147","files":null},{"type":3,"author":{"id":"2db4f5da888e2bc30bfef7de9879ca52bb0fac9e"},"timestamp":1732135141,"metadata":{"github-id":"IC_kwDODBZtRc6UYsPt","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489500653"},"message":"@yarikoptic yep, during `dandi/tests/test_dandiapi.py::test_publish_and_manipulate` it celery was at or near 100%  (also looks like its staying near 100 even after it had timed out and moved on to other tests)\n\n![image](https://github.com/user-attachments/assets/7290f8fd-2644-4094-b0ed-c0823b970f16)\n\n\nTo answer your other question:\n```\n$ docker exec -it dandiarchive-docker-celery-1 bash -c 'ulimit -n'\n1073741816\n$ docker exec -it dandiarchive-docker-celery-1 python -c \"import os;print(os.sysconf('SC_OPEN_MAX'))\"\n1073741816\n```","files":null},{"type":6,"author":{"id":"2db4f5da888e2bc30bfef7de9879ca52bb0fac9e"},"timestamp":1732135342,"metadata":{"github-id":"UCE_lALODBZtRc6UYsPtzlWl5m4"},"target":"14b03c440b1b08f7a29e70e9db2f555fe9c4fd96fe875393ead3e13c0b3d7224","message":"@yarikoptic yep, during `dandi/tests/test_dandiapi.py::test_publish_and_manipulate`  celery was at or near 100%  CPU (also looks like its staying near 100 even after that test had timed out and moved on to other tests)\n\n![image](https://github.com/user-attachments/assets/7290f8fd-2644-4094-b0ed-c0823b970f16)\n\n\nTo answer your other question:\n```\n$ docker exec -it dandiarchive-docker-celery-1 bash -c 'ulimit -n'\n1073741816\n$ docker exec -it dandiarchive-docker-celery-1 python -c \"import os;print(os.sysconf('SC_OPEN_MAX'))\"\n1073741816\n```","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732137441,"metadata":{"github-id":"IC_kwDODBZtRc6UY6ij","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489559203"},"message":"FWIW, submitted \n- https://github.com/celery/billiard/pull/417\n\nWith that patched billiard (kudos to py-spy project which allowed me to point to culprit) I proceed ... and test complets green as expected!\n\nThe solution should be - limiting number of open files, ideally at docker compose level but for me that didn't work out yet.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732138646,"metadata":{"github-id":"IC_kwDODBZtRc6UZCK5","github-url":"https://github.com/dandi/dandi-cli/issues/1488#issuecomment-2489590457"},"message":"I also addressed it at the system configuration level (ignore the `storage-driver`)\n\n```shell\nbilena# cat /etc/docker/daemon.json \n{\n  \"storage-driver\": \"btrfs\",\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Name\": \"nofile\",\n      \"Hard\": 2048,\n      \"Soft\": 1024\n    }\n  }\n}\n```","files":null},{"type":4,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1732150476,"metadata":{"github-id":"CE_lADODBZtRc6TG4xqzwAAAAOUY1uJ"},"status":2}]}