{"version":2,"ops":[{"type":5,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1597266802,"metadata":{"github-id":"MDEyOkxhYmVsZWRFdmVudDM2NDk0Nzc1MTg="},"added":["DX"],"removed":[]},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1597347319,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY3MzY3MDk3Ng==","github-url":"https://github.com/dandi/dandi-cli/issues/191#issuecomment-673670976"},"message":"I can reproduce the several seconds of nothing, and it appears to be due to `joblib.Parallel` somehow.  If I disable `joblib.Parallel` by setting `DEVEL_DANDI=1` in the test environment and adding `\"--devel-debug\"` to `cmd` in `test_organize_nwb_test_data`, the time gap goes away.  However, I can't figure out why joblib is doing this; trying to make its logs show up by setting the root logger's log level to DEBUG did not produce any messages from joblib, even though there are references to logging in its code.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1597361873,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY3Mzc2MTMzMw==","github-url":"https://github.com/dandi/dandi-cli/issues/191#issuecomment-673761333"},"message":"THANK YOU for digging into it\nTL;DR summary from my end -- I would at least partially blame a heavy nature of `pynwb` imports - it imports even pandas and ends up even with matplotlib imported.  Filed a motivational PR https://github.com/NeurodataWithoutBorders/pynwb/pull/1282 to start dialog ;-)\n\nif I defer import of pynwb into `get_metadata` call (5e4eb5a0cde5be116e429050bd6588649b9183b1) I would get overall test run of 36 sec, whenever master takes 44 sec (well -- only 8 sec difference, there is still some 2 sec wasted between Loading and Calling)\n\n\u003cdetails\u003e\n\u003csummary\u003eSome additional blurb\u003c/summary\u003e \n\noh - may be it is busy serializing/pickling to pass into subprocesses... and may be we got some heavy thing bound somewhere... I tried to py-spy it but it blew up...\nas for logging, it seems to be quite all over the place in joblib where they do not use a dedicated one and some borrowed submodules do\n```\n$\u003e git grep logging\nconftest.py:import logging\nconftest.py:    \"\"\"Setup multiprocessing logging for the tests\"\"\"\nconftest.py:        log = mp.util.log_to_stderr(logging.DEBUG)\nconftest.py:        log.handlers[0].setFormatter(logging.Formatter(\njoblib/externals/cloudpickle/cloudpickle_fast.py:import logging\njoblib/externals/cloudpickle/cloudpickle_fast.py:    return logging.getLogger, (obj.name,)\njoblib/externals/cloudpickle/cloudpickle_fast.py:    return logging.getLogger, ()\njoblib/externals/cloudpickle/cloudpickle_fast.py:    _dispatch_table[logging.Logger] = _logger_reduce\njoblib/externals/cloudpickle/cloudpickle_fast.py:    _dispatch_table[logging.RootLogger] = _root_logger_reduce\njoblib/externals/loky/_base.py:import logging\njoblib/externals/loky/_base.py:    LOGGER = logging.getLogger(\"concurrent.futures\")\njoblib/externals/loky/backend/fork_exec.py:    # Make sure to keep stdout and stderr open for logging purpose\njoblib/externals/loky/backend/resource_tracker.py:#  * add some VERBOSE logging\njoblib/externals/loky/backend/spawn.py:#  * Improve logging data\njoblib/externals/loky/backend/spawn.py:        import logging\njoblib/externals/loky/backend/spawn.py:            logging.Formatter(data['log_fmt'])\njoblib/func_inspect.py:    # XXX: Not using logging framework\njoblib/logger.py:Helpers for logging.\njoblib/logger.py:import logging\njoblib/logger.py:    \"\"\" Base class for logging messages.\njoblib/logger.py:        logging.warning(\"[%s]: %s\" % (self, msg))\njoblib/logger.py:        logging.debug(\"[%s]: %s\" % (self, msg))\njoblib/memory.py:# TODO: Same remark for the logger, and probably use the Python logging\njoblib/test/test_memory.py:    # capture stdlib logging output (see\njoblib/test/test_memory.py:    # logging destination) to make sure there is no exception while\njoblib/test/test_memory.py:    # logging destination) to make sure there is no exception  \n```\n\nIn DataLad I usually end up just reassigning handlers from datalad if I want some 3rd party module to log... tried here but it lead to the  opposite effect.  Setting root logger to DEBUG though worked. That is the patch\n```\n$\u003e git diff\ndiff --git a/dandi/__init__.py b/dandi/__init__.py\nindex a6caf46..d956171 100644\n--- a/dandi/__init__.py\n+++ b/dandi/__init__.py\n@@ -36,3 +36,7 @@ lgr = get_logger()\n set_logger_level(lgr, os.environ.get(\"DANDI_LOG_LEVEL\", logging.INFO))\n FORMAT = \"%(asctime)-15s [%(levelname)8s] %(message)s\"\n logging.basicConfig(format=FORMAT)\n+\n+logging.root.setLevel(logging.DEBUG)\n+# somehow complitely disables our logging!\n+#logging.root.handlers = lgr.handlers\n```\nand what I saw was:\n```\n$ DANDI_LOG_LEVEL=DEBUG python -m pytest -s -v -k test_organize_nwb_test_data\n...\n2020-08-13 18:26:05,295 [    INFO] Loading metadata from 31 files\n2020-08-13 18:26:08,096 [   DEBUG] CONFIGDIR=/home/yoh/.config/matplotlib\n2020-08-13 18:26:08,102 [   DEBUG] (private) matplotlib data path: /usr/share/matplotlib/mpl-data\n2020-08-13 18:26:08,103 [   DEBUG] loaded rc file /etc/matplotlibrc\n2020-08-13 18:26:08,106 [   DEBUG] matplotlib version 3.2.1\n2020-08-13 18:26:08,106 [   DEBUG] interactive is False\n2020-08-13 18:26:08,107 [   DEBUG] platform is linux\n2020-08-13 18:26:08,107 [   DEBUG] loaded modules: ['sys', 'builtins', '_frozen_importlib',....\n... [more or less the above repeated 12 times, I guess one per core/parallel process]\n020-08-13 18:26:08,763 [   DEBUG] CACHEDIR=/home/yoh/.cache/matplotlib\n2020-08-13 18:26:08,766 [   DEBUG] matplotlib data path: /usr/share/matplotlib/mpl-data\n2020-08-13 18:26:08,773 [   DEBUG] CACHEDIR=/home/yoh/.cache/matplotlib\n2020-08-13 18:26:08,773 [   DEBUG] matplotlib data path: /usr/share/matplotlib/mpl-data\n2020-08-13 18:26:08,782 [   DEBUG] Using fontManager instance from /home/yoh/.cache/matplotlib/fontlist-v310.json\n2020-08-13 18:26:08,799 [   DEBUG] Using fontManager instance from /home/yoh/.cache/matplotlib/fontlist-v310.json\n2020-08-13 18:26:08,812 [   DEBUG] CACHEDIR=/home/yoh/.cache/matplotlib\n2020-08-13 18:26:08,816 [   DEBUG] matplotlib data path: /usr/share/matplotlib/mpl-data\n2020-08-13 18:26:08,824 [   DEBUG] Using fontManager instance from /home/yoh/.cache/matplotlib/fontlist-v310.json\n2020-08-13 18:26:10,597 [   DEBUG] Calling memoized version of \u003cfunction get_metadata at 0x7fb0092c3c10\u003e for /home/yoh/.tmp/tmpcynfjpr3/v2.0.1/test_CurrentClampSeries.nwb\n```\n\nhere it was just 5 not 9 seconds (may be 0.6.0 got faster? ;)), but still awhile -- so it took 3.5 seconds to start subprocess?! those VERY long lists of modules in each subprocess, I hope they are not really reimported again and just reported to be imported at that point ... then there is an additional 1.5 sec at the end before calling memoized which is not accounted for... \n\n\u003c/details\u003e\n\nBut overall I now question my initial assessment that I have not experienced that slow operation before... may be later I will try to time travel into original dates when I introduced parallelization there and see if I could avoid seeing the effect...  By a quick downgrade of pynwb alone I do get leaner \"imports profile\"  but overall \"quality\" is comparable.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1597368818,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDY3MzgwNzY5Ng==","github-url":"https://github.com/dandi/dandi-cli/issues/191#issuecomment-673807696"},"message":"I would need to compare on some real use case (dataset) where we would actually need to load metadata (so there would be no cache) to see how it compares to multiprocessing approach but as for the tests\n```patch\ndiff --git a/dandi/cli/cmd_organize.py b/dandi/cli/cmd_organize.py\nindex 2424bcd..8c0d2e4 100644\n--- a/dandi/cli/cmd_organize.py\n+++ b/dandi/cli/cmd_organize.py\n@@ -182,7 +182,7 @@ def organize(\n             # to no benefit from Parallel without using multiproc!  But that would\n             # complicate progress bar indication... TODO\n             metadata = list(\n-                Parallel(n_jobs=-1, verbose=10)(\n+                Parallel(n_jobs=-1, verbose=10, backend=\"threading\")(\n                     delayed(_get_metadata)(path) for path in paths\n                 )\n             )\n```\nmakes those tests run in 10-12 instead of 47 seconds ... we could have exposed type of parallelization as an option, but I think that would be overkill.","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1597368818,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6MzkyOTkxOTIx"},"target":"a5e741dc72e6d600114bd5030ebbb4d5e3ac48ac6447299b9c5b28fbcc414f44","message":"I would need to compare on some real use case (dataset) where we would actually need to load metadata (so there would be no cache) to see how it compares to multiprocessing approach but as for the tests\n```patch\ndiff --git a/dandi/cli/cmd_organize.py b/dandi/cli/cmd_organize.py\nindex 2424bcd..8c0d2e4 100644\n--- a/dandi/cli/cmd_organize.py\n+++ b/dandi/cli/cmd_organize.py\n@@ -182,7 +182,7 @@ def organize(\n             # to no benefit from Parallel without using multiproc!  But that would\n             # complicate progress bar indication... TODO\n             metadata = list(\n-                Parallel(n_jobs=-1, verbose=10)(\n+                Parallel(n_jobs=-1, verbose=10, backend=\"threading\")(\n                     delayed(_get_metadata)(path) for path in paths\n                 )\n             )\n```\nmakes those tests run in 10-12 instead of 47 seconds ... we could have exposed type of parallelization as an option, but I think that would be overkill.\n\nedit: comment in the code says that it is CPU intensive so parallelization for the cases where there is no cached metadata -- would better be done via parallel processes.  We could make it \"smarter\" by exposing interface to query the cache on which ones would need to be really computed, and then invoke Parallel only on those. But it would need some kind of a helper shim on top of Parallel (e.g. `CachedParallel`) to avoid coding for such cases all the time with identical code.","files":null},{"type":5,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1600117093,"metadata":{"github-id":"MDEyOkxhYmVsZWRFdmVudDM3NjUzMTU3NDQ="},"added":["no solution identified"],"removed":[]}]}