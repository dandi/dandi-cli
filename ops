{"version":2,"ops":[{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639666789,"metadata":{"github-id":"IC_kwDODBZtRc47XCxJ","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-995896393"},"message":"@dchiquito Questions on the zarr upload API:\n\n* The docs for `POST /api/zarr/{zarr_id}/upload/` say \"The number of files being uploaded must be less than some experimentally defined limit\".  What is the current/recommended limit?\n* Number 3 in \"Requirements\" states \"The CLI uses some kind of tree hashing scheme to compute a checksum for the entire zarr archive,\" but the \"Upload flow\" section makes no mention of such a checksum.  Does the client have to compute a checksum or not, and, if it does, where is it used?\n* The document says \"The asset metadata will contain the information required to determine if an asset is a normal file blob or a zarr file.\"  Exactly what information is that, and is it added by the client or the server?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639667701,"metadata":{"github-id":"UCE_lALODBZtRc47XCxJzh-lnk0"},"target":"3560824ebc3a235ac835a54f448cae2af1cecebc598b0f02bf3bc9ac17f4a7e7","message":"@dchiquito Questions on the zarr upload API:\n\n* The docs for `POST /api/zarr/{zarr_id}/upload/` say \"The number of files being uploaded must be less than some experimentally defined limit\".  What is the current/recommended limit?\n* Number 3 in \"Requirements\" states \"The CLI uses some kind of tree hashing scheme to compute a checksum for the entire zarr archive,\" but the \"Upload flow\" section makes no mention of such a checksum.  Does the client have to compute a checksum or not, and, if it does, where is it used?\n* The document says \"The asset metadata will contain the information required to determine if an asset is a normal file blob or a zarr file.\"  Exactly what information is that, and is it added by the client or the server?\n* For `POST /zarr/`, what exactly should the \"name\" field in the request body be set to?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639668393,"metadata":{"github-id":"UCE_lALODBZtRc47XCxJzh-ls9s"},"target":"3560824ebc3a235ac835a54f448cae2af1cecebc598b0f02bf3bc9ac17f4a7e7","message":"@dchiquito Questions on the zarr upload API:\n\n* The docs for `POST /api/zarr/{zarr_id}/upload/` say \"The number of files being uploaded must be less than some experimentally defined limit\".  What is the current/recommended limit?\n* Number 3 in \"Requirements\" states \"The CLI uses some kind of tree hashing scheme to compute a checksum for the entire zarr archive,\" but the \"Upload flow\" section makes no mention of such a checksum.  Does the client have to compute a checksum or not, and, if it does, where is it used?\n* The document says \"The asset metadata will contain the information required to determine if an asset is a normal file blob or a zarr file.\"  Exactly what information is that, and is it added by the client or the server?\n* For `POST /zarr/`, what exactly should the \"name\" field in the request body be set to?\n* \".checksum files format\" says \"Each zarr file and directory in the archive has a path and a checksum (md5). For files, this is simply the ETag.\"  So is the checksum for a file an md5 or a DANDI e-tag?  If the latter, why are we using different digests for files on their own versus in directories?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639668503,"metadata":{"github-id":"UCE_lALODBZtRc47XCxJzh-ltw4"},"target":"3560824ebc3a235ac835a54f448cae2af1cecebc598b0f02bf3bc9ac17f4a7e7","message":"@dchiquito Questions on the zarr upload API:\n\n* The docs for `POST /api/zarr/{zarr_id}/upload/` say \"The number of files being uploaded must be less than some experimentally defined limit\".  What is the current/recommended limit?\n* Number 3 in \"Requirements\" states \"The CLI uses some kind of tree hashing scheme to compute a checksum for the entire zarr archive,\" but the \"Upload flow\" section makes no mention of such a checksum.  Does the client have to compute a checksum or not, and, if it does, where is it used?\n* The document says \"The asset metadata will contain the information required to determine if an asset is a normal file blob or a zarr file.\"  Exactly what information is that, and is it added by the client or the server?\n* For `POST /zarr/`, what exactly should the \"name\" field in the request body be set to?\n* \".checksum files format\" says \"Each zarr file and directory in the archive has a path and a checksum (md5). For files, this is simply the ETag.\"  So is the checksum for a file an md5 or a DANDI e-tag?  If the latter, why are we using different digests for files on their own versus in directories?\n* \"Upload flow\" states \"The client calculates the MD5 of each file.\" and \"The client sends the paths+MD5s to ...\", but the field name in the request body for `POST /zarr/{zarr_id}/upload/` is `\"etag\"`, not `\"md5\"`.  Which digest is supposed to be used?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639668914,"metadata":{"github-id":"UCE_lALODBZtRc47XCxJzh-lxFI"},"target":"3560824ebc3a235ac835a54f448cae2af1cecebc598b0f02bf3bc9ac17f4a7e7","message":"@dchiquito Questions on the zarr upload API:\n\n* The docs for `POST /api/zarr/{zarr_id}/upload/` say \"The number of files being uploaded must be less than some experimentally defined limit\".  What is the current/recommended limit?\n* Number 3 in \"Requirements\" states \"The CLI uses some kind of tree hashing scheme to compute a checksum for the entire zarr archive,\" but the \"Upload flow\" section makes no mention of such a checksum.  Does the client have to compute a checksum or not, and, if it does, where is it used?\n* The document says \"The asset metadata will contain the information required to determine if an asset is a normal file blob or a zarr file.\"  Exactly what information is that, and is it added by the client or the server?\n* For `POST /zarr/`, what exactly should the \"name\" field in the request body be set to?\n* \".checksum files format\" says \"Each zarr file and directory in the archive has a path and a checksum (md5). For files, this is simply the ETag.\"  So is the checksum for a file an md5 or a DANDI e-tag?  If the latter, why are we using different digests for files on their own versus in directories?\n* \"Upload flow\" states \"The client calculates the MD5 of each file.\" and \"The client sends the paths+MD5s to ...\", but the field name in the request body for `POST /zarr/{zarr_id}/upload/` is `\"etag\"`, not `\"md5\"`.  Which digest is supposed to be used?\n* Unlike `POST /uploads/initialize/`, `POST /zarr/{zarr_id}/upload/` does not return information on different parts of files.  Does this mean that each file in a zarr directory is to be uploaded in a single request?  What if a file exceeds S3's part size limit of 5 GB?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639668997,"metadata":{"github-id":"UCE_lALODBZtRc47XCxJzh-lxtw"},"target":"3560824ebc3a235ac835a54f448cae2af1cecebc598b0f02bf3bc9ac17f4a7e7","message":"@dchiquito Questions on the zarr upload API:\n\n* The docs for `POST /api/zarr/{zarr_id}/upload/` say \"The number of files being uploaded must be less than some experimentally defined limit\".  What is the current/recommended limit?\n* Number 3 in \"Requirements\" states \"The CLI uses some kind of tree hashing scheme to compute a checksum for the entire zarr archive,\" but the \"Upload flow\" section makes no mention of such a checksum.  Does the client have to compute a checksum or not, and, if it does, where is it used?\n* The document says \"The asset metadata will contain the information required to determine if an asset is a normal file blob or a zarr file.\"  Exactly what information is that, and is it added by the client or the server?\n* For `POST /zarr/`, what exactly should the \"name\" field in the request body be set to?\n* \".checksum files format\" says \"Each zarr file and directory in the archive has a path and a checksum (md5). For files, this is simply the ETag.\"  So is the checksum for a file an md5 or a DANDI e-tag?  If the latter, why are we using different digests for files on their own versus in directories?\n* \"Upload flow\" states \"The client calculates the MD5 of each file.\" and \"The client sends the paths+MD5s to ...\", but the field name in the request body for `POST /zarr/{zarr_id}/upload/` is `\"etag\"`, not `\"md5\"`.  Which digest is supposed to be used?\n    * Similarly, the docs for `POST /api/zarr/{zarr_id}/upload/` say, \"Requires a list of file paths and ETags (md5 checksums)\".  E-tags and MD5 checksums are not the same thing.\n* Unlike `POST /uploads/initialize/`, `POST /zarr/{zarr_id}/upload/` does not return information on different parts of files.  Does this mean that each file in a zarr directory is to be uploaded in a single request?  What if a file exceeds S3's part size limit of 5 GB?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639669691,"metadata":{"github-id":"UCE_lALODBZtRc47XCxJzh-l3aM"},"target":"3560824ebc3a235ac835a54f448cae2af1cecebc598b0f02bf3bc9ac17f4a7e7","message":"@dchiquito Questions on the zarr upload API:\n\n* The docs for `POST /api/zarr/{zarr_id}/upload/` say \"The number of files being uploaded must be less than some experimentally defined limit\".  What is the current/recommended limit?\n* Number 3 in \"Requirements\" states \"The CLI uses some kind of tree hashing scheme to compute a checksum for the entire zarr archive,\" but the \"Upload flow\" section makes no mention of such a checksum.  Does the client have to compute a checksum or not, and, if it does, where is it used?\n* The document says \"The asset metadata will contain the information required to determine if an asset is a normal file blob or a zarr file.\"  Exactly what information is that, and is it added by the client or the server?\n* For `POST /zarr/`, what exactly should the \"name\" field in the request body be set to?\n* \".checksum files format\" says \"Each zarr file and directory in the archive has a path and a checksum (md5). For files, this is simply the ETag.\"  So is the checksum for a file an md5 or a DANDI e-tag?  If the latter, why are we using different digests for files on their own versus in directories?\n* \"Upload flow\" states \"The client calculates the MD5 of each file.\" and \"The client sends the paths+MD5s to ...\", but the field name in the request body for `POST /zarr/{zarr_id}/upload/` is `\"etag\"`, not `\"md5\"`.  Which digest is supposed to be used?\n    * Similarly, the docs for `POST /api/zarr/{zarr_id}/upload/` say, \"Requires a list of file paths and ETags (md5 checksums)\".  E-tags and MD5 checksums are not the same thing.\n* Unlike `POST /uploads/initialize/`, `POST /zarr/{zarr_id}/upload/` does not return information on different parts of files.  Does this mean that each file in a zarr directory is to be uploaded in a single request?  What if a file exceeds S3's part size limit of 5 GB?\n* The Swagger docs don't describe the request body format for `DELETE /api/zarr/{zarr_id}/files/`.","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639684394,"metadata":{"github-id":"UCE_lALODBZtRc47XCxJzh-nlXA"},"target":"3560824ebc3a235ac835a54f448cae2af1cecebc598b0f02bf3bc9ac17f4a7e7","message":"@dchiquito Questions on the zarr upload API:\n\n* The docs for `POST /api/zarr/{zarr_id}/upload/` say \"The number of files being uploaded must be less than some experimentally defined limit\".  What is the current/recommended limit?\n* Number 3 in \"Requirements\" states \"The CLI uses some kind of tree hashing scheme to compute a checksum for the entire zarr archive,\" but the \"Upload flow\" section makes no mention of such a checksum.  Does the client have to compute a checksum or not, and, if it does, where is it used?\n* The document says \"The asset metadata will contain the information required to determine if an asset is a normal file blob or a zarr file.\"  Exactly what information is that, and is it added by the client or the server?\n* For `POST /zarr/`, what exactly should the \"name\" field in the request body be set to?\n* \".checksum files format\" says \"Each zarr file and directory in the archive has a path and a checksum (md5). For files, this is simply the ETag.\"  So is the checksum for a file an md5 or a DANDI e-tag?  If the latter, why are we using different digests for files on their own versus in directories?\n* \"Upload flow\" states \"The client calculates the MD5 of each file.\" and \"The client sends the paths+MD5s to ...\", but the field name in the request body for `POST /zarr/{zarr_id}/upload/` is `\"etag\"`, not `\"md5\"`.  Which digest is supposed to be used?\n    * Similarly, the docs for `POST /api/zarr/{zarr_id}/upload/` say, \"Requires a list of file paths and ETags (md5 checksums)\".  E-tags and MD5 checksums are not the same thing.\n* Unlike `POST /uploads/initialize/`, `POST /zarr/{zarr_id}/upload/` does not return information on different parts of files.  Does this mean that each file in a zarr directory is to be uploaded in a single request?  What if a file exceeds S3's part size limit of 5 GB?\n* The Swagger docs don't describe the request body format for `DELETE /api/zarr/{zarr_id}/files/`.\n* How do you download a Zarr?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1639670194,"metadata":{"github-id":"IC_kwDODBZtRc47XPpP","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-995949135"},"message":"use the zarr python library to open the nested directory store (i think to start with we will only support this backend). it will check consistency. files will not necessarily have zarr extension. in fact ngff uses .ngff extension. also ngff validator not in place at the moment, so zarr is the closest. i've posted this issue for seeking additional clarity: https://github.com/zarr-developers/zarr-python/issues/912","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639670259,"metadata":{"github-id":"IC_kwDODBZtRc47XP3-","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-995950078"},"message":"@dchiquito I'm trying to upload a trivial test zarr with the following code:\n\n```python\n#!/usr/bin/env python3\nimport json\nimport os\nfrom pathlib import Path\nimport sys\nfrom dandi.dandiapi import DandiAPIClient, RESTFullAPIClient\nfrom dandi.support.digests import get_dandietag\nfrom dandi.utils import find_files\n\ndandiset_id = sys.argv[1]\nzarrdir = Path(sys.argv[2])\nif zarrdir.suffix != \".zarr\" or not zarrdir.is_dir():\n    sys.exit(f\"{zarrdir} is not a zarr directory\")\n\nwith DandiAPIClient.for_dandi_instance(\n    \"dandi-staging\", token=os.environ[\"DANDI_API_KEY\"]\n) as client:\n    r = client.post(\"/zarr/\", json={\"name\": zarrdir.name})\n    zarr_id = r[\"zarr_id\"]\n    zfiles = list(map(Path, find_files(r\".*\", str(zarrdir))))\n    upload_body = []\n    for zf in zfiles:\n        upload_body.append({\n            \"path\": zf.relative_to(zarrdir).as_posix(),\n            \"etag\": get_dandietag(zf).as_str(),\n        })\n    r = client.post(f\"/zarr/{zarr_id}/upload/\", json=upload_body)\n    with RESTFullAPIClient(\"http://nil.nil\") as storage:\n        for upspec in r:\n            with (zarrdir / upspec[\"path\"]).open(\"rb\") as fp:\n                storage.put(upspec[\"upload_url\"], data=fp, json_resp=False)\n    r = client.post(f\"/zarr/{zarr_id}/upload/complete/\")\n    #print(json.dumps(r, indent=4))\n    d = client.get_dandiset(dandiset_id, \"draft\", lazy=False)\n    r = client.post(\n        f\"{d.version_api_path}assets/\",\n        json={\"metadata\": asset_metadata, \"zarr_id\": zarr_id},\n    )\n    print(json.dumps(r, indent=4))\n```\n\nbut I'm getting a 403 response when PUTting the files to S3:\n\n    \u003cError\u003e\u003cCode\u003eAccessDenied\u003c/Code\u003e\u003cMessage\u003eAccess Denied\u003c/Message\u003e\u003cRequestId\u003eY16M1QN13AM6769S\u003c/RequestId\u003e\u003cHostId\u003e4uYw6UzqgIdTUIhqRXqW/PDmD6lXmIv53YYPHJFYv/u2rKi62895bV6jSCqrJCKF3qhJZJI1RCw=\u003c/HostId\u003e\u003c/Error\u003e","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639681571,"metadata":{"github-id":"UCE_lALODBZtRc47XP3-zh-nSdI"},"target":"567b74c594d5d0a8ec62ee9e2b1cff003ca93d551d2bcaa052247c128f5de096","message":"@dchiquito I'm trying to upload a trivial test zarr with the following code:\n\n```python\n#!/usr/bin/env python3\nimport json\nimport os\nfrom pathlib import Path\nimport sys\nfrom dandi.dandiapi import DandiAPIClient, RESTFullAPIClient\nfrom dandi.support.digests import get_dandietag\nfrom dandi.utils import find_files\n\ndandiset_id = sys.argv[1]\nzarrdir = Path(sys.argv[2])\nif zarrdir.suffix != \".zarr\" or not zarrdir.is_dir():\n    sys.exit(f\"{zarrdir} is not a zarr directory\")\n\nwith DandiAPIClient.for_dandi_instance(\n    \"dandi-staging\", token=os.environ[\"DANDI_API_KEY\"]\n) as client:\n    r = client.post(\"/zarr/\", json={\"name\": zarrdir.name})\n    zarr_id = r[\"zarr_id\"]\n    zfiles = list(map(Path, find_files(r\".*\", str(zarrdir))))\n    upload_body = []\n    for zf in zfiles:\n        upload_body.append({\n            \"path\": zf.relative_to(zarrdir).as_posix(),\n            \"etag\": get_dandietag(zf).as_str(),\n        })\n    r = client.post(f\"/zarr/{zarr_id}/upload/\", json=upload_body)\n    with RESTFullAPIClient(\"http://nil.nil\") as storage:\n        for upspec in r:\n            with (zarrdir / upspec[\"path\"]).open(\"rb\") as fp:\n                storage.put(upspec[\"upload_url\"], data=fp, json_resp=False)\n    r = client.post(f\"/zarr/{zarr_id}/upload/complete/\")\n    #print(json.dumps(r, indent=4))\n    d = client.get_dandiset(dandiset_id, \"draft\", lazy=False)\n    r = client.post(\n        f\"{d.version_api_path}assets/\",\n        json={\"metadata\": {\"path\": zarrdir.name}, \"zarr_id\": zarr_id},\n    )\n    print(json.dumps(r, indent=4))\n```\n\nbut I'm getting a 403 response when PUTting the files to S3:\n\n    \u003cError\u003e\u003cCode\u003eAccessDenied\u003c/Code\u003e\u003cMessage\u003eAccess Denied\u003c/Message\u003e\u003cRequestId\u003eY16M1QN13AM6769S\u003c/RequestId\u003e\u003cHostId\u003e4uYw6UzqgIdTUIhqRXqW/PDmD6lXmIv53YYPHJFYv/u2rKi62895bV6jSCqrJCKF3qhJZJI1RCw=\u003c/HostId\u003e\u003c/Error\u003e","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639670315,"metadata":{"github-id":"IC_kwDODBZtRc47XQEQ","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-995950864"},"message":"@satra \n\n\u003e files will not necessarily have zarr extension\n\nThen how do we tell whether something is a zarr directory or not?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1639671429,"metadata":{"github-id":"IC_kwDODBZtRc47XUDO","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-995967182"},"message":"\u003e Then how do we tell whether something is a zarr directory or not?\n\nopen it as a NestedDirectoryStore in read mode with the zarr-python library. it should be able to give you information about groups and shapes, and metadata. we would essentially validate based on those and for the moment write our own requirements for the ngff files till the ome-zarr-py library implements those. but since this is generic zarr to start with, i would use the xarray or zarr python libraries to read the dataset.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639671496,"metadata":{"github-id":"IC_kwDODBZtRc47XUSW","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-995968150"},"message":"@satra So we just try to open every single directory with the zarr library and see it if succeeds?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1639672069,"metadata":{"github-id":"IC_kwDODBZtRc47XWY_","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-995976767"},"message":"@satra:\n\n\u003e use the zarr python library to open the nested directory store (i think to start with we will only support this backend). it will check consistency. files will not necessarily have zarr extension. in fact ngff uses .ngff extension. also ngff validator not in place at the moment, so zarr is the closest. i've posted this issue for seeking additional clarity: [zarr-developers/zarr-python#912](https://github.com/zarr-developers/zarr-python/issues/912)\n\n@jwodder \n\n\u003e So we just try to open every single directory with the zarr library and see it if succeeds?\n\n1.  I think we should formalize \"supported zarr directory name extensions\" to a limited set one way or another\n2. We can't rely on \"sensing\" each directory alone since e.g. it alone would probably not tell us (if errors out) if a directory potentially contains a (broken) zarr archive or just a collection of files\n3. But we must not allow for upload of \"heavy trees\" as any zarr would likely to be -- we would flood with thousands of assets which would only need to be removed\n\nThus I think we should\n1. do formalize \"supported zarr directory name extensions\" e.g. to a set of `.zarr` and `.ngff`\n2. for any of those do test if they are \"ok\" by trying to open.  If not -- error/skip\n3. if we detect file tree deeper than some N directories down (5?) -- we error out.  Might need to become an option somewhere.  In current `dandi organize`ed scheme (`sub-*/{files}`) we have only 1 level down.  In BIDS (`sub-*/ses-*/{modality}/{files}/potentially-onemore`) we have 4.","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1639672429,"metadata":{"github-id":"UCE_lALODBZtRc47XWY_zh-mNvk"},"target":"0176499488c7bde39c3a08cdf8754f779ab355e9338bd4bdd20c247f200c2a91","message":"@satra:\n\n\u003e use the zarr python library to open the nested directory store (i think to start with we will only support this backend). it will check consistency. files will not necessarily have zarr extension. in fact ngff uses .ngff extension. also ngff validator not in place at the moment, so zarr is the closest. i've posted this issue for seeking additional clarity: [zarr-developers/zarr-python#912](https://github.com/zarr-developers/zarr-python/issues/912)\n\n@jwodder \n\n\u003e So we just try to open every single directory with the zarr library and see it if succeeds?\n\n1.  I think we should formalize \"supported zarr directory name extensions\" to a limited set one way or another\n2. We can't rely on \"sensing\" each directory alone since e.g. it alone would probably not tell us (if errors out) if a directory potentially contains a (broken) zarr archive or just a collection of files\n3. But we must not allow for upload of \"heavy trees\" as any zarr would likely to be -- we would flood with thousands of assets which would only need to be removed\n\nThus I think we should\n1. do formalize \"supported zarr directory name extensions\" e.g. to a set of `.zarr` and `.ngff`\n2. for any of those do test if they are \"ok\" by trying to open.  If not -- error/skip\n3. if we detect file tree deeper than some N directories down (5?) -- we error out.  Might need to become an option somewhere.  In current `dandi organize`ed scheme (`sub-*/{files}`) we have only 1 level down.  In BIDS (`sub-*/ses-*/{modality}/{files}/potentially-onemore`) we have 4.\n\nedit 1: I think it is ok to add `zarr` as a dependency\n\n\u003cdetails\u003e\n\u003csummary\u003eadds only 2 dependencies -- asciitree and numcodecs (this one seems a bit heavy though).  and zarr is in conda-forge\u003c/summary\u003e \n\n```shell\n$\u003e pip install zarr\nCollecting zarr\n  Using cached zarr-2.10.3-py3-none-any.whl (146 kB)\nRequirement already satisfied: numpy\u003e=1.7 in ./venvs/dev3/lib/python3.9/site-packages (from zarr) (1.21.4)\nRequirement already satisfied: fasteners in ./venvs/dev3/lib/python3.9/site-packages (from zarr) (0.16.3)\nCollecting asciitree\n  Using cached asciitree-0.3.3-py3-none-any.whl\nCollecting numcodecs\u003e=0.6.4\n  Using cached numcodecs-0.9.1-cp39-cp39-manylinux2010_x86_64.whl (6.4 MB)\nRequirement already satisfied: six in ./venvs/dev3/lib/python3.9/site-packages (from fasteners-\u003ezarr) (1.16.0)\nInstalling collected packages: numcodecs, asciitree, zarr\nSuccessfully installed asciitree-0.3.3 numcodecs-0.9.1 zarr-2.10.3\n```\n\u003c/details\u003e","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1639672569,"metadata":{"github-id":"IC_kwDODBZtRc47XYLV","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-995984085"},"message":"\u003e do formalize \"supported zarr directory name extensions\" e.g. to a set of .zarr and .ngff\n\ni think for the moment this would be fine.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1639672626,"metadata":{"github-id":"IC_kwDODBZtRc47XYXg","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-995984864"},"message":"zarr with also require compression codecs to be added.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639673922,"metadata":{"github-id":"IC_kwDODBZtRc47Xcsk","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996002596"},"message":"@yarikoptic @satra Note that zarr seems to ignore files with invalid/unexpected names, and a `*.zarr` directory containing only such files with no `.zarray` or `.zgroup` is treated as an empty group.  How should directories like this be handled?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1639674658,"metadata":{"github-id":"IC_kwDODBZtRc47XfYZ","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996013593"},"message":"\u003e a *.zarr directory containing only such files with no .zarray or .zgroup is treated as an empty group. How should directories like this be handled?\n\nfor the moment, let's say no empty groups allowed.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639677875,"metadata":{"github-id":"IC_kwDODBZtRc47XpS4","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996054200"},"message":"@yarikoptic\n\n* When uploading a Zarr directory, should all files within it be uploaded, even \"extra\"/unrecognized ones?  Note that Zarr does not seem to provide a way to only list recognized files.\n* Should `--allow-any-path` have any effect on the treatment of Zarr directories, particularly invalid ones?  If not, should there be a way to force processing of an invalid Zarr directory?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1639680662,"metadata":{"github-id":"IC_kwDODBZtRc47XxQx","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996086833"},"message":"\u003e \u003e a *.zarr directory containing only such files with no .zarray or .zgroup is treated as an empty group. How should directories like this be handled?\n\u003e \n\u003e for the moment, let's say no empty groups allowed.\n\nFWIW, pretty much we need `zarr_validate` (as if to complement `pynwb_validate`) which would do all those checks. Then it would be interfaced in `validate` and `upload`.\n\n\u003e Should --allow-any-path have any effect on the treatment of Zarr directories, particularly invalid ones? If not, should there be a way to force processing of an invalid Zarr directory?\n\nif per above we just add `zarr_validate` and since we do allow to upload without validation -- uniform way would be to \"allow\" users to upload invalid zarrs if they say so via `--validation [skip|ignore]`","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639681898,"metadata":{"github-id":"IC_kwDODBZtRc47X07P","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996101839"},"message":"@yarikoptic So, in the absence of or prior to validation, would we just treat any directory with a `.zarr` or `.ngff` extension as a Zarr directory?\n\nAlso, how should metadata be determined for Zarr assets?  (cc @satra)","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1639682767,"metadata":{"github-id":"IC_kwDODBZtRc47X3mh","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996112801"},"message":"\u003e @yarikoptic So, in the absence of or prior to validation, would we just treat any directory with a `.zarr` or `.ngff` extension as a Zarr directory?\n\nYes","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1639683150,"metadata":{"github-id":"IC_kwDODBZtRc47X4pe","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996117086"},"message":"for the moment i would limit metadata extraction similar to the bids data, so based on names rather than internal metadata. in the future once we get better ngff metadata we will write additional extractors. i can help with the metadata extraction beyond basic metadata (size, encoding type, subject id). for dandi this would be part of bids datasets, so we will have additional info available for sticking into the asset metadata from participants.tsv and samples.tsv files.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639690387,"metadata":{"github-id":"IC_kwDODBZtRc47YPeG","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996210566"},"message":"@yarikoptic FYI: I'm implementing the metadata \u0026 validation for Zarr by giving NWB, Zarr, and generic files their own classes with metadata and validation methods; however, fscacher doesn't currently support caching of instance methods, so some caching is going to have to be disabled for now.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1639692030,"metadata":{"github-id":"IC_kwDODBZtRc47YTWA","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996226432"},"message":"hm, do you see how fscacher could gain support for bound methods? if not, I wonder if we shouldn't just concentrate logic in @staticmethods of such classes which would be explicitly passed a path instead of an instance?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1639693078,"metadata":{"github-id":"IC_kwDODBZtRc47YV54","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996236920"},"message":"@yarikoptic We would have to add a variant of `memoize_path()` that gets the affected path from a given attribute of the decorated method's `self` parameter.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1639693448,"metadata":{"github-id":"IC_kwDODBZtRc47YWvo","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-996240360"},"message":"right -- sounds good! somehow it didn't occur to me ;)","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1640280897,"metadata":{"github-id":"IC_kwDODBZtRc47oYmx","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1000442289"},"message":"\u003e The docs for POST /api/zarr/{zarr_id}/upload/ say \"The number of files being uploaded must be less than some experimentally defined limit\". What is the current/recommended limit?\n\nCurrently there is no limit. Now that we have the code up on a real server I need to do some experimentation to determine that value. The limit is mostly there to enforce that no request takes longer than 30 seconds, since Heroku will forcibly cancel any requests that exceed that timeout.\n\n\u003e Number 3 in \"Requirements\" states \"The CLI uses some kind of tree hashing scheme to compute a checksum for the entire zarr archive,\" but the \"Upload flow\" section makes no mention of such a checksum. Does the client have to compute a checksum or not, and, if it does, where is it used?\n\nThe client would need to calculate the checksum to do integrity checking of the zarr upload. I wrote https://github.com/dandi/dandi-api/blob/master/dandiapi/api/zarr_checksums.py to handle all of that logic, and I put it in dandi-api for now for quicker iteration. It should be moved to a common location soon so that the CLI can use it as well.\n\n\u003e The document says \"The asset metadata will contain the information required to determine if an asset is a normal file blob or a zarr file.\" Exactly what information is that, and is it added by the client or the server?\n\nThat seems ambigious to me at the moment. The server is not adding that metadata right now. @satra any preference on where/how asset metadata is updated?\n\n\u003e For POST /zarr/, what exactly should the \"name\" field in the request body be set to?\n\nI would assume that would be the name of the directory containing the zarr data, unless zarr metadata contains a more descriptive name.\n\n\u003e \".checksum files format\" says \"Each zarr file and directory in the archive has a path and a checksum (md5). For files, this is simply the ETag.\" So is the checksum for a file an md5 or a DANDI e-tag? If the latter, why are we using different digests for files on their own versus in directories?\n\"Upload flow\" states \"The client calculates the MD5 of each file.\" and \"The client sends the paths+MD5s to ...\", but the field name in the request body for POST /zarr/{zarr_id}/upload/ is \"etag\", not \"md5\". Which digest is supposed to be used?\nSimilarly, the docs for POST /api/zarr/{zarr_id}/upload/ say, \"Requires a list of file paths and ETags (md5 checksums)\". E-tags and MD5 checksums are not the same thing.\n\nI would say we are using simple MD5, and that for these purposes MD5 and ETag are the same thing. Files in the zarr archive are limited to 5GB so that they can use the simple upload endpoint, and for files uploaded simply (as opposed to multipart), their ETag is the MD5 of the file. DANDI Etags are defined to be the ETags of multipart uploads, which are MD5s with a suffix relating to the number of parts.\n\n\u003e Unlike POST /uploads/initialize/, POST /zarr/{zarr_id}/upload/ does not return information on different parts of files. Does this mean that each file in a zarr directory is to be uploaded in a single request? What if a file exceeds S3's part size limit of 5 GB?\n\nCorrect. Multipart upload requires substantially more engineering, so the executive decision was made to cap zarr component files at 5GB. Zarr is relatively easy to simply rechunk smaller if that is exceeded.\n\n\u003e The Swagger docs don't describe the request body format for DELETE /api/zarr/{zarr_id}/files/.\n\nSorry, my bad. https://github.com/dandi/dandi-api/issues/648\n\n\u003e How do you download a Zarr?\n\nThat is done directly from S3 using the `zarr` library. You should be able to pass the S3 path of the zarr directory and zarr will read that directly from S3.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1640281589,"metadata":{"github-id":"IC_kwDODBZtRc47oZy1","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1000447157"},"message":"\u003e That seems ambigious to me at the moment. The server is not adding that metadata right now. @satra any preference on where/how asset metadata is updated?\n\nthis should be done as is done currently by the CLI for other types of files. the exact details of encodingformat can be worked out in the relevant CLI PR.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1640282884,"metadata":{"github-id":"IC_kwDODBZtRc47ocJR","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1000456785"},"message":"@jwodder I confirm that I am encountering the same error with your script. I will look closer next week.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1640617974,"metadata":{"github-id":"IC_kwDODBZtRc47s2QS","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1001612306"},"message":"@jwodder I got the script working against `dandi-api-local-docker-tests` by changing the ETag from `get_dandietag(zf).as_str()` to `md5(blob).hexdigest()`. I'm still getting the 403 errors in staging though, so I am still investigating.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1640626594,"metadata":{"github-id":"IC_kwDODBZtRc47tEp_","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1001671295"},"message":"@jwodder I missed a spot with AWS permissions, the staging API has been updated. You need to specify `X-Amz-ACL: bucket-owner-full-control` as a request header for the upload.\n\nMy adaptation of your example looks like this:\n```\nwith DandiAPIClient.for_dandi_instance(\n    \"dandi-staging\", token=os.environ[\"DANDI_API_KEY\"]\n) as client:\n    r = client.post(\"/zarr/\", json={\"name\": zarrdir.name})\n    zarr_id = r[\"zarr_id\"]\n    zfiles = list(map(Path, find_files(r\".*\", str(zarrdir))))\n    upload_body = []\n    for zf in zfiles:\n        with open(zf, \"rb\") as f:\n            blob = f.read()\n        upload_body.append({\n            \"path\": zf.relative_to(zarrdir).as_posix(),\n            \"etag\": md5(blob).hexdigest(), # Simple MD5 instead of dandi-etag\n        })\n    r = client.post(f\"/zarr/{zarr_id}/upload/\", json=upload_body)\n    with RESTFullAPIClient(\"http://nil.nil\") as storage:\n        for upspec in r:\n            with (zarrdir / upspec[\"path\"]).open(\"rb\") as fp:\n                storage.put(upspec[\"upload_url\"], data=fp, json_resp=False, headers={\"X-Amz-ACL\": \"bucket-owner-full-control\"}) # X-Amz-ACL header is required\n    r = client.post(f\"/zarr/{zarr_id}/upload/complete/\")\n```","files":null},{"type":6,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1640626613,"metadata":{"github-id":"UCE_lALODBZtRc47tEp_zh_RjiQ"},"target":"87843caf07ced81966cf644ccbe9d53275fe7a161d59489a3e82ef1b8ed4c4ac","message":"@jwodder I missed a spot with AWS permissions, the staging API has been updated. You need to specify `X-Amz-ACL: bucket-owner-full-control` as a request header for the upload.\n\nMy adaptation of your example looks like this:\n```python\nwith DandiAPIClient.for_dandi_instance(\n    \"dandi-staging\", token=os.environ[\"DANDI_API_KEY\"]\n) as client:\n    r = client.post(\"/zarr/\", json={\"name\": zarrdir.name})\n    zarr_id = r[\"zarr_id\"]\n    zfiles = list(map(Path, find_files(r\".*\", str(zarrdir))))\n    upload_body = []\n    for zf in zfiles:\n        with open(zf, \"rb\") as f:\n            blob = f.read()\n        upload_body.append({\n            \"path\": zf.relative_to(zarrdir).as_posix(),\n            \"etag\": md5(blob).hexdigest(), # Simple MD5 instead of dandi-etag\n        })\n    r = client.post(f\"/zarr/{zarr_id}/upload/\", json=upload_body)\n    with RESTFullAPIClient(\"http://nil.nil\") as storage:\n        for upspec in r:\n            with (zarrdir / upspec[\"path\"]).open(\"rb\") as fp:\n                storage.put(upspec[\"upload_url\"], data=fp, json_resp=False, headers={\"X-Amz-ACL\": \"bucket-owner-full-control\"}) # X-Amz-ACL header is required\n    r = client.post(f\"/zarr/{zarr_id}/upload/complete/\")\n```","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1641219607,"metadata":{"github-id":"IC_kwDODBZtRc472bmc","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1004124572"},"message":"@dchiquito \n\n\u003e The client would need to calculate the checksum to do integrity checking of the zarr upload.\n\nCould you elaborate on how this \"integrity checking\" fits into the upload flow?  Exactly what does the client compare its computed value against?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1641404846,"metadata":{"github-id":"IC_kwDODBZtRc479XGC","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1005941122"},"message":"@dchiquito could you please clarify above question of @jwodder ?  In the [example above](https://github.com/dandi/dandi-cli/issues/852#issuecomment-1001671295) you posted I do not see zarr \"file\" integrity checking anywhere. Is providing it optional???","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1641478073,"metadata":{"github-id":"IC_kwDODBZtRc47_8el","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1006618533"},"message":"[My example](https://github.com/dandi/dandi-cli/issues/852#issuecomment-1001671295) is just [John's snippet](https://github.com/dandi/dandi-cli/issues/852#issuecomment-995950078) with some corrections, I did not include integrity checking in it.\n\nAfter uploading the entire zarr archive, the CLI would query `https://api.dandiarchive.org/api/zarr/{zarr_id}/` and compare the `checksum` value to the checksum calculated against the local files. The file contents were checked as part of the upload process, but this checksum check verifies that all of the files were uploaded into the correct places within the archive.\n\nIf it fails, I'm not sure what the best path toward recovery would be. Recursively traversing the archive and identifying where in the tree the checksums begin to deviate would be challenging and potentially a lot of information to present. Aborting and retrying the upload would be simple.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1641482186,"metadata":{"github-id":"IC_kwDODBZtRc48AJOv","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1006670767"},"message":"the nested directory structure on a filesystem is just one way to store zarr data. and it was adopted to reduce the number of files at a given level by using the `/` separator vs the `.` separator which used to be the default. at the end of the day, zarr groups (i.e. n-dimensional arrays) are still simply a list of files. so each group should have a fixed depth and pattern. \n\na few thoughts.\n1. a new upload: this should simply involve ensuring the number of files and their checksums are appropriate. the checksum appropriateness is now baked into each upload. so the CLI would get an error if an upload fails. \n2. a partial upload(s): this is where the CLI would need to do some investigation to determine which parts of the file need to change and this is where the tree checksum should come in handy. this may involve both additions/deletions/replacements. i don't know what the api has in removing a set of files, but again for an s3 store this should simply be removing a specific set of s3 objects.\n3. just like dandi-etag, we should have a function that computes the checksum of a zarr object. folders on s3 don't exist so there is associated etag. this is being computed by the server side validation process. thus the etag computation in dandischema should be extended to return the checksum of a zarr tree. this could of course use the zarr checksum function or something else.\n4. locally a zarr checksum tree could be stored in a dandi-cli cache, but i would suggest a binary object rather than the mechanism daniel used on s3 to not blow up inodes locally.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1641566580,"metadata":{"github-id":"IC_kwDODBZtRc48DKDC","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007460546"},"message":"@dchiquito Your modified code still isn't working for me; now the PUT fails with a 501 and the response:\n\n```\n\u003cError\u003e\u003cCode\u003eNotImplemented\u003c/Code\u003e\u003cMessage\u003eA header you provided implies functionality that is not implemented\u003c/Message\u003e\u003cHeader\u003eTransfer-Encoding\u003c/Header\u003e\u003cRequestId\u003e31Q78EZ8BQKMTNBK\u003c/RequestId\u003e\u003cHostId\u003eIZBDMAB3lKARg+2grxlKWqjOrv3m6wFHf6X3TdUpicJ+Fidrbh3vwHgFGrPBs65E5BprOM7iFcY=\u003c/HostId\u003e\u003c/Error\u003e\n```","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1641567705,"metadata":{"github-id":"UCE_lALODBZtRc48DKDCzh_9kLQ"},"target":"279b3079b2fcf1885f2a371003f3beb402f984bc321d43a7b96bd3258152377f","message":"@dchiquito Your modified code still isn't working for me; now the PUT fails with a 501 and the response:\n\n```\n\u003cError\u003e\u003cCode\u003eNotImplemented\u003c/Code\u003e\u003cMessage\u003eA header you provided implies functionality that is not implemented\u003c/Message\u003e\u003cHeader\u003eTransfer-Encoding\u003c/Header\u003e\u003cRequestId\u003e31Q78EZ8BQKMTNBK\u003c/RequestId\u003e\u003cHostId\u003eIZBDMAB3lKARg+2grxlKWqjOrv3m6wFHf6X3TdUpicJ+Fidrbh3vwHgFGrPBs65E5BprOM7iFcY=\u003c/HostId\u003e\u003c/Error\u003e\n```\n\nEDIT: The problem goes away if I delete an empty file I added to the ZARR dir for testing purposes.","files":null},{"type":5,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1641572849,"metadata":{"github-id":"LE_lADODBZtRc5Ad-zQzwAAAAFdNb-6"},"added":["priority-high"],"removed":[]},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1641573471,"metadata":{"github-id":"IC_kwDODBZtRc48Dgj1","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007552757"},"message":"@dchiquito Observations after being able to upload a Zarr to staging:\n\n* I see that the asset structures returned by `/dandisets/{versions__dandiset__pk}/versions/{versions__version}/assets/` now include \"blob\" and \"zarr\" keys.  Are these what should be used to determine whether an asset is a Zarr?  If so, we would have a problem when accessing individual assets by ID, as the individual asset endpoints only return asset metadata, which does not include this information.  @satra [seems to imply](https://github.com/dandi/dandi-cli/issues/852#issuecomment-1000447157) that zarrness should be indicated in the metadata by setting the `encodingFormat` metadata field to some as-yet-undetermined value; I wouldn't trust this unless the server validates correctness of this field. \n    * Is the new \"blob\" key ever of any use/relevance to the client?\n* The server-generated metadata for a Zarr asset includes a digest with a type of \"dandi:dandi-zarr-checksum\", yet such a digest type does not seem to be recognized by dandischema; CC @satra.\n* The non-API `contentURL` for a Zarr asset is of the form `\"zarr/9b107765-7011-4027-bead-81b5bf7aa028/\"`; how would one use this to download a Zarr?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1641576269,"metadata":{"github-id":"IC_kwDODBZtRc48DpqX","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007590039"},"message":"@jwodder - can you please share a staging api url for a zarr asset so i can see the current metadata? indeed that should be harmonized both with the schema \n\n`dandi:dandi-zarr-checksum` for this to show up, it would need to be added to dandischema, and as with dandi-etag, i would say the implementation for generating this checksum should also be there. can someone send a PR with this? \n\nregarding the encoding format filed an issue here: https://github.com/zarr-developers/zarr-specs/issues/123","files":null},{"type":6,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1641576298,"metadata":{"github-id":"UCE_lALODBZtRc48DpqXzh_-be4"},"target":"72354317121a72a2ab3067f22db6b1aa1fadb9d6f7649312dbe87cf5b17195e6","message":"@jwodder - can you please share a staging api url for a zarr asset so i can see the current metadata? indeed that should be harmonized with the schema across all assets.\n\n`dandi:dandi-zarr-checksum` for this to show up, it would need to be added to dandischema, and as with dandi-etag, i would say the implementation for generating this checksum should also be there. can someone send a PR with this? \n\nregarding the encoding format filed an issue here: https://github.com/zarr-developers/zarr-specs/issues/123","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1641576751,"metadata":{"github-id":"IC_kwDODBZtRc48Dq8e","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007595294"},"message":"@satra \n\n```\n$ curl -fsSL https://api-staging.dandiarchive.org/api/dandisets/100956/versions/draft/assets/2126d0e6-7733-44ac-9ab0-f9ac197c0507/ | jq .\n{\n  \"id\": \"dandiasset:2126d0e6-7733-44ac-9ab0-f9ac197c0507\",\n  \"path\": \"example.zarr\",\n  \"digest\": {\n    \"dandi:dandi-zarr-checksum\": \"bf5da3a8c757e736ac99f9d0d86106f4\"\n  },\n  \"contentUrl\": [\n    \"https://api.dandiarchive.org/api/assets/2126d0e6-7733-44ac-9ab0-f9ac197c0507/download/\",\n    \"zarr/9b107765-7011-4027-bead-81b5bf7aa028/\"\n  ],\n  \"identifier\": \"2126d0e6-7733-44ac-9ab0-f9ac197c0507\",\n  \"contentSize\": 862\n}\n```","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1641577340,"metadata":{"github-id":"IC_kwDODBZtRc48Dsgy","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007601714"},"message":"thank you @jwodder - indeed under `contentUrl` that prefix should be a full s3 path. let's use `encodingFormat: \"application/x-zarr\"` for now and we can adjust if the zarr folks say otherwise.\n\n@dchiquito - any idea while dandiset size is 0 for that dandiset ? are zarr assets not being included in the asset count ?\n\nfiled an api issue here: https://github.com/dandi/dandi-api/issues/674","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1641577882,"metadata":{"github-id":"IC_kwDODBZtRc48Dt7y","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007607538"},"message":"@satra So, for Zarr assets, currently the metadata set by the client should be the same as for a generic non-NWB file, but with an `encodingFormat` of `\"application/x-zarr\"`, correct?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1641580252,"metadata":{"github-id":"IC_kwDODBZtRc48D1el","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007638437"},"message":"\u003e currently the metadata set by the client should be the same as for a generic non-NWB file\n\nthat is correct at present. \n\nhowever, since the zarr file is also a bids file, we should also extract the bids metadata from the filename. pinging @thechymera as this is related to the bids validation work and should be added to dandi cli for augmenting metadata extraction for bids datasets/assets.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1641580817,"metadata":{"github-id":"IC_kwDODBZtRc48D3Az","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007644723"},"message":"@dchiquito \n\n* Can you provide code for downloading a Zarr from Dandiarchive (assuming the lack of a proper S3 URL in `contentUrl` is fixed)?\n* I note that trying to access the API `/download/` URL listed in a Zarr asset's `contentUrl` currently returns a 404.  Should that URL even be present in the metadata?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1641581586,"metadata":{"github-id":"IC_kwDODBZtRc48D5Ds","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007653100"},"message":"@jwodder - downloading may involve opening with the zarr library and then saving locally into the filename with a nested directory storage backend. the alternative would involve listing every object in the store with the uuid prefix, which i think would require an additional api endpoint to return paginated lists of all the objects.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1641591475,"metadata":{"github-id":"IC_kwDODBZtRc48ESx9","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1007758461"},"message":"\u003e I see that the asset structures returned by /dandisets/{versions__dandiset__pk}/versions/{versions__version}/assets/ now include \"blob\" and \"zarr\" keys. Are these what should be used to determine whether an asset is a Zarr? If so, we would have a problem when accessing individual assets by ID, as the individual asset endpoints only return asset metadata, which does not include this information. @satra seems to imply that zarrness should be indicated in the metadata by setting the encodingFormat metadata field to some as-yet-undetermined value; I wouldn't trust this unless the server validates correctness of this field.\n\nMy plan was for those properties would be available to determine zarr-ness.\nThe API currently isn't enforcing an encodingType appropriate for zarr files, but it should: https://github.com/dandi/dandi-api/issues/676\n\u003e Is the new \"blob\" key ever of any use/relevance to the client?\n\nI don't think so.\n\n\u003e The non-API contentURL for a Zarr asset is of the form \"zarr/9b107765-7011-4027-bead-81b5bf7aa028/\"; how would one use this to download a Zarr?\n\nComing soon https://github.com/dandi/dandi-api/issues/677\n\n\u003e dandi:dandi-zarr-checksum for this to show up, it would need to be added to dandischema, and as with dandi-etag, i would say the implementation for generating this checksum should also be there. can someone send a PR with this?\n\nI added the new digest type in https://github.com/dandi/dandischema/pull/108. I will move the checksum calculation code next week.\n\n\u003e Can you provide code for downloading a Zarr from Dandiarchive (assuming the lack of a proper S3 URL in contentUrl is fixed)?\n\nI will do this ASAP next week. It will involve the `zarr` python client.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1641841918,"metadata":{"github-id":"IC_kwDODBZtRc48KAzp","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1009257705"},"message":"@dchiquito \n\n\u003e the executive decision was made to cap zarr component files at 5GB\n\nSo is the client supposed to do file-size checking?  Zarr component file sizes don't seem to be reported to the API prior to actually uploading.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1641843712,"metadata":{"github-id":"IC_kwDODBZtRc48KGW4","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1009280440"},"message":"Any S3 request to upload a file \u003e5GB will fail, but I'm not sure of when or what the response code would be. If you want a better error message you could do file-size checking preemptively.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1642005514,"metadata":{"github-id":"IC_kwDODBZtRc48RkRi","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1011237986"},"message":"\u003e Can you provide code for downloading a Zarr from Dandiarchive (assuming the lack of a proper S3 URL in contentUrl is fixed)?\n\n@jwodder I cannot for the life of me figure out how to properly save a zarr group. This is what I have so far:\n```\nimport zarr\n\nZARR_ID = '79bb04a0-3f1b-46ae-8821-1071fc69ed6e'\nDOWNLOAD_PATH = '/home/daniel/git/dandi-api/download.zarr'\n\n\ndef download_array(path: str, array: zarr.Array):\n    zarr.save(f'{DOWNLOAD_PATH}{array.name}', array)\n\n\ndef download_group(path: str, group: zarr.Group):\n    for k in group.array_keys():\n        download_array(path, group[k])\n    for k in group.group_keys():\n        download_group(path, group[k])\n    # TODO how to save the .zgroup information?\n\n\nif __name__ == '__main__':\n    z = zarr.open(f'https://api-staging.dandiarchive.org/api/zarr/{ZARR_ID}.zarr/')\n    download_group(DOWNLOAD_PATH, z)\n    print(z.tree())\n\n```\nIt will recursively save all of the arrays in the group, which is all of the actual data, but the `.zgroup` data is lost so zarr can't load the information again.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642013273,"metadata":{"github-id":"IC_kwDODBZtRc48R_Ud","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1011348765"},"message":"\u003e @jwodder - downloading may involve opening with the zarr library and then saving locally into the filename with a nested directory storage backend. the alternative would involve listing every object in the store with the uuid prefix, which i think would require an additional api endpoint to return paginated lists of all the objects.\n\nSounds like using `zarr` library is indeed the easiest way forward, but my gut \"resists\" a little, since it is quite a heavy use of a library to just download content from the archive. Some concerns/questions I have\n\n### z.save\n\nI really hope that we would find some `z.save` or alike and avoid crafting ad-hoc code since then every user interfacing with API directly would need to redo that.\n\n### is it byte-to-byte identical?\n\nis there a guarantee that such `zarr.save` would produce byte-to-byte identical copy? or there could be some variability in filenaming, or having some timestamps embedded in data, or data files getting recompressed.  Would behavior be identical across zarr library version changes?\nIf any effects from above --  our checksum validation would fail, we loose integrity validation\n\n### we might need similar traversal of zarr tree during upload:\n\nPlease correct me if we are \"all ok\" in a following hypothetical use case:\n- during upload local .zarr folder has some extra data files\n- it validates by zarr just fine since it just ignores extra files\n- dandi-cli uploads all the files, and provides checksum which is validated by dandi-api to be ok\n- another user downloads that .zarr via `zarr.save` which doesn't bother about downloading extra files, checksum would mismatch since locally we would not have those extra files\n\nSo, altogether I feel that we might step into a shady territory, which we can at large avoid if we simply extend API to return a list of \"subpaths\" for zarr to be downloaded just as regular files, and then not worry about anything from above.  WDYT?","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642014955,"metadata":{"github-id":"UCE_lALODBZtRc48R_UdziAZBtk"},"target":"4b44b06f5f7c65de5714fbfb8f4f0e78ebe4888d99c9babf36e61802c7de98ef","message":"\u003e @jwodder - downloading may involve opening with the zarr library and then saving locally into the filename with a nested directory storage backend. the alternative would involve listing every object in the store with the uuid prefix, which i think would require an additional api endpoint to return paginated lists of all the objects.\n\nSounds like using `zarr` library is indeed the easiest way forward, but my gut \"resists\" a little, since it is quite a heavy use of a library to just download content from the archive. Some concerns/questions I have\n\n### z.save\n\nI really hope that we would find some `z.save` or alike and avoid crafting ad-hoc code since then every user interfacing with API directly would need to redo that.\n\n### is it byte-to-byte identical?\n\nis there a guarantee that such `zarr.save` would produce byte-to-byte identical copy? or there could be some variability in filenaming, or having some timestamps embedded in data, or data files getting recompressed.  Would behavior be identical across zarr library version changes?\nIf any effects from above --  our checksum validation would fail, we loose integrity validation\n\n### we might need similar traversal of zarr tree during upload:\n\nPlease correct me if we are \"all ok\" in a following hypothetical use case:\n- during upload local .zarr folder has some extra data files\n- it validates by zarr just fine since it just ignores extra files\n- dandi-cli uploads all the files, and provides checksum which is validated by dandi-api to be ok\n- another user downloads that .zarr via `zarr.save` which doesn't bother about downloading extra files, checksum would mismatch since locally we would not have those extra files\n\nSo, altogether I feel that we might step into a shady territory, which we can at large avoid if we simply extend API to return a list of \"subpaths\" for zarr to be downloaded just as regular files, and then not worry about anything from above.  WDYT?\n\nedit 1: extra aspects which keep coming up\n- **download progress indication**: how would we deduce/report overall download progress?  It would be quite a bad UX if we would not report it. The only way I would see is to have some separate thread which would repeatedly rescan output folder for the total size (would grow more and more expensive as more files are downloaded; or tricky to implement requiring smth like inotify) and report % from total size client obtains from the API. Taking care about explicit download of individual files would make such reporting trivial.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642014314,"metadata":{"github-id":"IC_kwDODBZtRc48SCij","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1011361955"},"message":"@yarikoptic \n\n\u003e So, altogether I feel that we might step into a shady territory, which we can at large avoid if we simply extend API to return a list of \"subpaths\" for zarr to be downloaded just as regular files, and then not worry about anything from above. WDYT?\n\nSounds like a good idea to me.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642015181,"metadata":{"github-id":"IC_kwDODBZtRc48SFVW","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1011373398"},"message":"ok, re \"listing\" -- added extra item to @satra 's https://github.com/dandi/dandi-api/issues/674 but I guess could be an issue on its own.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642016816,"metadata":{"github-id":"IC_kwDODBZtRc48SKVe","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1011393886"},"message":"\u003e @yarikoptic\n\u003e \n\u003e \u003e So, altogether I feel that we might step into a shady territory, which we can at large avoid if we simply extend API to return a list of \"subpaths\" for zarr to be downloaded just as regular files, and then not worry about anything from above. WDYT?\n\u003e \n\u003e Sounds like a good idea to me.\n\napparently already there! see https://github.com/dandi/dandi-api/issues/674#issuecomment-1011383921 and I checked it in staging swagger -- seems to work nicely, and since we would know all needed paths/urls to download -- we could parallelize it across this swarm of files and otherwise it is just a matter of downloading from urls (`Last-Modified` from url would be good to set on the file to decide if to redownload)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642017604,"metadata":{"github-id":"IC_kwDODBZtRc48SMjc","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1011402972"},"message":"@yarikoptic So when the client is asked to download a Zarr, should it, instead of downloading a monolithic asset, effectively treat the Zarr as a tree of independent blobs, each one of which is separately compared against whatever's on-disk at the final location?  What exactly should pyout display when downloading a multifile Zarr?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642018822,"metadata":{"github-id":"IC_kwDODBZtRc48SQTT","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1011418323"},"message":"\u003e @yarikoptic So when the client is asked to download a Zarr, should it, instead of downloading a monolithic asset, effectively treat the Zarr as a tree of independent blobs, each one of which is separately compared against whatever's on-disk at the final location?\n\ncorrect\n\n\u003e What exactly should pyout display when downloading a multifile Zarr?\n\nI think for user reporting (pyout) we could consider zarr file (well -- directory) to be a single asset/file.  So % progress would be based on \"total\" for that directory. Eventually we might want to return/expose some progress in # of files within zarr but I think there is no immediate need for that.","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642018923,"metadata":{"github-id":"UCE_lALODBZtRc48SQTTziAZdFM"},"target":"68cc35aedc8162141440843b232961d3756b0d4f41a2208f61c6789362875616","message":"\u003e @yarikoptic So when the client is asked to download a Zarr, should it, instead of downloading a monolithic asset, effectively treat the Zarr as a tree of independent blobs, each one of which is separately compared against whatever's on-disk at the final location?\n\ncorrect\n\nedit: with an overall checksum comparison across the tree... if possible to make it \"on the fly\" as we do with digest for an individual file, but traversing the same order as desired for computation of the zarr checksum, would be awesome!\n\n\u003e What exactly should pyout display when downloading a multifile Zarr?\n\nI think for user reporting (pyout) we could consider zarr file (well -- directory) to be a single asset/file.  So % progress would be based on \"total\" for that directory. Eventually we might want to return/expose some progress in # of files within zarr but I think there is no immediate need for that.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642091056,"metadata":{"github-id":"IC_kwDODBZtRc48VmcS","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1012295442"},"message":"@yarikoptic @dchiquito What should the Zarr upload do if a Zarr contains an empty directory?  There doesn't seem to be a way to inform the API of such a directory's existence, yet the directory is still used in calculating the checksum, and so there will be a checksum mismatch after uploading.","files":null},{"type":3,"author":{"id":"818a29386a430d42cbb57ad6dcccbe0fb486ffc3"},"timestamp":1642091913,"metadata":{"github-id":"IC_kwDODBZtRc48Vpmz","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1012308403"},"message":"I don't think well-formed zarr files _can_ contain empty directories. I would assume it's not a thing, but perhaps we should appeal to someone with more knowledge than I.\n\nS3 has no concept of \"directories\", so they can't be empty. My vote is that the checksum calculator should just ignore empty directories and the empty directory will not be considered a part of the zarr.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642100838,"metadata":{"github-id":"IC_kwDODBZtRc48WGcG","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1012426502"},"message":"@yarikoptic In order to show only a single entry in pyout when downloading a Zarr, I think the code would need to call `_download_file()` in a separate thread for each file in the Zarr and then interleave \u0026 combine the results.  Assuming I can figure out a good way to do that, the biggest problem I can think of would be how we would limit the maximum number of concurrent download threads, which is currently managed by pyout.  Unless there's some way to get pyout to do the entry-combination for us?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642117559,"metadata":{"github-id":"IC_kwDODBZtRc48W1r4","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1012620024"},"message":"Can `_download_file()` in pyout'ed thread, when handling a Zarr just start its own pool of threads for `_download_file()` ing individual files, and then report back to pyout an aggregate progress?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642117786,"metadata":{"github-id":"IC_kwDODBZtRc48W2Kn","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1012621991"},"message":"Attn @satra on above https://github.com/dandi/dandi-cli/issues/852#issuecomment-1012308403 question about empty directories. Smells like we should ignore them and exclude from checksum compute","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1642118039,"metadata":{"github-id":"IC_kwDODBZtRc48W2mY","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1012623768"},"message":"\u003e Smells like we should ignore them and exclude from checksum compute\ni would agree with that. \n\nthe only thing is that we should perhaps test what an empty array (https://zarr.readthedocs.io/en/stable/api/creation.html#zarr.creation.empty) looks like in a nested directory store. and whether zarr's checksum of a zarr dataset changes or not.","files":null},{"type":6,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1642118048,"metadata":{"github-id":"UCE_lALODBZtRc48W2mYziAieNk"},"target":"9b11672e727447f4abc4a5e4eb6c7a3cd85561fe1d0c82d6688a6c00b9051253","message":"\u003e Smells like we should ignore them and exclude from checksum compute\n\ni would agree with that. \n\nthe only thing is that we should perhaps test what an empty array (https://zarr.readthedocs.io/en/stable/api/creation.html#zarr.creation.empty) looks like in a nested directory store. and whether zarr's checksum of a zarr dataset changes or not.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642166926,"metadata":{"github-id":"IC_kwDODBZtRc48Yu5R","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1013116497"},"message":"@yarikoptic \n\n\u003e Can `_download_file()` in pyout'ed thread, when handling a Zarr just start its own pool of threads for `_download_file()` ing individual files, and then report back to pyout an aggregate progress?\n\nThat's the basic plan, but the problem is determining the size of the thread pool.  Normally, on a quad-core computer, pyout only runs 8 threads at once, so there are only at most 8 concurrent downloads.  If we set the size of the Zarr thread pools to some *n*, we could end up with 8*n* concurrent downloads  and if we try to avoid that by only downloading one file in a Zarr at a time, we may end up taking longer than normal to download everything if there's ever a point where fewer than 8 downloads are running in total.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1642167394,"metadata":{"github-id":"IC_kwDODBZtRc48Ywvn","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1013124071"},"message":"@jwodder - for downloads and uploads isn't the bottleneck in disk and network bandwidth much more than number of cores? could something be done to auto adjust the number of processes ?\n\nalso when dealing with lee's data i found pyout too constraining by default, especially because the number of jobs it runs is limited by the display of the table. it would be nice if there was an option to not care about a tabular display but just a summary display. it does seem that decoupling download processes from display constraints would be a good step, and may provide a greater ability to optimize it.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642167641,"metadata":{"github-id":"IC_kwDODBZtRc48Yxeu","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1013127086"},"message":"@satra It is indeed possible to configure the maximum number of workers pyout uses when instantiating a table.  How exactly do you recommend we determine this value?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642168218,"metadata":{"github-id":"IC_kwDODBZtRc48YzLd","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1013134045"},"message":"@satra Also, regarding the number of jobs in pyout being limited by the display of the table: It seems that, by default, [pyout waits for the top three jobs to complete before adding a new row that would cause the topmost to go off-screen](https://github.com/pyout/pyout/blob/8007f33b553a897e2fcf06fa8b58279d8213f411/pyout/tabular.py#L113-L116), though this can be configured or disabled.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1642169114,"metadata":{"github-id":"IC_kwDODBZtRc48Y19x","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1013145457"},"message":"@jwodder - i don't a good answer on how to optimize, but perhaps a test download would tell you how many threads optimizes bandwidth given disk constraints (keep increasing the number of threads/processes till it saturates). on clusters where disks may be on different interfaces, they could even be competing with download bandwidth. it may be worthwhile looking up if someone has implemented an algorithm for this. \n\nregarding pyout, the issue i ran into was when one of the rows near the top was uploading (say a large file) and everything else was complete, it would not add any more jobs. if the bandwidth was being fully utilized i don't care too much, but in some cases this result in less efficient upload. the other use case is when a lot of small files are being uploaded then the job limit was 10 and could not be increased even though bandwidth wasn't an issue and devices weren't being saturated. hence the notion of removing the tabular display altogether and simply providing the summary display.\n\nbetween optimizing number of threads and removing tabular display, i would prioritize the latter first so that the number of uploads/downloads do not have to be limited to 10.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642170547,"metadata":{"github-id":"IC_kwDODBZtRc48Y7jU","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1013168340"},"message":"re jobs -- I think we should adopt what we have for `upload` already:\n```\n  -J, --jobs N[:M]                Number of files to upload in parallel and,\n                                  optionally, number of upload threads per\n                                  file\n```\nso user could explicitly control on how many threads per file (if possible, like in case of zarr) it would be.  Default to e.g. 4 and be \"done\".\n\nIn the longer run, we are still to RF all the interfacing and pyout should not take care then about parallelizing and only for reporting/UI.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642170868,"metadata":{"github-id":"IC_kwDODBZtRc48Y9lA","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1013176640"},"message":"@yarikoptic So you're suggesting adding a `--jobs` option to `dandi download` for separately setting the number of \"main\" download threads and the number of threads per Zarr asset, then?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642190547,"metadata":{"github-id":"IC_kwDODBZtRc48Z9La","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1013437146"},"message":"yes, that is what I am thinking about. In principle we could even thread downloads of regular files via multiple range requests but I do not see much need in that, so we could just comment that `[:M]` is in effect only for assets containing multiple files (such as .zarr directories)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642445249,"metadata":{"github-id":"IC_kwDODBZtRc48fMpp","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1014811241"},"message":"@yarikoptic I've written [a library for running multiple iterators in threads and collecting the results](https://github.com/jwodder/interleave).  Now comes the bikeshedding part: deciding what status dicts should be emitted for a combined set of status dicts.\n\nA refresher: The columns shown in the download pyout table are \"path\", \"size\", \"done\", \"done%\", \"checksum\", \"status\", and \"message\".  The status dicts yielded for a given path while it's downloading take the following forms, yielded roughly in the order shown:\n\n* `{\"status\": \"skipped\", \"message\": \"\u003cMESSAGE\u003e\"}`\n* `{\"size\": \u003cint\u003e}`\n* `{\"status\": \"downloading\"}`\n* `{\"done\": \u003cbytes downloaded\u003e, \"done%\": \u003cpercentage downloaded, from 0 to 100\u003e}`\n* `{\"status\": \"error\", \"message\": \"\u003cMESSAGE\u003e\"}`\n* `{\"checksum\": \"differs\", \"status\": \"error\", \"message\": \"\u003cMESSAGE\u003e\"}`\n* `{\"checksum\": \"ok\"}`\n* `{\"status\": \"setting mtime\"}`\n* `{\"status\": \"done\"}`\n\nHence the following questions \u0026 thoughts:\n\n* Should the \"message\" for a combined download always be in the form \"X downloading, Y errored, Z skipped\"?  If so, what should happen to the error messages for individual files?\n* When at least one file is downloading and at least one file has errored, should the combined status be \"downloading\" or \"error\"?  Likewise when skipped files are added to the mix.\n* When a download errors, should its contribution to the combined download progress be removed, left where it was, or treated like 100%?\n* `{\"status\": \"setting mtime\"}` status dict should probably just be discarded on input and not reported in the combined output.\n* It'll be simplest if the combined \"size\" and \"done%\" only reflect files seen so far rather than reporting the total Zarr size from the start.\n* Should the combined \"checksum\" field be \"ok\" as long as at least one downloaded file has its checksum verified, or should that only be shown once all checksums are verified?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642619441,"metadata":{"github-id":"IC_kwDODBZtRc48mt6k","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1016782500"},"message":"\u003e * Should the \"message\" for a combined download always be in the form \"X downloading, Y errored, Z skipped\"?  If so, what should happen to the error messages for individual files?\n\nLet's make it \"X done, Y errored, Z skipped\" with any of those omitted whenever it is just 0.\n\n\u003e * When at least one file is downloading and at least one file has errored, should the combined status be \"downloading\" or \"error\"?  Likewise when skipped files are added to the mix.\n\nI would keep it \"downloading\" until we are done with all files, and then if some failed, yield \"error\" instead of \"done\".\n(note: if any errorred, no need to do checksum I guess)\n\n\u003e * When a download errors, should its contribution to the combined download progress be removed, left where it was, or treated like 100%?\n\nmy vote would be for \"removed\" so user would get adequate \"done%\" as for how much of zarr was downloaded.\n\n\u003e * `{\"status\": \"setting mtime\"}` status dict should probably just be discarded on input and not reported in the combined output.\n\nwe will be setting it for every file, right? then indeed now worth yielding, unless may be for the .zarr/ directory at the end of the process.\n\n\u003e * It'll be simplest if the combined \"size\" and \"done%\" only reflect files seen so far rather than reporting the total Zarr size from the start.\n\n\"size\" should report total zarr (as reported by the server); \"done\" and \"done%\" - seen so far.\n\n\u003e * Should the combined \"checksum\" field be \"ok\" as long as at least one downloaded file has its checksum verified, or should that only be shown once all checksums are verified?\n\nI think checksum should be reported once for the entire zarr at the end of the download process.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642620538,"metadata":{"github-id":"IC_kwDODBZtRc48mxmM","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1016797580"},"message":"@yarikoptic I don't think the code should calculate a checksum for the whole Zarr after downloading; digests are already checked for the individual files, and that should be enough.  Moreover, checksumming a Zarr would fail if downloading into a pre-existing local Zarr containing files that don't exist in the Zarr being downloaded.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642620976,"metadata":{"github-id":"IC_kwDODBZtRc48my7d","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1016803037"},"message":"\u003e Moreover, checksumming a Zarr would fail if downloading into a pre-existing local Zarr containing files that don't exist in the Zarr being downloaded.\n\nI think it is actually important enough to guarantee consistency of zarr as downloaded to match what is on the server to avoid confusion and e.g. reupload of a zarr with some garbage in it.  So, as the last step in the zarr download, it would be good to delete files which are not listed/present in the remote (on server) copy.  And then checksum should match.  I guess we can rely on fscacher to cache individual files checksumming to make it reasonably fast.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642684724,"metadata":{"github-id":"IC_kwDODBZtRc48pcUG","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1017496838"},"message":"@yarikoptic Hashing of downloaded files is performed piecemeal as they're being downloaded without using fscacher, so checksumming a downloaded Zarr will have to start from scratch.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642686055,"metadata":{"github-id":"IC_kwDODBZtRc48pgzf","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1017515231"},"message":"\u003e @yarikoptic Hashing of downloaded files is performed piecemeal as they're being downloaded without using fscacher, so checksumming a downloaded Zarr will have to start from scratch.\n\nso for downloaded in current session we would have md5s from \"on the fly\" digestion right? for those others found in the folder we would need to redigest them from disk.  And that is where I thought fscacher would kick in.  Or where was I wrong?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642686383,"metadata":{"github-id":"IC_kwDODBZtRc48ph4M","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1017519628"},"message":"@yarikoptic\n\n\u003e so for downloaded in current session we would have md5s from \"on the fly\" digestion right?\n\nThe digests aren't returned from the download manager; they're just discarded after they're checked.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1642689884,"metadata":{"github-id":"IC_kwDODBZtRc48pxAC","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1017581570"},"message":"\u003e The digests aren't returned from the download manager; they're just discarded after they're checked.\n\nthat's unfortunate.  Didn't check the code, but may be you see a reasonably easy way to still make it possible to get their digests for overall zarr digest computation \"on the fly\"?  if not -- we will be doomed to re-digest the entire zarr tree in full upon completion.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1642692238,"metadata":{"github-id":"IC_kwDODBZtRc48p6RU","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1017619540"},"message":"@yarikoptic I figured out a way.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1642771459,"metadata":{"github-id":"IC_kwDODBZtRc48tRzN","github-url":"https://github.com/dandi/dandi-cli/issues/852#issuecomment-1018502349"},"message":"i will copy the directory over and retry, but perhaps symbolic links should be supported.\n\n```\n$ dandi upload -i dandi-staging sub-MITU01_run-1_sample-178_stain-LEC_chunk-1_spim.ngff\n2022-01-21 08:20:05,787 [ WARNING] A newer version (0.34.1) of dandi/dandi-cli is available. You are using 0.34.0+50.gc12f72b\n2022-01-21 08:20:33,114 [ WARNING] /mnt/beegfs/satra/zarrhdf/101233/sub-MITU01/ses-20210521h17m17s06/microscopy/sub-MITU01_run-1_sample-178_stain-LEC_chunk-1_spim.ngff: Ignoring unsupported symbolic link to directory\n2022-01-21 08:20:33,114 [    INFO] Found 0 files to consider\n2022-01-21 08:20:33,126 [    INFO] Logs saved in /home/satra/.cache/dandi-cli/log/20220121132004Z-1470910.log\n```","files":null},{"type":4,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1643136543,"metadata":{"github-id":"CE_lADODBZtRc5Ad-zQzwAAAAFixcTF"},"status":2},{"type":5,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1643146995,"metadata":{"github-id":"LE_lADODBZtRc5Ad-zQzwAAAAFi061L"},"added":["zarr"],"removed":[]}]}