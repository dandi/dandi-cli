{"version":2,"ops":[{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1657208089,"metadata":{"github-id":"IC_kwDODBZtRc5GM97n","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1177804519"},"message":"@yarikoptic I'm thinking something like this:\n\n* Add a `DatasetDescriptionAsset(LocalFileAsset)` class for representing `dataset_description.json` files\n    * Any BIDS dataset-wide operations like metadata generation and validation are performed as methods of this class\n* Add a `BIDSFileAsset(LocalFileAsset)` class for representing any other file underneath a directory with a `dataset_description.json`; instances will have an attribute storing the corresponding `DatasetDescriptionAsset`\n* Modify `find_dandi_files()` to check whether each directory contains a `dataset_description.json` and, if its does, produce the appropriate asset classes for the files within\n* Give `dandi_file` a `dataset_description: Optional[DatasetDescriptionAsset] = None` argument for use when the file is known to reside within a BIDS dataset; if this argument is `None`, it is assumed to not be in one\n\nNote that this assumes that, if a Dandiset contains one or more BIDS datasets that aren't at the root, then we want any non-NWB, non-Zarr files outside of the BIDS datasets to not be uploaded as assets by default.","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1657208157,"metadata":{"github-id":"UCE_lALODBZtRc5GM97nziTpH-Q"},"target":"4dafb421d4ec287adb37e2899b9e971afa396be8e8ecda352de6dad12f3bf07d","message":"@yarikoptic I'm thinking something like this:\n\n* Add a `DatasetDescriptionAsset(LocalFileAsset)` class for representing `dataset_description.json` files\n    * Any BIDS dataset-wide operations like metadata generation and validation are performed as methods of this class\n* Add a `BIDSFileAsset(LocalFileAsset)` class for representing any other file underneath a directory with a `dataset_description.json`; instances will have an attribute storing the corresponding `DatasetDescriptionAsset`\n* Add a `NWBBidsAsset(NWBAsset, BIDSFileAsset)` class for representing NWBs in BIDS datasets, with any necessary custom behavior\n* Modify `find_dandi_files()` to check whether each directory contains a `dataset_description.json` and, if its does, produce the appropriate asset classes for the files within\n* Give `dandi_file` a `dataset_description: Optional[DatasetDescriptionAsset] = None` argument for use when the file is known to reside within a BIDS dataset; if this argument is `None`, it is assumed to not be in one\n\nNote that this assumes that, if a Dandiset contains one or more BIDS datasets that aren't at the root, then we want any non-NWB, non-Zarr files outside of the BIDS datasets to not be uploaded as assets by default.","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1657208171,"metadata":{"github-id":"UCE_lALODBZtRc5GM97nziTpIJo"},"target":"4dafb421d4ec287adb37e2899b9e971afa396be8e8ecda352de6dad12f3bf07d","message":"@yarikoptic I'm thinking something like this:\n\n* Add a `DatasetDescriptionAsset(LocalFileAsset)` class for representing `dataset_description.json` files\n    * Any BIDS dataset-wide operations like metadata generation and validation are performed as methods of this class\n* Add a `BIDSFileAsset(LocalFileAsset)` class for representing any other file underneath a directory with a `dataset_description.json`; instances will have an attribute storing the corresponding `DatasetDescriptionAsset`\n* Add a `NWBBidsAsset(NWBAsset, BIDSFileAsset)` class for representing NWBs in BIDS datasets, with any necessary custom behavior\n* Modify `find_dandi_files()` to check whether each directory contains a `dataset_description.json` and, if it does, produce the appropriate asset classes for the files within\n* Give `dandi_file` a `dataset_description: Optional[DatasetDescriptionAsset] = None` argument for use when the file is known to reside within a BIDS dataset; if this argument is `None`, it is assumed to not be in one\n\nNote that this assumes that, if a Dandiset contains one or more BIDS datasets that aren't at the root, then we want any non-NWB, non-Zarr files outside of the BIDS datasets to not be uploaded as assets by default.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1657639915,"metadata":{"github-id":"IC_kwDODBZtRc5Gcpj7","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1181915387"},"message":"Sounds good for a start!  I dislike a little implicit `DatasetDescriptionAsset` implying having a BIDS dataset. I guess we can start with that since I don't have a better idea ATM. Let's just make it more explicit `BIDSDatasetDescriptionAsset` and `bids_dataset_description`.\n \nMy main concern is the combinatoric `NWBBidsAsset(NWBAsset, BIDSFileAsset)` since we might then need to also do it for any other asset type such as `VideoAsset` and get `VideoBidsAsset`. I guess we can start doing this until we run into some real explosion would require instead establishing some more *flexible* construct. But what would you say about having `CompositeBIDSFileAsset(BIDSFileAsset)` which would be just parametrized at instantiation with an instance of underlying `NWBAsset` (or `VideoAsset`) which could be asked for its metadata/validation. This way it would just delegate to some \"underlying\" data type consistently across all such combinations without requiring multiple inheritance. WDYT?\n\nNB I would prefer consistent capitalization of `BIDS` so to have `NWBBIDSAsset` even though parsing between two acronyms becomes impossible.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1657640484,"metadata":{"github-id":"IC_kwDODBZtRc5GcsXD","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1181926851"},"message":"@yarikoptic I'd prefer multiple inheritance to delegating to an attribute.  For one thing, given an instance of an NWB in a BIDS dataset, I would expect `isinstance(nwb, NWBAsset)` and `isinstance(nwb, BIDSFileAsset)` to both be true.\n\n\u003e Let's just make it more explicit ... `bids_dataset_description`.\n\nAre you suggesting changing the name of the file that denotes a BIDS dataset?  I don't think that's up to us.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1657645313,"metadata":{"github-id":"IC_kwDODBZtRc5GdClj","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1182017891"},"message":"\u003e \u003e Let's just make it more explicit ... `bids_dataset_description`.\n\u003e \n\u003e Are you suggesting changing the name of the file that denotes a BIDS dataset? I don't think that's up to us.\n\nno, I am just suggesting adding BIDS to our class/option names as e.g. in `dataset_description: Optional[DatasetDescriptionAsset] = None` argument you suggested.\n\n\u003e @yarikoptic I'd prefer multiple inheritance to delegating to an attribute. For one thing, given an instance of an NWB in a BIDS dataset, I would expect `isinstance(nwb, NWBAsset)` and `isinstance(nwb, BIDSFileAsset)` to both be true.\n\nhm... ok... but then let's have still have the logic of behavior centralized, may be it should be a mixin `NWBBidsAsset(NWBAsset, BIDSFileAsset, CompositeBIDSFileAssetMixIn)` where it would be the `CompositeBIDSFileAssetMixIn` centralizing that logic of preferring BIDSFileAsset extracted metadata and doing the validation for consistency?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1657720058,"metadata":{"github-id":"IC_kwDODBZtRc5Ghu2t","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1183247789"},"message":"@jwodder will work on RFing code per above, @TheChymera please point him to the interfaces for BIDS validation and metadata extraction he should use.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1657727241,"metadata":{"github-id":"IC_kwDODBZtRc5GiRfq","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1183389674"},"message":"@yarikoptic @TheChymera Question: What happens if a directory contains a `dataset_description.json` file and one of its (possibly nested) subdirectories also contains a `dataset_description.json` file?  Does the inner file start a new BIDS dataset, or is it treated as just another file of the outer dataset?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1657729425,"metadata":{"github-id":"IC_kwDODBZtRc5Gia6W","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1183428246"},"message":"treat it as a \"start of a new BIDS dataset\". It will be for validator to decide if that is a permitted location to have such nested BIDS dataset (some, like `rawdata/` or any directory under `derivatives/` folder are ok to be nested BIDS datasets, and in turn have other nested BIDS datasets)","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1657789328,"metadata":{"github-id":"IC_kwDODBZtRc5GlUdf","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1184188255"},"message":"@jwodder from the point of view of the pilot DANDI support we have just merged, both would be detected as candidates, and the validation would be run for both.\nHowever, running the validator on the parent directory will *also* index the child directory *and* will flag any duplicate “top level” files (such as README) as invalid.\n\nThis is a repercussion of top level patterns being unique, i.e. unlike entity files there should only ever be one file matching one pattern...\n\nUltimately this is a shortcoming of the validator path selection and not of our DANDI wrapper.\nI'll work on fixing it in the validator code.","files":null},{"type":6,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1657790832,"metadata":{"github-id":"UCE_lALODBZtRc5GlUdfziUhpyQ"},"target":"2f831b73fe08e2438c27dcf737450e396cfe11ee7b1b2adb4164ec75aa10e4ec","message":"@jwodder from the point of view of the pilot DANDI support we have just merged, both would be detected as candidates, and the validation would be run for both.\nHowever, running the validator on the parent directory will *also* index the child directory *and* will flag any duplicate “top level” files (such as README) as invalid.\n\nThis is a repercussion of top level matches being necessarily unique, i.e. unlike entity files there should only ever be one file matching one pattern...\n\nUltimately this is a shortcoming of the validator path selection and not of our DANDI wrapper.\nI'll work on fixing it in the validator code.","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1657790474,"metadata":{"github-id":"IC_kwDODBZtRc5GlZKX","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1184207511"},"message":"Fix incoming: https://github.com/bids-standard/bids-specification/pull/1145","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658243398,"metadata":{"github-id":"IC_kwDODBZtRc5G4V9f","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1189175135"},"message":"@TheChymera So, if I have a `dataset_description.json` file and a list of all files in a BIDS dataset:\n\n* How do I validate the dataset/assets?\n* How does the DANDI metadata for the assets differ from non-BIDS assets?","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658264308,"metadata":{"github-id":"IC_kwDODBZtRc5G5v8V","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1189543701"},"message":"@jwodder if the `dataset_description.json` file is inside the BIDS directory, you would just just point the validator to the BIDS directory, e.g.:\n\n```console\nchymera@decohost ~/src/bids-examples $ dandi validate-bids asl003/\n2022-07-19 16:53:10,234 [ WARNING] A newer version (0.45.1) of dandi/dandi-cli is available. You are using 0.44.1+45.g2ed8929.dirty\n2022-07-19 16:53:10,384 [    INFO] Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n2022-07-19 16:53:10,384 [    INFO] NumExpr defaulting to 8 threads.\n2022-07-19 16:53:11,608 [ WARNING] BIDSVersion 1.5.0 is less than the minimal working 1.7.0+369. Falling back to 1.7.0+369. To force the usage of earlier versions specify them explicitly when calling the validator.\nAll filenames are BIDS-valid and no mandatory files are missing.\n2022-07-19 16:53:12,824 [    INFO] Logs saved in /home/chymera/.cache/dandi-cli/log/20220719205309Z-9230.log\n```\n\nIf you do not have the directory at hand and just a list of paths which do not exist on your system this will only be possible after we have merge `dummy_path` support:\n* Upstream PR including this improvement: https://github.com/bids-standard/bids-specification/pull/1120\n* DANDI fast-track PR which includes this feature before we start using upstream code as a library: https://github.com/dandi/dandi-cli/pull/1061\n\n\u003e How does the DANDI metadata for the assets differ from non-BIDS assets?\n\nCurrently it differs in that the keys are not fully homogenized with the DANDI nomenclature. I did a bit of standardizing [here](https://github.com/dandi/dandi-cli/blob/1c947365311732943753e15199a57c9bfd2759bf/dandi/metadata.py#L49-L52), but there are a few more keys coming in a new PR soon.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658264463,"metadata":{"github-id":"IC_kwDODBZtRc5G5wbI","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1189545672"},"message":"@TheChymera\n\n* I meant, how would I do the validation using the Python API, not the CLI?\n* How should I generate the metadata for BIDS assets?","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658265665,"metadata":{"github-id":"IC_kwDODBZtRc5G50ld","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1189562717"},"message":"@jwodder\n\n\u003e I meant, how would I do the validation using the Python API, not the CLI?\n\ncurrently that can't be done on :master because we need the actual paths, however, as soon as the `dummy_paths` parameter is in, you could do it as I have in this PR (it's the same PR which implements dummy path support ahead of upstream):\nhttps://github.com/dandi/dandi-cli/blob/1ae0a3b2a8ab407b9f7347ba0d7e33c3b627fc87/dandi/bids_utils.py#L116-L131\n\n\u003e How should I generate the metadata for BIDS assets?\n\nThe validator returns a list of dictionaries for all matches found, where the keys are BIDS-standard (but not DANDI-standard) metadata fields:\nhttps://github.com/dandi/dandi-cli/blob/1ae0a3b2a8ab407b9f7347ba0d7e33c3b627fc87/dandi/bids_utils.py#L134","files":null},{"type":6,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658265709,"metadata":{"github-id":"UCE_lALODBZtRc5G50ldziVNvHk"},"target":"46d2d4c6339f6050f85804f39bd4567356e7f619323dce6a5f6376ac36f658e6","message":"@jwodder\n\n\u003e I meant, how would I do the validation using the Python API, not the CLI?\n\ncurrently that can't be done on :master because we need the actual paths, however, as soon as the `dummy_paths` parameter is in, you could do it as I have in [this PR](https://github.com/dandi/dandi-cli/pull/1061) (it's the same PR which implements dummy path support ahead of upstream):\nhttps://github.com/dandi/dandi-cli/blob/1ae0a3b2a8ab407b9f7347ba0d7e33c3b627fc87/dandi/bids_utils.py#L116-L131\n\n\u003e How should I generate the metadata for BIDS assets?\n\nThe validator returns a list of dictionaries for all matches found, where the keys are BIDS-standard (but not DANDI-standard) metadata fields:\nhttps://github.com/dandi/dandi-cli/blob/1ae0a3b2a8ab407b9f7347ba0d7e33c3b627fc87/dandi/bids_utils.py#L134","files":null},{"type":6,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658265773,"metadata":{"github-id":"UCE_lALODBZtRc5G50ldziVNvp8"},"target":"46d2d4c6339f6050f85804f39bd4567356e7f619323dce6a5f6376ac36f658e6","message":"@jwodder\n\n\u003e I meant, how would I do the validation using the Python API, not the CLI?\n\ncurrently that can't be done on :master because we need the actual paths, however, as soon as the `dummy_paths` parameter is in, you could do it as I have in [this PR](https://github.com/dandi/dandi-cli/pull/1061) (it's the same PR which implements dummy path support ahead of upstream):\nhttps://github.com/dandi/dandi-cli/blob/1ae0a3b2a8ab407b9f7347ba0d7e33c3b627fc87/dandi/bids_utils.py#L116-L131\n\n\u003e How should I generate the metadata for BIDS assets?\n\nThe validator returns a list of dictionaries for all matches found, where the keys are BIDS-standard (but not DANDI-standard) metadata fields, that is the [standardization](https://github.com/dandi/dandi-cli/blob/1c947365311732943753e15199a57c9bfd2759bf/dandi/metadata.py#L49-L52) I previously mentioned:\nhttps://github.com/dandi/dandi-cli/blob/1ae0a3b2a8ab407b9f7347ba0d7e33c3b627fc87/dandi/bids_utils.py#L134","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1658266701,"metadata":{"github-id":"IC_kwDODBZtRc5G5316","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1189576058"},"message":"\u003e \u003e I meant, how would I do the validation using the Python API, not the CLI?\n\u003e \n\u003e currently that can't be done on :master because we need the actual paths\n\nAFAIK @jwodder would have access to actual paths for any purpose of validation or upload","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658267411,"metadata":{"github-id":"IC_kwDODBZtRc5G559e","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1189584734"},"message":"\u003e AFAIK @jwodder would have access to actual paths for any purpose of validation or upload\n\nThen it's as simple as:\n\n```\nchymera@decohost ~ $ cat lala.py\nfrom dandi.validate import validate_bids\nresult = validate_bids(\"~/src/bids-examples/asl003\")\nfor i in result[\"match_listing\"]:\n    print(i[\"path\"])\n    try:\n        print(i[\"subject\"])\n        print(i[\"session\"])\n    except:\n        pass\nchymera@decohost ~ $ python lala.py\n2022-07-19 17:48:55,647 [ WARNING] BIDSVersion 1.5.0 is less than the minimal working 1.7.0+369. Falling back to 1.7.0+369. To force the usage of earlier versions specify them explicitly when calling the validator.\n/home/chymera/src/bids-examples/asl003/CHANGES\n/home/chymera/src/bids-examples/asl003/dataset_description.json\n/home/chymera/src/bids-examples/asl003/README\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_m0scan.nii.gz\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_aslcontext.tsv\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_m0scan.json\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.json\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asllabeling.jpg\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/anat/sub-Sub1_T1w.json\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/anat/sub-Sub1_T1w.nii.gz\nSub1\nNone\n```","files":null},{"type":6,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658267444,"metadata":{"github-id":"UCE_lALODBZtRc5G559eziVN-34"},"target":"195094c88f0c5f656e16367d4039358ac8d3c12d99852edef974519a798ee5d2","message":"\u003e AFAIK @jwodder would have access to actual paths for any purpose of validation or upload\n\nThen it's as simple as:\n\n```console\nchymera@decohost ~ $ cat lala.py\nfrom dandi.validate import validate_bids\nresult = validate_bids(\"~/src/bids-examples/asl003\")\nfor i in result[\"match_listing\"]:\n    print(i[\"path\"])\n    try:\n        print(i[\"subject\"])\n        print(i[\"session\"])\n    except:\n        pass\nchymera@decohost ~ $ python lala.py\n2022-07-19 17:48:55,647 [ WARNING] BIDSVersion 1.5.0 is less than the minimal working 1.7.0+369. Falling back to 1.7.0+369. To force the usage of earlier versions specify them explicitly when calling the validator.\n/home/chymera/src/bids-examples/asl003/CHANGES\n/home/chymera/src/bids-examples/asl003/dataset_description.json\n/home/chymera/src/bids-examples/asl003/README\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_m0scan.nii.gz\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_aslcontext.tsv\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_m0scan.json\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.json\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asllabeling.jpg\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/anat/sub-Sub1_T1w.json\nSub1\nNone\n/home/chymera/src/bids-examples/asl003/sub-Sub1/anat/sub-Sub1_T1w.nii.gz\nSub1\nNone\n```","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658417777,"metadata":{"github-id":"IC_kwDODBZtRc5HBv14","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1191640440"},"message":"@yarikoptic Should the validation errors for a BIDS dataset be attached to the `dataset_description.json` object or to the relevant assets (for those that are asset-specific)?  Note that the latter will be much harder to do when processing upload assets in parallel.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658418098,"metadata":{"github-id":"IC_kwDODBZtRc5HBxQB","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1191646209"},"message":"@yarikoptic @satra @TheChymera Also, the `get_validation_errors()` method of asset objects takes a `schema_version` object denoting the version of the Dandi metadata schema to use.  Is there a way to map this version to a BIDS schema version?","files":null},{"type":6,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658418108,"metadata":{"github-id":"UCE_lALODBZtRc5HBxQBziVisVQ"},"target":"24a913ef9e1405805305a51da91d1da5d436af4528d84a896d8b2ea697e855d7","message":"@yarikoptic @satra @TheChymera Also, the `get_validation_errors()` method of asset objects takes a `schema_version` argument denoting the version of the Dandi metadata schema to use.  Is there a way to map this version to a BIDS schema version?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1658418778,"metadata":{"github-id":"IC_kwDODBZtRc5HB0Mx","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1191658289"},"message":"\u003e Is there a way to map this version to a BIDS schema version?\n\n@jwodder - there isn't, since those are two distinct schemas. just like NWB has a version, i would keep the bids version as described in the dataset_description.json. and if you are referring to the bids schema as implemented in dandi, one could add validation information, including dandi-specific bids schema version, in the provenance section of the metadata (`wasGeneratedBy`) since that is a list of activities.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1658522935,"metadata":{"github-id":"IC_kwDODBZtRc5HGpV8","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1192924540"},"message":"\u003e @yarikoptic Should the validation errors for a BIDS dataset be attached to the `dataset_description.json` object or to the relevant assets (for those that are asset-specific)? Note that the latter will be much harder to do when processing upload assets in parallel.\n\nShort overall answer before I outline a possible design I see: I think errors associated with paths should be available (assigned or query-able) for those specific paths and it should be possible to get all errors across all paths of the dataset (#943 relates as might need tune up depending on what we come up with here or there ;)).\n \nATM, `upload` allows to skip \"invalid\" files while uploading \"valid\" files. Originally it was done because validation was done \"in-line\" with upload -- it was taking time to validate, so we did not pre-validate all files to say that entire dataset is bad, but discovered/reported bad assets in the course of the upload.  Moreover, validation of `dandiset.yaml` was done independently from all other files, although it is more of a \"Dandiset level validation\". *For BIDS we still can do the same since current validation can take individual path at a time I believe.  But overall we might want to step away from that practice.*\nWith that in mind, even though in short term we could associate with some superficial `DatasetDescriptionAsset` to signal that it is a BIDS dataset, we might better provide more explicit structure. \n\nI thought I had described it here or elsewhere but I failed to find it, so coming up with my thinking here.  I think we overall have to work with a following structure. I was not thinking yet on how and whether it should relate to existing Dandiset interfaces, may be consider them all below `Local` as in `files.py`:\n\n- `Dandiset`\n  - `Dandiset` must have `dandiset.yaml` (`DandisetMetadataFile`) which we know how to validate, it represents metadata about entire Dandiset\n  - `Dandiset` contains either DANDIDataset or BIDSDataset (AFAIK **cannot** be both, @satra please confirm if our thinking aligned). I might even say we must **not** have any \"abstract\" Dataset - they all should be DANDIDataset or BIDSDataset(s). Default (if not BIDS) is DANDIDataset.\n    - I decided for \"contains\" instead of just subclassing from Dandiset because nested BIDSDataset might not to \"contain\" `dandiset.yaml`.  But may be those specific ones could be subclasses (thuse `DANDIDandiset` and `BIDSDandiset`), and there would be PlainBIDSDataset implementing Dandiset independent logic (such as validation), and then `BIDSDandiset(Dandiset, PlainBIDSDataset)` to mix in. For now I will continue with `Dataset` delegated flavors below: \n    - `DANDIDataset` is the one `dandi organize` renames into, and the layout (directory/filenames) of which we do not yet validate AFAIK although should (I noted that in my reply here https://github.com/dandi/dandi-cli/issues/1074#issuecomment-1192866526). Errors will be associated with specific paths.\n    - `BIDSDataset` (like 000026) allows for nested BIDSDatasets underneath (e.g. under `derivatives/{name}` or `rawdata/` or somewhere under `sourcedata/`). Currently code finds them anywhere (not necessarily in those locations) and validates them independently. That it is a BIDSDatasets (and not DANDIDatasets) decided by having `dataset_description.json`.\n  - Each asset should be associated with some specific (not just abstract Dataset) BIDSDataset or DANDIDataset, even if that asset is not \"valid\".\n     - BIDSDataset might be a part of another BIDSDataset. `BIDSDataset.subdatasets: dict` could provide mapping.\n     - each Dataset could have some validation errors associated purely with it, which cannot relate to specific asset(s). An example -- error about *missing* required asset (e.g. `README` with some extension, required in BIDS, is missing)\n     - some validation errors might relate to groups of assets (e.g. some inconsistency across files).  I am not sure yet how/where we should keep them, probably within Dataset level, just record containing all those paths.\n     - we should be able to tell what errors are for an asset. \n     - given all the above, I feel that we should collect errors at the level of Dataset and allow for querying for errors associated with a specific sub-path (might be within subdataset in BIDS).\n\nSo, I am thinking, that for upload/validation, given the optional paths (limiting what we can find) we first figure out the Dandiset (location of `dandiset.yaml`) and its Dataset (BIDS or DANDI), and subdatasets (if paths provided, for those paths). If there is no Dataset -- nothing to upload.  If no paths given, it will be for the type of the dataset to figure out which paths to pick up (like we get .nwb and video files for DANDIDataset).\nThen before upload `Dandiset.validate` would do its `self.DandisetMetadataFile-instance.validate()` followed by `self.Dataset-instance.validate()` (which for `DANDIDataset` would do nothing ATM; for BIDSDataset -- might do something in the future, like may be full dataset validation). Some method like `.discover_local_assets(path: list[str]=None)` should be available to associate/discover optionally provided paths with corresponding Datasets, might be BIDS vs DANDI dependent, and thus building up the tree. `Dandiset.validate_asset(path: str)` could lazily (unless already full validation ran, and that path was marked \"ok\" or have already results associated) initiate corresponding Dataset (if needed, e.g. if nested BIDSDataset), trigger corresponding (depending on Dataset type and type of the asset, e.g. NWBAsset) `LocalAsset.validate()`, store result of the validation within corresponding `Dataset`, and return validity status for upload to use.\n\nRelating to WiP in #1076, `find_dandi_files` and `dandi_file` should become method of that `Dandiset` (already takes, albeit optional `dandiset_path` . If we figure out dandiset, which must exist, first -- can become methods). There it should figure out corresponding to the path `Dataset` and associate it with that `Asset`.  To contain all validation results in corresponding Dataset, may be `LocalAsset.validate` should request to update its record within its `Dataset`.\n\nSomething like that. WDYT?","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1658766466,"metadata":{"github-id":"UCE_lALODBZtRc5HGpV8ziV854U"},"target":"a6fb9035d2dfa586ba592fdfff3d9a02a41c10cf25c263c5b7a2ee554bb5734e","message":"\u003e @yarikoptic Should the validation errors for a BIDS dataset be attached to the `dataset_description.json` object or to the relevant assets (for those that are asset-specific)? Note that the latter will be much harder to do when processing upload assets in parallel.\n\nShort overall answer before I outline a possible design I see: I think errors associated with paths should be available (assigned or query-able) for those specific paths and it should be possible to get all errors across all paths of the dataset (#943 relates as might need tune up depending on what we come up with here or there ;)).\n \nATM, `upload` allows to skip \"invalid\" files while uploading \"valid\" files. Originally it was done because validation was done \"in-line\" with upload -- it was taking time to validate, so we did not pre-validate all files to say that entire dataset is bad, but discovered/reported bad assets in the course of the upload.  Moreover, validation of `dandiset.yaml` was done independently from all other files, although it is more of a \"Dandiset level validation\". *For BIDS we still can do the same since current validation can take individual path at a time I believe.  But overall we might want to step away from that practice.*\nWith that in mind, even though in short term we could associate with some superficial `DatasetDescriptionAsset` to signal that it is a BIDS dataset, we might better provide more explicit structure. \n\n\u003cdetails\u003e\n\u003csummary\u003eExtracted as a separate https://github.com/dandi/dandi-cli/issues/1082 now\u003c/summary\u003e\n\nI thought I had described it here or elsewhere but I failed to find it, so coming up with my thinking here.  I think we overall have to work with a following structure. I was not thinking yet on how and whether it should relate to existing Dandiset interfaces, may be consider them all below `Local` as in `files.py`:\n\n- `Dandiset`\n  - `Dandiset` must have `dandiset.yaml` (`DandisetMetadataFile`) which we know how to validate, it represents metadata about entire Dandiset\n  - `Dandiset` contains either DANDIDataset or BIDSDataset (AFAIK **cannot** be both, @satra please confirm if our thinking aligned). I might even say we must **not** have any \"abstract\" Dataset - they all should be DANDIDataset or BIDSDataset(s). Default (if not BIDS) is DANDIDataset.\n    - I decided for \"contains\" instead of just subclassing from Dandiset because nested BIDSDataset might not to \"contain\" `dandiset.yaml`.  But may be those specific ones could be subclasses (thuse `DANDIDandiset` and `BIDSDandiset`), and there would be PlainBIDSDataset implementing Dandiset independent logic (such as validation), and then `BIDSDandiset(Dandiset, PlainBIDSDataset)` to mix in. For now I will continue with `Dataset` delegated flavors below: \n    - `DANDIDataset` is the one `dandi organize` renames into, and the layout (directory/filenames) of which we do not yet validate AFAIK although should (I noted that in my reply here https://github.com/dandi/dandi-cli/issues/1074#issuecomment-1192866526). Errors will be associated with specific paths.\n    - `BIDSDataset` (like 000026) allows for nested BIDSDatasets underneath (e.g. under `derivatives/{name}` or `rawdata/` or somewhere under `sourcedata/`). Currently code finds them anywhere (not necessarily in those locations) and validates them independently. That it is a BIDSDatasets (and not DANDIDatasets) decided by having `dataset_description.json`.\n  - Each asset should be associated with some specific (not just abstract Dataset) BIDSDataset or DANDIDataset, even if that asset is not \"valid\".\n     - BIDSDataset might be a part of another BIDSDataset. `BIDSDataset.subdatasets: dict` could provide mapping.\n     - each Dataset could have some validation errors associated purely with it, which cannot relate to specific asset(s). An example -- error about *missing* required asset (e.g. `README` with some extension, required in BIDS, is missing)\n     - some validation errors might relate to groups of assets (e.g. some inconsistency across files).  I am not sure yet how/where we should keep them, probably within Dataset level, just record containing all those paths.\n     - we should be able to tell what errors are for an asset. \n     - given all the above, I feel that we should collect errors at the level of Dataset and allow for querying for errors associated with a specific sub-path (might be within subdataset in BIDS).\n\nSo, I am thinking, that for upload/validation, given the optional paths (limiting what we can find) we first figure out the Dandiset (location of `dandiset.yaml`) and its Dataset (BIDS or DANDI), and subdatasets (if paths provided, for those paths). If there is no Dataset -- nothing to upload.  If no paths given, it will be for the type of the dataset to figure out which paths to pick up (like we get .nwb and video files for DANDIDataset).\nThen before upload `Dandiset.validate` would do its `self.DandisetMetadataFile-instance.validate()` followed by `self.Dataset-instance.validate()` (which for `DANDIDataset` would do nothing ATM; for BIDSDataset -- might do something in the future, like may be full dataset validation). Some method like `.discover_local_assets(path: list[str]=None)` should be available to associate/discover optionally provided paths with corresponding Datasets, might be BIDS vs DANDI dependent, and thus building up the tree. `Dandiset.validate_asset(path: str)` could lazily (unless already full validation ran, and that path was marked \"ok\" or have already results associated) initiate corresponding Dataset (if needed, e.g. if nested BIDSDataset), trigger corresponding (depending on Dataset type and type of the asset, e.g. NWBAsset) `LocalAsset.validate()`, store result of the validation within corresponding `Dataset`, and return validity status for upload to use.\n\nRelating to WiP in #1076, `find_dandi_files` and `dandi_file` should become method of that `Dandiset` (already takes, albeit optional `dandiset_path` . If we figure out dandiset, which must exist, first -- can become methods). There it should figure out corresponding to the path `Dataset` and associate it with that `Asset`.  To contain all validation results in corresponding Dataset, may be `LocalAsset.validate` should request to update its record within its `Dataset`.\n\nSomething like that. WDYT?\n\u003cdetails\u003e","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1658524524,"metadata":{"github-id":"IC_kwDODBZtRc5HGtJx","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1192940145"},"message":"\u003e @yarikoptic @satra @TheChymera Also, the `get_validation_errors()` method of asset objects takes a `schema_version` argument denoting the version of the Dandi metadata schema to use. Is there a way to map this version to a BIDS schema version?\n\ndon't worry about BIDS version here! And looking at how we \"use\" `schema_version` in `get_validation_errors` it seems that ability to provide version is \"superficial\" since we simply fail to validate if specified version is not current...  And in general relying on `dandischema` to do a possible upgrade for current dandischema version if version in the record is outdated.  So I feel that may be ability to specify version is \"legacy\" (@satra -- do you remember anything on that?).\nFor `validate-bids` I believe we provided ability to specify BIDS schema version, since we keep working on BIDS spec to fit our needs so some times \"cheat\" in that we provide another (our) version of BIDS schema to use.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658763250,"metadata":{"github-id":"IC_kwDODBZtRc5HLk_f","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1194217439"},"message":"@yarikoptic I'm thinking that such a restructuring of the classes should happen in a separate issue/PR.  For right now, if we want individual BIDS assets to have their own validation errors (with BIDS dataset-wide errors just associated with the `dataset_description.json` asset for now), at what point in the upload process should this validation occur?  Should all BIDS datasets be validated before processing any individual file uploads, with asset-specific errors attached to the respective assets for reporting once the upload code gets around to them, or should the individual BIDS assets be validated in the course of the upload by calling some method (that is both cached and thread-locked) of the `dataset_description.json` asset?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1658766644,"metadata":{"github-id":"IC_kwDODBZtRc5HL-fc","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1194321884"},"message":"\u003e @yarikoptic I'm thinking that such a restructuring of the classes should happen in a separate issue/PR. \n\ndone - #1082 \n\n\u003e For right now, if we want individual BIDS assets to have their own validation errors (with BIDS dataset-wide errors just associated with the `dataset_description.json` asset for now), at what point in the upload process should this validation occur? Should all BIDS datasets be validated before processing any individual file uploads, with asset-specific errors attached to the respective assets for reporting once the upload code gets around to them, or should the individual BIDS assets be validated in the course of the upload by calling some method (that is both cached and thread-locked) of the `dataset_description.json` asset?\n\nFor consistency with how nwbs are handled and since ATM it is allowed -- easiest is to do at the point of getting to that file during upload.  It would cause some duplicate avoidable \"find the root of BIDS dataset\" but it could (if not already) be cached so shouldn't add too much penalty hopefully.  In the course of #1082 refactoring, as I have mentioned, for BIDS such validation might need to happen at the level of the BIDS dataset first before any file from it gets considered for upload.  We might even later introduce policy/behavior to disallow upload of DANDI or BIDS datasets altogether by default if any file fails validation (it is not current behavior).","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658771662,"metadata":{"github-id":"IC_kwDODBZtRc5HMVRl","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1194415205"},"message":"@TheChymera Does the structure returned by `validate_bids()` currently contain any information on asset-specific validation errors, or are all the errors dataset-wide things like required files being missing?  If the former, how do I extract the relevant information from the return value?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1658849087,"metadata":{"github-id":"IC_kwDODBZtRc5HQ9So","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1195627688"},"message":"ping @TheChymera . After all current BIDS work is merged into BIDS we really need to work out to establish more \"accessible\" interface to BIDS validation (proper \"error records\", #943) -- currently returned `dict` is hard to \"understand\".\n\n@jwodder - consider following keys in the returned dict, and @TheChymera please expand/fix if I am incomplete/wrong in any of that \n\n- `path-listing` -- should have paths which were \"tested\". So if you gave a folder for validation, this would list all paths which were tested\n- `path_tracking` -- if path is listed in that list -- it is invalid. So it is per-asset error\n- `schema-tracking` -- dataset-level errors for missing required assets among those which were given to be validated.\n\nThat deep meaning over those keys could be gathered by looking at the reporting code at https://github.com/dandi/dandi-cli/blob/HEAD/dandi/bids_validator_xs.py#L513","files":null},{"type":6,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1658849100,"metadata":{"github-id":"UCE_lALODBZtRc5HQ9SoziWIJhc"},"target":"71a5315770f6e20c7e07736a323906d9a4ee7913b0ab7d20e509993ff435423f","message":"ping @TheChymera . After all current BIDS work is merged into BIDS we really need to work out to establish more \"accessible\" interface to BIDS validation (proper \"error records\", #943) -- currently returned `dict` is hard to \"understand\".\n\n@jwodder - consider following keys in the returned dict, and @TheChymera please expand/fix if I am incomplete/wrong in any of that \n\n- `path_listing` -- should have paths which were \"tested\". So if you gave a folder for validation, this would list all paths which were tested\n- `path_tracking` -- if path is listed in that list -- it is invalid. So it is per-asset error\n- `schema_tracking` -- dataset-level errors for missing required assets among those which were given to be validated.\n\nThat deep meaning over those keys could be gathered by looking at the reporting code at https://github.com/dandi/dandi-cli/blob/HEAD/dandi/bids_validator_xs.py#L513","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658849370,"metadata":{"github-id":"IC_kwDODBZtRc5HQ-wf","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1195633695"},"message":"@TheChymera @yarikoptic Should files listed in `path_tracking` be treated as failing validation?  The code [here](https://github.com/dandi/dandi-cli/blob/1c947365311732943753e15199a57c9bfd2759bf/dandi/bids_validator_xs.py#L688-L689) implies that such a state is just a warning rather than an error.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1658856010,"metadata":{"github-id":"IC_kwDODBZtRc5HReM8","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1195762492"},"message":"Yes AFAIK, treat as error","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658856284,"metadata":{"github-id":"IC_kwDODBZtRc5HRfRq","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1195766890"},"message":"\u003e @TheChymera please expand/fix if I am incomplete/wrong in any of that\n\nthe description is correct. Sorry if it seemed convoluted, what it's doing is that the “tracking” list is eroded with each match.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658930370,"metadata":{"github-id":"IC_kwDODBZtRc5HVbgl","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1196800037"},"message":"@TheChymera If I already have the paths of all the BIDS assets in the dataset, is there a preferred way to call the validation code with those paths so that it doesn't fetch the paths again, or do I have to modify what's already in `bids_validator_xs.py`?","files":null},{"type":5,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658935269,"metadata":{"github-id":"LE_lADODBZtRc5NKxKPzwAAAAGlpGhh"},"added":["BIDS"],"removed":[]},{"type":5,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658935269,"metadata":{"github-id":"LE_lADODBZtRc5NKxKPzwAAAAGlpGhn"},"added":["Python API"],"removed":[]},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658943050,"metadata":{"github-id":"IC_kwDODBZtRc5HWgQV","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1197081621"},"message":"@jwodder you can pass a list of files to `validate_bids()` under the `bids_paths` parameter:\n\nhttps://github.com/dandi/dandi-cli/blob/468e39c9c3f7962e5078e9bdc4b6ad1f16fa4ee8/dandi/support/bids/validator.py#L730\n\nThe get file logic will be run, but it will bypass most the checks: https://github.com/dandi/dandi-cli/blob/468e39c9c3f7962e5078e9bdc4b6ad1f16fa4ee8/dandi/support/bids/validator.py#L61-L63\n\nIf you want to not run the get file logic at all (in which case you can also use paths which don't exist locally) you can use the `dummy_path` mechanic I described earlier.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658949233,"metadata":{"github-id":"IC_kwDODBZtRc5HXLp5","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1197259385"},"message":"I believe I've properly integrated validation in b83ba3f.\n\n@TheChymera @satra @yarikoptic Could someone please explain exactly how to acquire BIDS metadata and integrate it into Dandi metadata?","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658950619,"metadata":{"github-id":"IC_kwDODBZtRc5HXRLK","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1197281994"},"message":"@jwodder I had some basic support for that here:\n\nhttps://github.com/dandi/dandi-cli/blob/1c947365311732943753e15199a57c9bfd2759bf/dandi/metadata.py#L124-L128\n\nBasically the keys in the dictionary returned by the bundled module are just changed to the DANDI equivalent.","files":null},{"type":6,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658950696,"metadata":{"github-id":"UCE_lALODBZtRc5HXRLKziWWocU"},"target":"04885ef07c8f8995cac633cc4f7b70011dc98f069578ad885bf98176a4ed47a7","message":"@jwodder I had some basic support for that here:\n\nhttps://github.com/dandi/dandi-cli/blob/1c947365311732943753e15199a57c9bfd2759bf/dandi/metadata.py#L124-L128\n\nBasically the keys in the `result[match_listing]` dictionary returned by the bundled module are just changed to the DANDI equivalent.","files":null},{"type":6,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658953151,"metadata":{"github-id":"UCE_lALODBZtRc5HXRLKziWXCTo"},"target":"04885ef07c8f8995cac633cc4f7b70011dc98f069578ad885bf98176a4ed47a7","message":"@jwodder I had some basic support for that here:\n\nhttps://github.com/dandi/dandi-cli/blob/1c947365311732943753e15199a57c9bfd2759bf/dandi/metadata.py#L124-L128\n\nBasically the keys in the `result[match_listing]` dictionaries returned by the bundled module are just changed to the DANDI equivalent.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1658951347,"metadata":{"github-id":"IC_kwDODBZtRc5HXUE0","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1197293876"},"message":"@TheChymera That involves running `validate_bids()` on just the individual path.  If I've already run it on all the assets in the BIDS dataset at once, how do I get an individual asset's metadata out of the result?","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658953061,"metadata":{"github-id":"IC_kwDODBZtRc5HXa56","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1197321850"},"message":"@jwodder to do that you need a set of keys, the values of which unambiguously identify the file, `path` is of course the obvious one.\n\n```console\nchymera@decohost ~ $ cat /tmp/lala.py \nfrom dandi.validate import validate_bids\nresult = validate_bids(\"~/src/bids-examples/asl003\")\n\nprint(\"These are the valid paths:\")\nfor i in result[\"match_listing\"]:\n    print(i[\"path\"])\n\nselection = \"/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz\"\nprint(f\"Now let's take one and get the info: {selection}\")\nprint([i for i in result[\"match_listing\"] if i[\"path\"] == selection])\nchymera@decohost ~ $ python /tmp/lala.py \n2022-07-27 16:15:57,799 [ WARNING] BIDSVersion 1.5.0 is less than the minimal working 1.7.0+369. Falling back to 1.7.0+369. To force the usage of earlier versions specify them explicitly when calling the validator.\nThese are the valid paths:\n/home/chymera/src/bids-examples/asl003/CHANGES\n/home/chymera/src/bids-examples/asl003/dataset_description.json\n/home/chymera/src/bids-examples/asl003/README\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_m0scan.nii.gz\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_aslcontext.tsv\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_m0scan.json\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.json\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asllabeling.jpg\n/home/chymera/src/bids-examples/asl003/sub-Sub1/anat/sub-Sub1_T1w.json\n/home/chymera/src/bids-examples/asl003/sub-Sub1/anat/sub-Sub1_T1w.nii.gz\nNow let's take one and get the info: /home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz\n[{'subject': 'Sub1', 'session': None, 'acquisition': None, 'reconstruction': None, 'direction': None, 'run': None, 'path': '/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz'}]\n```","files":null},{"type":6,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1658953096,"metadata":{"github-id":"UCE_lALODBZtRc5HXa56ziWXBmk"},"target":"460ebb9deb684d20990418415bf18ef29172566d0d4e61bf46167986b484bded","message":"@jwodder to do that you need a set of keys, the values of which unambiguously identify the file, `path` is of course the obvious one, though it can be larger sets as well.\n\n```console\nchymera@decohost ~ $ cat /tmp/lala.py \nfrom dandi.validate import validate_bids\nresult = validate_bids(\"~/src/bids-examples/asl003\")\n\nprint(\"These are the valid paths:\")\nfor i in result[\"match_listing\"]:\n    print(i[\"path\"])\n\nselection = \"/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz\"\nprint(f\"Now let's take one and get the info: {selection}\")\nprint([i for i in result[\"match_listing\"] if i[\"path\"] == selection])\nchymera@decohost ~ $ python /tmp/lala.py \n2022-07-27 16:15:57,799 [ WARNING] BIDSVersion 1.5.0 is less than the minimal working 1.7.0+369. Falling back to 1.7.0+369. To force the usage of earlier versions specify them explicitly when calling the validator.\nThese are the valid paths:\n/home/chymera/src/bids-examples/asl003/CHANGES\n/home/chymera/src/bids-examples/asl003/dataset_description.json\n/home/chymera/src/bids-examples/asl003/README\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_m0scan.nii.gz\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_aslcontext.tsv\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_m0scan.json\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.json\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz\n/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asllabeling.jpg\n/home/chymera/src/bids-examples/asl003/sub-Sub1/anat/sub-Sub1_T1w.json\n/home/chymera/src/bids-examples/asl003/sub-Sub1/anat/sub-Sub1_T1w.nii.gz\nNow let's take one and get the info: /home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz\n[{'subject': 'Sub1', 'session': None, 'acquisition': None, 'reconstruction': None, 'direction': None, 'run': None, 'path': '/home/chymera/src/bids-examples/asl003/sub-Sub1/perf/sub-Sub1_asl.nii.gz'}]\n```","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1659015781,"metadata":{"github-id":"IC_kwDODBZtRc5Han3I","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198161352"},"message":"@TheChymera @yarikoptic @satra [The code linked by Horea above](https://github.com/dandi/dandi-cli/blob/1c947365311732943753e15199a57c9bfd2759bf/dandi/metadata.py#L124-L128) only applies BIDS metadata to NWB files, and it replaces the normal NWB metadata.  Are either of those things that we want to do/keep doing?","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1659017807,"metadata":{"github-id":"IC_kwDODBZtRc5HayH-","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198203390"},"message":"the normal metadata for NWB should not be replaced nor should the bids metadata be applied to NWB files. \n\nif it's bids, then there is the possibility of an NWB file inside it, so it should also do the related NWB processing for those files.\n\nhowever, setting that aside, any bids path should result in metadata that augments all the standard metadata we have for an asset. so similar to `nwb2asset`, we will want a `bidspath2asset`. this latter function also needs to handle a `participants.tsv` file and, if present, a `sessions.tsv` and a `samples.tsv` files to fill in the relevant pieces of the asset metadata,\n\nthe original use of the metadata dictionary was to support organize, and that would be irrelevant for a bids dandiset.","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1659017910,"metadata":{"github-id":"IC_kwDODBZtRc5Hayln","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198205287"},"message":"@jwodder \n\n\u003e only applies BIDS metadata to NWB files,\n\nFrom my local testing with `ls` it seemed to source the data of other files as well, e.g. `.nii.gz`.\n\n\u003e it replaces the normal NWB metadata\n\nYes, this is true, It doesn't replace it, though, it just doesn't go on to source the nwb data at all if it's a BIDS dataset. Perhaps this isn't such a bad idea? i.e. if it's a BIDS dataset we source the information via the BIDS standard.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1659018137,"metadata":{"github-id":"IC_kwDODBZtRc5HaztV","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198209877"},"message":"@satra So:\n\n* Given a BIDS asset that is not an NWB file, how should I determine the metadata?\n* Given a BIDS asset that is also an NWB file, how should I determine the metadata?","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1659018707,"metadata":{"github-id":"IC_kwDODBZtRc5Ha2jP","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198221519"},"message":"@TheChymera \n\n\u003e From my local testing with `ls` it seemed to source the data of other files as well, e.g. `.nii.gz`.\n\n`dandi ls` does not accurately reflect metadata for non-NWB files, as it uses the `get_metadata()` function, which the upload routines only use for NWBs.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1659018776,"metadata":{"github-id":"IC_kwDODBZtRc5Ha2-9","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198223293"},"message":"nwb file have had two notions of metadata, one that is extracted from the file and then the Asset structure (which is what i would call metadata). are you talking about the former or both? \n\nfor current flow: nwb --\u003e metadata --\u003e Asset metadata\n\nfor bids it would be: \nbidspath --\u003e metadata --\u003e asset metadata\nbidspath + nwb --\u003e metadata (joint info from both processors) --\u003e asset metadata\n\nperhaps the refactoring can simply create one metadata structure, the Asset structure, which can then be used for other things (ls, organize, validate, etc.,.).","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1659018910,"metadata":{"github-id":"IC_kwDODBZtRc5Ha3zq","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198226666"},"message":"@satra \n\n\u003e nwb file have had two notions of metadata, one that is extracted from the file and then the Asset structure (which is what i would call metadata). are you talking about the former or both?\n\nI'm talking about the final `dandischema.BareAsset` value.\n\n\u003e for bids it would be:  \n\u003e bidspath --\u003e metadata --\u003e asset metadata  \n\u003e bidspath + nwb --\u003e metadata (joint info from both processors) --\u003e asset metadata\n\nDescribe these steps in detail.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1659020248,"metadata":{"github-id":"IC_kwDODBZtRc5HbArW","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198262998"},"message":"\u003e Describe these steps in detail.\n\ni would leave that to @TheChymera and can review it, but describing those steps would be almost equivalent to writing the code. essentially one has to follow the details of the bids-specification here and map each field or piece of information into the bareasset structure.","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1659028721,"metadata":{"github-id":"IC_kwDODBZtRc5Hboj-","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198426366"},"message":"\u003e @satra So:\n\u003e \n\u003e * Given a BIDS asset that is not an NWB file, how should I determine the metadata?\n\nThis would be for the @TheChymera to clarify.\n\n\u003e * Given a BIDS asset that is also an NWB file, how should I determine the metadata?\n\nGiven the current (absent yet overall) agreement in https://github.com/bids-standard/bids-specification/pull/761#issuecomment-1183710164 about overloading of metadata in side-car files (and thus in filepath), I would say: for extraction (not validation), take  metadata contained in .nwb file, and only complement (add fields which have no non-None value) with the metadata from the BIDS filename (we aren't even getting anything from sidecar yet, right @TheChymera ?)","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1659029201,"metadata":{"github-id":"IC_kwDODBZtRc5HbqUy","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198433586"},"message":"@yarikoptic @TheChymera What about things like BIDS' \"subject\" field, which the code currently maps to our \"subject_id\" field?  Should the metadata for a BIDS NWB asset use the subject_id from the NWB, from BIDS, or what?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1659030176,"metadata":{"github-id":"IC_kwDODBZtRc5Hbtxf","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198447711"},"message":"\u003e @yarikoptic @TheChymera What about things like BIDS' \"subject\" field, which the code currently maps to our \"subject_id\" field? Should the metadata for a BIDS NWB asset use the subject_id from the NWB, from BIDS, or what?\n\nper my comment [above](https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198426366) let's do from .nwb for now for consistency (unless empty there).  It would be for (our) validator to complain if there is inconsistency, and it will be for .nwb overloading (WiP) to be used to \"fix it\" if needed.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1659034321,"metadata":{"github-id":"IC_kwDODBZtRc5Hb-TR","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198515409"},"message":"@yarikoptic @TheChymera Based on what Horea's posted so far, the BIDS metadata contains the following fields:\n\n* `subject` (currently renamed to `subject_id`)\n* `session` (currently renamed to `session_id`)\n* `path`\n* `acquisition`\n* `reconstruction`\n* `direction`\n* `run`\n* `bids_schema_version`\n\nJust where in a `dandischema.BareAsset` should these fields be mapped to?  Aside from the first three fields, none of these seem to have any presence in the Dandi schema, and I believe they're currently discarded when uploading.","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1659034626,"metadata":{"github-id":"IC_kwDODBZtRc5Hb_VR","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198519633"},"message":"@yarikoptic \n\u003e we aren't even getting anything from sidecar yet,\n\nyes, no files are being opened.\n\nI don't have any strong opinions either way, really, but the reason why I suggested going BIDS-first (or rather path-first) is for what I thought was consistency/usability. We don't get any metadata from the NIfTI headers either, so in addition to privileging NWB over BIDS we'd be privileging NWB over NIfTI. Not least of all, path metadata can be easily sourced without downloading any files... But if you disagree, don't mind me :) just explaining why I suggested BIDS first.","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1659037008,"metadata":{"github-id":"IC_kwDODBZtRc5HcHza","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198554330"},"message":"\u003e @yarikoptic @TheChymera Based on what Horea's posted so far, the BIDS metadata contains the following fields:\n\u003e ...\n\nThose are the fields for a file with that specific suffix.\n\nIn total BIDS files can have any of the following entities, though they might be `None` if none is given (most are optional) or not present as keys in the output if they would be invalid.\n\n```console\nchymera@darkhost ~/src/bids-specification/src/schema $ ag \"^[a-z]*?:\" objects/entities.yaml \n6:acquisition:\n28:atlas:\n38:ceagent:\n49:chunk:\n58:density:\n69:description:\n79:direction:\n88:echo:\n101:flip:\n113:hemisphere:\n125:inversion:\n137:label:\n148:modality:\n157:mtransfer:\n171:part:\n195:processing:\n209:reconstruction:\n218:recording:\n228:resolution:\n239:run:\n256:sample:\n268:session:\n289:space:\n307:split:\n329:stain:\n341:subject:\n348:task:\n358:tracer:\n```","files":null},{"type":6,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1659037058,"metadata":{"github-id":"UCE_lALODBZtRc5HcHzaziWh7Ys"},"target":"fa45dd85b8a43732734198b2352125bf8b67fdea28ce4e3142d2d8cde1c95dd8","message":"\u003e @yarikoptic @TheChymera Based on what Horea's posted so far, the BIDS metadata contains the following fields:\n\u003e ...\n\nThose are the fields for a file with that specific suffix.\n\nIn total BIDS files can have any of the following entities, though they might be `None` if none is given (most are optional) or not present as keys in the output if they would be invalid for the specific suffix.\n\n```console\nchymera@darkhost ~/src/bids-specification/src/schema $ ag \"^[a-z]*?:\" objects/entities.yaml \n6:acquisition:\n28:atlas:\n38:ceagent:\n49:chunk:\n58:density:\n69:description:\n79:direction:\n88:echo:\n101:flip:\n113:hemisphere:\n125:inversion:\n137:label:\n148:modality:\n157:mtransfer:\n171:part:\n195:processing:\n209:reconstruction:\n218:recording:\n228:resolution:\n239:run:\n256:sample:\n268:session:\n289:space:\n307:split:\n329:stain:\n341:subject:\n348:task:\n358:tracer:\n```","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1659385795,"metadata":{"github-id":"IC_kwDODBZtRc5HoEg7","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1201686587"},"message":"@jwodder I do not think we map much from BIDS to our metadata ATM. Just use what we have at https://github.com/dandi/dandi-cli/blob/HEAD/dandi/metadata.py#L124 \n\n```python\n        _meta = validate_bids(path)\n        meta = _meta[\"match_listing\"][0]\n        meta[\"bids_schema_version\"] = _meta[\"bids_schema_version\"]\n        meta = _rename_bids_keys(meta)\n```\nmay be RF it into a function `get_bids_raw_metadata`. Then eventually we would add more keys mappings into that `_rename_bids_keys` I guess. And also extraction of `sex` is not there yet at all.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1659386812,"metadata":{"github-id":"IC_kwDODBZtRc5HoIp4","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1201703544"},"message":"@yarikoptic - for mapping to bareasset the function should map subject, session, modality info. i would suggest a bids2asset function similar to the nwb2asset function.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1659444843,"metadata":{"github-id":"IC_kwDODBZtRc5HrGUE","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1202480388"},"message":"@yarikoptic Using that code will result in only subject and session being set, as those are the only keys that are mapped to something that `prepare_metadata()` (née `metadata2asset()`) recognizes.  Are you sure that's what you want?\n\n@satra \n\n\u003e for mapping to bareasset the function should map subject, session, modality info.\n\nWhere/how is modality represented in the Dandi schema?\n\n\u003e i would suggest a bids2asset function similar to the nwb2asset function.\n\nAssuming this function returns a `BareAsset` instance, do you have a recommendation for how to combine a BIDS `BareAsset` and an NWB `BareAsset` in accordance with [this comment of Yarik's](https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1198426366)?","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1659445327,"metadata":{"github-id":"IC_kwDODBZtRc5HrK6T","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1202499219"},"message":"\u003e @yarikoptic Using that code will result in only subject and session being set, as those are the only keys that are mapped to something that `prepare_metadata()` (née `metadata2asset()`) recognizes. Are you sure that's what you want?\n\nLet's start with that. We will improve upon this later.  I will leave to @satra to clarify about modality","files":null},{"type":3,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1659547199,"metadata":{"github-id":"IC_kwDODBZtRc5Hx3f1","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1204254709"},"message":"\u003e I will leave to @satra to clarify about modality\n\n@jwodder, let's proceed without mapping modality for now, we could always improve that later.","files":null},{"type":3,"author":{"id":"364914f8b1c8a9131d300bf2978e4ffe2ff1aeeb"},"timestamp":1659553303,"metadata":{"github-id":"IC_kwDODBZtRc5HyROB","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1204360065"},"message":"@TheChymera @satra So now I know how to determine metadata for BIDS assets, but what should the Dandi metadata be for the `dataset_description.json` file itself?","files":null},{"type":3,"author":{"id":"0ab0e40999605aa8d5ec6be68372511fdc4a2fdf"},"timestamp":1659558075,"metadata":{"github-id":"IC_kwDODBZtRc5HyjcX","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1204434711"},"message":"@jwodder I actually don't think it would have any, it's a metadata file, so perhaps it should be handled in the same way as `dandiset.yaml`? Regarding the information it can contain it also overlaps with `dandiset.yaml`, though currently the validator does not parse any of its contents other than `BIDSVersion` which applies to the entire dataset.","files":null},{"type":3,"author":{"id":"12cb25e6e3f19b53447b05d00d7d4d53c925c908"},"timestamp":1659567014,"metadata":{"github-id":"IC_kwDODBZtRc5HzCCK","github-url":"https://github.com/dandi/dandi-cli/issues/1044#issuecomment-1204560010"},"message":"\u003e Assuming this function returns a BareAsset instance, do you have a recommendation for how to combine a BIDS BareAsset and an NWB BareAsset\n\ni would say this should apply: bareasset_dict.update(**nwb_asset_dict)\n\nfor now treat dataset_description.json as just another asset. we could help populate it from dandiset metadata, but that's a separate feature in my opinion. we are also not doing anything for all other json and tsv files at the moment. \n\nmodality, sessions/samples/participants.tsv can come in a separate PR.","files":null},{"type":4,"author":{"id":"34bb3b7763ea6d04cae1c21ede40209df34305b5"},"timestamp":1660062285,"metadata":{"github-id":"CE_lADODBZtRc5NKxKPzwAAAAGqmAwU"},"status":2}]}