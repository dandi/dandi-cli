#!/usr/bin/env python

import argparse
from collections import defaultdict
import json
import logging
import os
from pathlib import Path
import sys

import requests

# Configure the logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    stream=sys.stderr,
)

lgr = logging.getLogger(__name__)


def fetch_dandisets(me, bibtex_results=None, get_metadata=False):
    if bibtex_results is None:
        bibtex_results = {}
    # Construct the query URL
    url = (
        f"https://api.dandiarchive.org/api/dandisets/?draft=false&empty=false"
        f"&embargoed=false{'&user=me' if me else ''}"
        # f"&search=000003"  # datacite error in 4.3 schema due to use of ROR
        # f"&search=000029"  # no BibTex
        # f"&search=000623"  # no DOI
        # f"&page_size=2"
    )
    headers = {"accept": "application/json"}
    metadata_records = []
    while True:
        # Fetch the dandisets
        lgr.info(f"Fetching {url}")
        response = requests.get(url, headers=headers)
        j = response.json()
        dandisets = j["results"]
        url = j.get("next")
        lgr.info(f"Got {len(dandisets)} dandisets listed")
        # for dandiset in [{'identifier': '000027', 'most_recent_published_version':
        #                 {'version': '0.210831.2033'}}]:
        for dandiset in dandisets:
            identifier = dandiset["identifier"]
            # Fetch versions for each dandiset
            versions_url = (
                f"https://api.dandiarchive.org/api/dandisets/{identifier}/versions/"
                "?page_size=1000"
            )
            versions_response = requests.get(versions_url, headers=headers)
            versions = versions_response.json()["results"]
            for version in versions:
                version_id = version["version"]
                if version_id == "draft":
                    continue
                if identifier not in bibtex_results:
                    bibtex_results[identifier] = {}
                if version_id not in bibtex_results[identifier]:
                    doi_url = (
                        f"https://doi.org/10.48324/dandi.{identifier}/{version_id}"
                    )
                    doi_headers = {"Accept": "application/x-bibtex; charset=utf-8"}
                    doi_response = requests.get(doi_url, headers=doi_headers)
                    bibtex = doi_response.text
                    if not bibtex.startswith("@"):
                        lgr.error(
                            "Did not get a valid BibTeX for %s version %s",
                            identifier,
                            version_id,
                        )
                        bibtex_results[identifier][version_id] = (
                            f"# No valid BibTeX for {identifier}/{version_id}. "
                            f"Starts with {bibtex.splitlines()[0][:20]}"
                        )
                    else:
                        bibtex_results[identifier][version_id] = bibtex.replace(
                            "@misc{https://doi.org/10.48324/", "@misc{"
                        )
                    if get_metadata:
                        # fetch metadata record
                        metadata_response = requests.get(
                            f"https://api.dandiarchive.org/api/dandisets/"
                            f"{identifier}/versions/{version_id}/",
                            headers=headers,
                        )
                        metadata_records.append(json.loads(metadata_response.text))
            # The default to be cited -- ATM we do not have DOI for it, use latest version
            v = dandiset["most_recent_published_version"]["version"]
            # TODO: might want to robustify using `re` etc.
            if v not in bibtex_results[identifier]:
                lgr.error(
                    "Got no record for the most recent version %s of %s", v, identifier
                )
            else:
                bibtex_results[identifier][None] = bibtex_results[identifier][
                    v
                ].replace(
                    f"@misc{{dandi.{identifier}/{v},", f"@misc{{dandi.{identifier},"
                )
        if not url:
            lgr.info("No further URL, exiting")
            break

    return bibtex_results, metadata_records


def main():
    parser = argparse.ArgumentParser(description="Fetch bibliography collection")
    # TODO: for --me to function, would need to bolt on the authorization to work
    parser.add_argument("--me", action="store_true", help="Fetch only my dandisets")
    parser.add_argument(
        "--metadata",
        type=str,
        help="Fetch also metadata records and store in the file",
        default=None,
    )
    parser.add_argument(
        "--datacite",
        type=str,
        help="Fetch metadata records and convert to datacite and store in the file",
        default=None,
    )
    parser.add_argument(
        "--results",
        type=str,
        help="Path to prior BibTeX results JSON file, or where to save/cache results JSON "
        "(only for bibtex)",
        default=None,
    )
    parser.add_argument(
        "--bibtex",
        type=str,
        help="Path to the BibTeX file to save the results to. If not provided -- stdout",
        default=None,
    )
    # TODO:
    # - add options for requested format (currently bibtex but probably others supported)
    # - add option to do it only for specific datasets. Orthogonal to --me

    args = parser.parse_args()

    if args.datacite:
        from dandischema.datacite import to_datacite, validate_datacite
        from jsonschema import ValidationError as JSONValidationError
        from pydantic import ValidationError

    bibtex_results = None
    if args.results and os.path.lexists(args.results):
        with open(args.results, "r") as f:
            bibtex_results = json.load(f)

    bibtex_results, metadata_records = fetch_dandisets(
        args.me, bibtex_results, args.metadata or args.datacite
    )

    if args.results:
        if os.path.lexists(args.results):
            os.unlink(args.results)
        with open(args.results, "w") as f:
            json.dump(bibtex_results, f, indent=2)
        lgr.info("Updated results have been saved to %s", args.results)

    # OUTPUT BibTeX
    try:
        out = open(args.bibtex, "w") if args.bibtex else sys.stdout
        for dataset, versions in bibtex_results.items():
            out.write(f"# DANDISET {dataset}\n")
            for version, rec in versions.items():
                if version is None:
                    out.write("# Take latest as the default\n")
                out.write(f"{rec}\n\n")
    finally:
        if args.bibtex:
            out.close()

    # Metadata and datacite
    if metadata_records is not None:
        lgr.info("Got %d metadata records", len(metadata_records))
        if args.metadata:
            with open(args.metadata, "w") as f:
                json.dump(metadata_records, f, indent=True)
        if args.datacite:
            meta_errors = defaultdict(list)
            datacite_errors = defaultdict(list)
            datacite_records = []
            for m in metadata_records:
                try:
                    datacite_record = to_datacite(m)
                    try:
                        validate_datacite(datacite_record)
                    except JSONValidationError as exc:
                        error_rec = {
                            "identifier": m["identifier"],
                            "version": m["version"],
                            "message": exc.message,
                            "path": list(exc.path),
                            "schema_path": list(exc.schema_path),
                        }
                        datacite_errors[m["identifier"]].append(error_rec)
                except ValidationError as exc:
                    errors_filtered = []
                    # filter out the "input" field from errors
                    for error in exc.errors():
                        error.pop("input")
                        error["identifier"] = m["identifier"]
                        error["version"] = m["version"]
                        errors_filtered.append(error)
                    meta_errors[m["identifier"]].extend(errors_filtered)

            datacite_path = Path(args.datacite)
            datacite_path.with_suffix(".meta-errors.json").write_text(
                json.dumps(meta_errors, indent=True)
            )
            datacite_path.with_suffix(".datacite-errors.json").write_text(
                json.dumps(datacite_errors, indent=True)
            )
            datacite_path.write_text(json.dumps(datacite_records, indent=True))


if __name__ == "__main__":
    main()
