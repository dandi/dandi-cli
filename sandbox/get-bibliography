#!/usr/bin/env python

import argparse
from collections import defaultdict
import json
import logging
import os
from pathlib import Path
import sys

import requests

# Configure the logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    stream=sys.stderr,
)

lgr = logging.getLogger(__name__)


def fetch_dandisets(me, bibliography=None, bibtype="bibtex", get_metadata=False):
    if bibliography is None:
        bibliography = {}
    doi_req_headers = {
        "Accept": "application/x-{}; charset=utf-8".format(
            {"bibtex": "bibtex", "ris": "research-info-systems"}[bibtype.lower()]
        )
    }

    # Construct the query URL
    url = (
        f"https://api.dandiarchive.org/api/dandisets/?draft=false&empty=false"
        f"&embargoed=false{'&user=me' if me else ''}"
        # f"&search=000003"  # datacite error in 4.3 schema due to use of ROR
        # f"&search=000029"  # no BibTex
        # f"&search=000623"  # no DOI
        # f"&page_size=2"
    )
    headers = {"accept": "application/json"}
    metadata_records = []
    while True:
        # Fetch the dandisets
        lgr.info(f"Fetching {url}")
        response = requests.get(url, headers=headers)
        j = response.json()
        dandisets = j["results"]
        url = j.get("next")
        lgr.info(f"Got {len(dandisets)} dandisets listed")
        # for dandiset in [{'identifier': '000027', 'most_recent_published_version':
        #                 {'version': '0.210831.2033'}}]:
        for dandiset in dandisets:
            identifier = dandiset["identifier"]
            # Fetch versions for each dandiset
            versions_url = (
                f"https://api.dandiarchive.org/api/dandisets/{identifier}/versions/"
                "?page_size=1000"
            )
            versions_response = requests.get(versions_url, headers=headers)
            versions = versions_response.json()["results"]
            for version in versions:
                version_id = version["version"]
                if version_id == "draft":
                    continue
                if identifier not in bibliography:
                    bibliography[identifier] = {}
                if version_id not in bibliography[identifier]:
                    doi_url = (
                        f"https://doi.org/10.48324/dandi.{identifier}/{version_id}"
                    )
                    doi_response = requests.get(doi_url, headers=doi_req_headers)
                    bibentry = doi_response.text
                    if not bibentry.strip():
                        lgr.error(
                            "Got no bibliography record for %s version %s",
                            identifier,
                            version_id,
                        )
                    if bibtype == "bibtex":
                        if not bibentry.startswith("@"):
                            lgr.error(
                                "Did not get a valid BibTeX for %s version %s",
                                identifier,
                                version_id,
                            )
                            bibliography[identifier][version_id] = (
                                f"# No valid BibTeX for {identifier}/{version_id}. "
                                f"Starts with {bibentry.splitlines()[0][:20]}"
                            )
                        else:
                            bibliography[identifier][version_id] = bibentry.replace(
                                "@misc{https://doi.org/10.48324/", "@misc{"
                            )
                    elif bibtype == "ris":
                        bibliography[identifier][version_id] = bibentry
                    else:
                        raise ValueError(bibtype)
                    if get_metadata:
                        # fetch metadata record
                        metadata_response = requests.get(
                            f"https://api.dandiarchive.org/api/dandisets/"
                            f"{identifier}/versions/{version_id}/",
                            headers=headers,
                        )
                        metadata_records.append(json.loads(metadata_response.text))
            # The default to be cited -- ATM we do not have DOI for it, use latest version
            v = dandiset["most_recent_published_version"]["version"]
            # TODO: might want to robustify using `re` etc.
            if v not in bibliography[identifier]:
                lgr.error(
                    "Got no record for the most recent version %s of %s", v, identifier
                )
            elif bibtype == "bibtex":
                bibliography[identifier][None] = bibliography[identifier][v].replace(
                    f"@misc{{dandi.{identifier}/{v},", f"@misc{{dandi.{identifier},"
                )
                # in RIS there is no record name/identifier per se so makes
                # little sense to fabricate one here
        if not url:
            lgr.info("No further URL, exiting")
            break

    return bibliography, metadata_records


def main():
    parser = argparse.ArgumentParser(description="Fetch bibliography collection")
    # TODO: for --me to function, would need to bolt on the authorization to work
    parser.add_argument("--me", action="store_true", help="Fetch only my dandisets")
    parser.add_argument(
        "--metadata",
        type=str,
        help="Fetch also metadata records and store in the file",
        default=None,
    )
    parser.add_argument(
        "--datacite",
        type=str,
        help="Fetch metadata records and convert to datacite and store in the file",
        default=None,
    )
    parser.add_argument(
        "--results",
        type=str,
        help="Path to prior BibTeX results JSON file, or where to save/cache results JSON "
        "(only for bibtex)",
        default=None,
    )
    parser.add_argument(
        "--bibtype",
        type=str,
        default="bibtex",
        choices=["bibtex", "ris"],
        help="Type of bibliography to fetch. RIS seems to contain more metadata (abstract) "
        "but lacks way to specify record 'identifier'",
    )
    parser.add_argument(
        "--bibfile",
        type=str,
        help="Path to the bibliography file to save the results to. If not provided -- stdout",
        default=None,
    )
    # TODO:
    # - add options for requested format (currently bibtex but probably others supported)
    # - add option to do it only for specific datasets. Orthogonal to --me

    args = parser.parse_args()

    if args.datacite:
        from dandischema.datacite import to_datacite, validate_datacite
        from jsonschema import ValidationError as JSONValidationError
        from pydantic import ValidationError

    bibliography = None
    if args.results and os.path.lexists(args.results):
        with open(args.results, "r") as f:
            bibliography = json.load(f)

    bibliography, metadata_records = fetch_dandisets(
        args.me,
        bibliography,
        bibtype=args.bibtype,
        get_metadata=args.metadata or args.datacite,
    )

    if args.results:
        if os.path.lexists(args.results):
            os.unlink(args.results)
        with open(args.results, "w") as f:
            json.dump(bibliography, f, indent=2)
        lgr.info("Updated results have been saved to %s", args.results)

    # OUTPUT Bibliography
    try:
        out = open(args.bibfile, "w") if args.bibfile else sys.stdout
        for dataset, versions in bibliography.items():
            if args.bibtype == "bibtex":
                out.write(f"# DANDISET {dataset}\n")
            for version, rec in versions.items():
                if version is None:
                    out.write("# Take latest as the default\n")
                out.write(f"{rec}\n\n")
    finally:
        if args.bibfile:
            out.close()

    # Metadata and datacite
    if metadata_records is not None:
        lgr.info("Got %d metadata records", len(metadata_records))
        if args.metadata:
            with open(args.metadata, "w") as f:
                json.dump(metadata_records, f, indent=True)
        if args.datacite:
            meta_errors = defaultdict(list)
            datacite_errors = defaultdict(list)
            datacite_records = []
            for m in metadata_records:
                try:
                    datacite_record = to_datacite(m)
                    try:
                        validate_datacite(datacite_record)
                    except JSONValidationError as exc:
                        error_rec = {
                            "identifier": m["identifier"],
                            "version": m["version"],
                            "message": exc.message,
                            "path": list(exc.path),
                            "schema_path": list(exc.schema_path),
                        }
                        datacite_errors[m["identifier"]].append(error_rec)
                except ValidationError as exc:
                    errors_filtered = []
                    # filter out the "input" field from errors
                    for error in exc.errors():
                        error.pop("input", None)
                        # Convert any exception objects to strings
                        if "ctx" in error and isinstance(error["ctx"], dict):
                            for key, value in error["ctx"].items():
                                if isinstance(value, Exception):
                                    error["ctx"][key] = str(value)
                        error["identifier"] = m["identifier"]
                        error["version"] = m["version"]
                        errors_filtered.append(error)
                    meta_errors[m["identifier"]].extend(errors_filtered)

            datacite_path = Path(args.datacite)
            datacite_path.with_suffix(".meta-errors.json").write_text(
                json.dumps(meta_errors, indent=True)
            )
            datacite_path.with_suffix(".datacite-errors.json").write_text(
                json.dumps(datacite_errors, indent=True)
            )
            datacite_path.write_text(json.dumps(datacite_records, indent=True))


if __name__ == "__main__":
    main()
